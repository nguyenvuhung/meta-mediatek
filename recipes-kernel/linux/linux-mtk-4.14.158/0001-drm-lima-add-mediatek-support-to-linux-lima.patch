From f99f5d2d625159b4e97bbecc0ab14f58652009ec Mon Sep 17 00:00:00 2001
From: Nguyen Vu Hung <hungnv9@vng.com.vn>
Date: Tue, 18 Feb 2020 09:44:12 +0700
Subject: [PATCH] drm/lima: add mediatek support to linux-lima

---
 .../bindings/arm/mediatek/mediatek,g3dsys.txt      |  30 +
 .../devicetree/bindings/gpu/arm,mali-utgard.txt    |   9 +
 arch/arm/boot/dts/mt7623.dtsi                      |  44 +-
 arch/arm/boot/dts/mt7623n-bananapi-bpi-r2.dts      |   5 +
 drivers/clk/mediatek/Kconfig                       |   6 +
 drivers/clk/mediatek/Makefile                      |   1 +
 drivers/clk/mediatek/clk-mt2701-g3d.c              |  95 +++
 drivers/gpu/drm/Kconfig                            |   7 +
 drivers/gpu/drm/Makefile                           |   2 +
 drivers/gpu/drm/lima/Kconfig                       |   9 +
 drivers/gpu/drm/lima/Makefile                      |  17 +
 drivers/gpu/drm/lima/lima.h                        | 226 +++++++
 drivers/gpu/drm/lima/lima_bcast.c                  |  47 ++
 drivers/gpu/drm/lima/lima_bcast.h                  |  29 +
 drivers/gpu/drm/lima/lima_ctx.c                    | 121 ++++
 drivers/gpu/drm/lima/lima_device.c                 | 517 ++++++++++++++
 drivers/gpu/drm/lima/lima_dlbu.c                   |  51 ++
 drivers/gpu/drm/lima/lima_dlbu.h                   |  29 +
 drivers/gpu/drm/lima/lima_drv.c                    | 418 ++++++++++++
 drivers/gpu/drm/lima/lima_gem.c                    | 555 +++++++++++++++
 drivers/gpu/drm/lima/lima_gem.h                    |  86 +++
 drivers/gpu/drm/lima/lima_gem_prime.c              | 106 +++
 drivers/gpu/drm/lima/lima_gem_prime.h              |  31 +
 drivers/gpu/drm/lima/lima_gp.c                     | 356 ++++++++++
 drivers/gpu/drm/lima/lima_l2_cache.c               |  70 ++
 drivers/gpu/drm/lima/lima_mmu.c                    | 197 ++++++
 drivers/gpu/drm/lima/lima_pmu.c                    |  90 +++
 drivers/gpu/drm/lima/lima_pp.c                     | 355 ++++++++++
 drivers/gpu/drm/lima/lima_sched.c                  | 473 +++++++++++++
 drivers/gpu/drm/lima/lima_sched.h                  | 103 +++
 drivers/gpu/drm/lima/lima_vm.c                     | 223 ++++++
 drivers/gpu/drm/lima/lima_vm.h                     |  75 +++
 drivers/gpu/drm/scheduler/Makefile                 |  26 +
 drivers/gpu/drm/scheduler/gpu_scheduler.c          | 744 +++++++++++++++++++++
 drivers/gpu/drm/scheduler/sched_fence.c            | 191 ++++++
 include/drm/gpu_scheduler.h                        | 173 +++++
 include/drm/gpu_scheduler_trace.h                  |  82 +++
 include/drm/spsc_queue.h                           | 122 ++++
 include/dt-bindings/clock/mt2701-clk.h             |   4 +
 include/dt-bindings/reset/mt2701-resets.h          |   3 +
 include/uapi/drm/lima_drm.h                        | 201 ++++++
 41 files changed, 5928 insertions(+), 1 deletion(-)
 create mode 100644 Documentation/devicetree/bindings/arm/mediatek/mediatek,g3dsys.txt
 create mode 100644 drivers/clk/mediatek/clk-mt2701-g3d.c
 create mode 100644 drivers/gpu/drm/lima/Kconfig
 create mode 100644 drivers/gpu/drm/lima/Makefile
 create mode 100644 drivers/gpu/drm/lima/lima.h
 create mode 100644 drivers/gpu/drm/lima/lima_bcast.c
 create mode 100644 drivers/gpu/drm/lima/lima_bcast.h
 create mode 100644 drivers/gpu/drm/lima/lima_ctx.c
 create mode 100644 drivers/gpu/drm/lima/lima_device.c
 create mode 100644 drivers/gpu/drm/lima/lima_dlbu.c
 create mode 100644 drivers/gpu/drm/lima/lima_dlbu.h
 create mode 100644 drivers/gpu/drm/lima/lima_drv.c
 create mode 100644 drivers/gpu/drm/lima/lima_gem.c
 create mode 100644 drivers/gpu/drm/lima/lima_gem.h
 create mode 100644 drivers/gpu/drm/lima/lima_gem_prime.c
 create mode 100644 drivers/gpu/drm/lima/lima_gem_prime.h
 create mode 100644 drivers/gpu/drm/lima/lima_gp.c
 create mode 100644 drivers/gpu/drm/lima/lima_l2_cache.c
 create mode 100644 drivers/gpu/drm/lima/lima_mmu.c
 create mode 100644 drivers/gpu/drm/lima/lima_pmu.c
 create mode 100644 drivers/gpu/drm/lima/lima_pp.c
 create mode 100644 drivers/gpu/drm/lima/lima_sched.c
 create mode 100644 drivers/gpu/drm/lima/lima_sched.h
 create mode 100644 drivers/gpu/drm/lima/lima_vm.c
 create mode 100644 drivers/gpu/drm/lima/lima_vm.h
 create mode 100644 drivers/gpu/drm/scheduler/Makefile
 create mode 100644 drivers/gpu/drm/scheduler/gpu_scheduler.c
 create mode 100644 drivers/gpu/drm/scheduler/sched_fence.c
 create mode 100644 include/drm/gpu_scheduler.h
 create mode 100644 include/drm/gpu_scheduler_trace.h
 create mode 100644 include/drm/spsc_queue.h
 create mode 100644 include/uapi/drm/lima_drm.h

diff --git a/Documentation/devicetree/bindings/arm/mediatek/mediatek,g3dsys.txt b/Documentation/devicetree/bindings/arm/mediatek/mediatek,g3dsys.txt
new file mode 100644
index 0000000..f6ce13b
--- /dev/null
+++ b/Documentation/devicetree/bindings/arm/mediatek/mediatek,g3dsys.txt
@@ -0,0 +1,30 @@
+MediaTek g3dsys controller
+============================
+
+The MediaTek g3dsys controller provides various clocks and reset controller to
+the system.
+
+Required Properties:
+
+- compatible: Should be:
+	- "mediatek,mt2701-g3dsys", "syscon":
+		for MT2701 SoC
+	- "mediatek,mt7623-ethsys", "mediatek,mt2701-g3dsys", "syscon":
+		for MT7623 SoC
+- #clock-cells: Must be 1
+- #reset-cells: Must be 1
+
+The ethsys controller uses the common clk binding from
+Documentation/devicetree/bindings/clock/clock-bindings.txt
+The available clocks are defined in dt-bindings/clock/mt*-clk.h.
+
+Example:
+
+g3dsys: clock-controller@13000000 {
+	compatible = "mediatek,mt7623-g3dsys",
+		     "mediatek,mt2701-g3dsys",
+		     "syscon";
+	reg = <0 0x13000000 0 0x200>;
+	#clock-cells = <1>;
+	#reset-cells = <1>;
+};
\ No newline at end of file
diff --git a/Documentation/devicetree/bindings/gpu/arm,mali-utgard.txt b/Documentation/devicetree/bindings/gpu/arm,mali-utgard.txt
index b4ebd56..98ec360 100644
--- a/Documentation/devicetree/bindings/gpu/arm,mali-utgard.txt
+++ b/Documentation/devicetree/bindings/gpu/arm,mali-utgard.txt
@@ -14,6 +14,7 @@ Required properties:
       + amlogic,meson-gxbb-mali
       + amlogic,meson-gxl-mali
       + stericsson,db8500-mali
+      + mediatek,mt7623-mali
 
   - reg: Physical base address and length of the GPU registers
 
@@ -68,6 +69,14 @@ to specify one more vendor-specific compatible, among:
       * interrupt-names and interrupts:
         + combined: combined interrupt of all of the above lines
 
+  - mediatek,mt7623-mali
+     Required properties:
+      * resets: phandle to the reset line for the GPU
+      * mediatek,larb: phandle pointed to the local arbiter used to control the
+	    access to external memory on the SoC.
+	    see Documentation/devicetree/bindings/memory-controllers/mediatek,smi-larb.txt
+	    for details
+
 Example:
 
 mali: gpu@1c40000 {
diff --git a/arch/arm/boot/dts/mt7623.dtsi b/arch/arm/boot/dts/mt7623.dtsi
index 1816acf..5383bbb 100644
--- a/arch/arm/boot/dts/mt7623.dtsi
+++ b/arch/arm/boot/dts/mt7623.dtsi
@@ -359,7 +359,7 @@
 		interrupts = <GIC_SPI 106 IRQ_TYPE_LEVEL_LOW>;
 		clocks = <&infracfg CLK_INFRA_M4U>;
 		clock-names = "bclk";
-		mediatek,larbs = <&larb0 &larb1 &larb2>;
+		mediatek,larbs = <&larb0 &larb1 &larb2 &larb3>;
 		#iommu-cells = <1>;
 	};
 
@@ -1241,4 +1241,46 @@
 		#clock-cells = <1>;
 		#reset-cells = <1>;
 	};
+
+	g3dsys: clock-controller@13000000 {
+		compatible = "mediatek,mt7623-g3dsys",
+			     "mediatek,mt2701-g3dsys",
+			     "syscon";
+		reg = <0 0x13000000 0 0x200>;
+		#clock-cells = <1>;
+		#reset-cells = <1>;
+	};
+
+	larb3: larb@13010000 {
+		compatible = "mediatek,mt7623-smi-larb",
+			     "mediatek,mt2701-smi-larb";
+		reg = <0 0x13010000 0 0x1000>;
+		mediatek,smi = <&smi_common>;
+		mediatek,larb-id = <3>;
+		clocks = <&clk26m>, <&clk26m>;
+		clock-names = "apb", "smi";
+		power-domains = <&scpsys MT2701_POWER_DOMAIN_MFG>;
+	};
+
+	mali: gpu@13040000 {
+		compatible = "mediatek,mt7623-mali", "arm,mali-450";
+		reg = <0 0x13040000 0 0x30000>;
+		interrupts = <GIC_SPI 170 IRQ_TYPE_LEVEL_LOW>,
+			     <GIC_SPI 171 IRQ_TYPE_LEVEL_LOW>,
+			     <GIC_SPI 172 IRQ_TYPE_LEVEL_LOW>,
+			     <GIC_SPI 173 IRQ_TYPE_LEVEL_LOW>,
+			     <GIC_SPI 174 IRQ_TYPE_LEVEL_LOW>,
+			     <GIC_SPI 175 IRQ_TYPE_LEVEL_LOW>,
+			     <GIC_SPI 176 IRQ_TYPE_LEVEL_LOW>,
+			     <GIC_SPI 177 IRQ_TYPE_LEVEL_LOW>,
+			     <GIC_SPI 180 IRQ_TYPE_LEVEL_LOW>;
+		interrupt-names = "gp", "gpmmu", "pp0", "ppmmu0", "pp1",
+				  "ppmmu1", "pp2", "ppmmu2", "pp";
+		clocks = <&topckgen CLK_TOP_MMPLL>,
+			 <&g3dsys CLK_G3DSYS_CORE>;
+		clock-names = "bus", "core";
+		power-domains = <&scpsys MT2701_POWER_DOMAIN_MFG>;
+		mediatek,larb = <&larb3>;
+		resets = <&g3dsys MT2701_G3DSYS_CORE_RST>;
+	};
 };
diff --git a/arch/arm/boot/dts/mt7623n-bananapi-bpi-r2.dts b/arch/arm/boot/dts/mt7623n-bananapi-bpi-r2.dts
index ed0efe3..09f38dc 100644
--- a/arch/arm/boot/dts/mt7623n-bananapi-bpi-r2.dts
+++ b/arch/arm/boot/dts/mt7623n-bananapi-bpi-r2.dts
@@ -311,6 +311,11 @@
 	status = "okay";
 };
 
+&mali {
+	vdd_g3d-supply = <&vdd_fixed_vgpu_reg>;
+	status = "okay";
+};
+
 &mmc0 {
 	pinctrl-names = "default", "state_uhs";
 	pinctrl-0 = <&mmc0_pins_default>;
diff --git a/drivers/clk/mediatek/Kconfig b/drivers/clk/mediatek/Kconfig
index 28739a9..b41f88d 100644
--- a/drivers/clk/mediatek/Kconfig
+++ b/drivers/clk/mediatek/Kconfig
@@ -50,6 +50,12 @@ config COMMON_CLK_MT2701_BDPSYS
 	---help---
 	  This driver supports Mediatek MT2701 bdpsys clocks.
 
+config COMMON_CLK_MT2701_G3DSYS
+	bool "Clock driver for MediaTek MT2701 g3dsys"
+	depends on COMMON_CLK_MT2701
+	---help---
+	  This driver supports MediaTek MT2701 g3dsys clocks.
+
 config COMMON_CLK_MT6797
        bool "Clock driver for Mediatek MT6797"
        depends on (ARCH_MEDIATEK && ARM64) || COMPILE_TEST
diff --git a/drivers/clk/mediatek/Makefile b/drivers/clk/mediatek/Makefile
index ba2a070..7c49cee 100644
--- a/drivers/clk/mediatek/Makefile
+++ b/drivers/clk/mediatek/Makefile
@@ -9,6 +9,7 @@ obj-$(CONFIG_COMMON_CLK_MT6797_VENCSYS) += clk-mt6797-venc.o
 obj-$(CONFIG_COMMON_CLK_MT2701) += clk-mt2701.o
 obj-$(CONFIG_COMMON_CLK_MT2701_BDPSYS) += clk-mt2701-bdp.o
 obj-$(CONFIG_COMMON_CLK_MT2701_ETHSYS) += clk-mt2701-eth.o
+obj-$(CONFIG_COMMON_CLK_MT2701_G3DSYS) += clk-mt2701-g3d.o
 obj-$(CONFIG_COMMON_CLK_MT2701_HIFSYS) += clk-mt2701-hif.o
 obj-$(CONFIG_COMMON_CLK_MT2701_IMGSYS) += clk-mt2701-img.o
 obj-$(CONFIG_COMMON_CLK_MT2701_MMSYS) += clk-mt2701-mm.o
diff --git a/drivers/clk/mediatek/clk-mt2701-g3d.c b/drivers/clk/mediatek/clk-mt2701-g3d.c
new file mode 100644
index 0000000..9b06db4
--- /dev/null
+++ b/drivers/clk/mediatek/clk-mt2701-g3d.c
@@ -0,0 +1,95 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (c) 2018 MediaTek Inc.
+ * Author: Sean Wang <sean.wang@mediatek.com>
+ *
+ */
+
+#include <linux/clk-provider.h>
+#include <linux/of.h>
+#include <linux/of_address.h>
+#include <linux/of_device.h>
+#include <linux/platform_device.h>
+
+#include "clk-mtk.h"
+#include "clk-gate.h"
+
+#include <dt-bindings/clock/mt2701-clk.h>
+
+#define GATE_G3D(_id, _name, _parent, _shift) {	\
+		.id = _id,				\
+		.name = _name,				\
+		.parent_name = _parent,			\
+		.regs = &g3d_cg_regs,			\
+		.shift = _shift,			\
+		.ops = &mtk_clk_gate_ops_setclr,	\
+	}
+
+static const struct mtk_gate_regs g3d_cg_regs = {
+	.sta_ofs = 0x0,
+	.set_ofs = 0x4,
+	.clr_ofs = 0x8,
+};
+
+static const struct mtk_gate g3d_clks[] = {
+	GATE_G3D(CLK_G3DSYS_CORE, "g3d_core", "mfg_sel", 0),
+};
+
+static int clk_mt2701_g3dsys_init(struct platform_device *pdev)
+{
+	struct clk_onecell_data *clk_data;
+	struct device_node *node = pdev->dev.of_node;
+	int r;
+
+	clk_data = mtk_alloc_clk_data(CLK_G3DSYS_NR);
+
+	mtk_clk_register_gates(node, g3d_clks, ARRAY_SIZE(g3d_clks),
+			       clk_data);
+
+	r = of_clk_add_provider(node, of_clk_src_onecell_get, clk_data);
+	if (r)
+		dev_err(&pdev->dev,
+			"could not register clock provider: %s: %d\n",
+			pdev->name, r);
+
+	mtk_register_reset_controller(node, 1, 0xc);
+
+	return r;
+}
+
+static const struct of_device_id of_match_clk_mt2701_g3d[] = {
+	{
+		.compatible = "mediatek,mt2701-g3dsys",
+		.data = clk_mt2701_g3dsys_init,
+	}, {
+		/* sentinel */
+	}
+};
+
+static int clk_mt2701_g3d_probe(struct platform_device *pdev)
+{
+	int (*clk_init)(struct platform_device *);
+	int r;
+
+	clk_init = of_device_get_match_data(&pdev->dev);
+	if (!clk_init)
+		return -EINVAL;
+
+	r = clk_init(pdev);
+	if (r)
+		dev_err(&pdev->dev,
+			"could not register clock provider: %s: %d\n",
+			pdev->name, r);
+
+	return r;
+}
+
+static struct platform_driver clk_mt2701_g3d_drv = {
+	.probe = clk_mt2701_g3d_probe,
+	.driver = {
+		.name = "clk-mt2701-g3d",
+		.of_match_table = of_match_clk_mt2701_g3d,
+	},
+};
+
+builtin_platform_driver(clk_mt2701_g3d_drv);
\ No newline at end of file
diff --git a/drivers/gpu/drm/Kconfig b/drivers/gpu/drm/Kconfig
index 83cb2a8..121b417 100644
--- a/drivers/gpu/drm/Kconfig
+++ b/drivers/gpu/drm/Kconfig
@@ -149,6 +149,10 @@ config DRM_VM
 	bool
 	depends on DRM && MMU
 
+config DRM_SCHED
+	tristate
+	depends on DRM
+
 source "drivers/gpu/drm/i2c/Kconfig"
 
 source "drivers/gpu/drm/arm/Kconfig"
@@ -178,6 +182,7 @@ config DRM_AMDGPU
 	depends on DRM && PCI && MMU
 	select FW_LOADER
         select DRM_KMS_HELPER
+	select DRM_SCHED
         select DRM_TTM
 	select POWER_SUPPLY
 	select HWMON
@@ -278,6 +283,8 @@ source "drivers/gpu/drm/tinydrm/Kconfig"
 
 source "drivers/gpu/drm/pl111/Kconfig"
 
+source "drivers/gpu/drm/lima/Kconfig"
+
 # Keep legacy drivers last
 
 menuconfig DRM_LEGACY
diff --git a/drivers/gpu/drm/Makefile b/drivers/gpu/drm/Makefile
index 8ce0703..fa43bae 100644
--- a/drivers/gpu/drm/Makefile
+++ b/drivers/gpu/drm/Makefile
@@ -51,6 +51,7 @@ obj-$(CONFIG_DRM)	+= drm.o
 obj-$(CONFIG_DRM_MIPI_DSI) += drm_mipi_dsi.o
 obj-$(CONFIG_DRM_ARM)	+= arm/
 obj-$(CONFIG_DRM_TTM)	+= ttm/
+obj-$(CONFIG_DRM_SCHED)	+= scheduler/
 obj-$(CONFIG_DRM_TDFX)	+= tdfx/
 obj-$(CONFIG_DRM_R128)	+= r128/
 obj-$(CONFIG_HSA_AMD) += amd/amdkfd/
@@ -101,3 +102,4 @@ obj-$(CONFIG_DRM_ZTE)	+= zte/
 obj-$(CONFIG_DRM_MXSFB)	+= mxsfb/
 obj-$(CONFIG_DRM_TINYDRM) += tinydrm/
 obj-$(CONFIG_DRM_PL111) += pl111/
+obj-$(CONFIG_DRM_LIMA)  += lima/
diff --git a/drivers/gpu/drm/lima/Kconfig b/drivers/gpu/drm/lima/Kconfig
new file mode 100644
index 0000000..c03e217
--- /dev/null
+++ b/drivers/gpu/drm/lima/Kconfig
@@ -0,0 +1,9 @@
+
+config DRM_LIMA
+       tristate "LIMA (DRM support for ARM Mali 400/450 GPU)"
+       depends on DRM
+       depends on ARCH_SUNXI || ARCH_ROCKCHIP || ARCH_EXYNOS || ARCH_MESON || ARCH_MEDIATEK
+       depends on (ARM && !ARM_LPAE) || ZONE_DMA32 || ZONE_DMA
+       select DRM_SCHED
+       help
+         DRM driver for ARM Mali 400/450 GPUs.
diff --git a/drivers/gpu/drm/lima/Makefile b/drivers/gpu/drm/lima/Makefile
new file mode 100644
index 0000000..a5ecdb0
--- /dev/null
+++ b/drivers/gpu/drm/lima/Makefile
@@ -0,0 +1,17 @@
+lima-y := \
+	lima_drv.o \
+	lima_device.o \
+	lima_pmu.o \
+	lima_l2_cache.o \
+	lima_mmu.o \
+	lima_gp.o \
+	lima_pp.o \
+	lima_gem.o \
+	lima_vm.o \
+	lima_sched.o \
+	lima_ctx.o \
+	lima_gem_prime.o \
+	lima_dlbu.o \
+	lima_bcast.o
+
+obj-$(CONFIG_DRM_LIMA) += lima.o
diff --git a/drivers/gpu/drm/lima/lima.h b/drivers/gpu/drm/lima/lima.h
new file mode 100644
index 0000000..0eb789d
--- /dev/null
+++ b/drivers/gpu/drm/lima/lima.h
@@ -0,0 +1,226 @@
+/*
+ * Copyright (C) 2017 Lima Project
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ */
+#ifndef __LIMA_H__
+
+#include <linux/rbtree.h>
+
+#include <drm/drmP.h>
+#include <drm/lima_drm.h>
+
+#include "lima_vm.h"
+#include "lima_sched.h"
+
+extern int lima_sched_timeout_ms;
+extern int lima_sched_max_tasks;
+
+enum lima_gpu_type {
+	GPU_MALI400 = 0,
+	GPU_MALI450,
+};
+
+enum lima_soc_type {
+	SOC_GENERIC = 0,
+	SOC_MEDIATEK = 1,
+};
+
+struct lima_device;
+
+#define LIMA_IP_MAX_NAME_LEN 32
+
+struct lima_ip {
+	struct lima_device *dev;
+	char name[LIMA_IP_MAX_NAME_LEN];
+	void __iomem *iomem;
+	int irq;
+};
+
+struct lima_pmu {
+	struct lima_ip ip;
+	uint32_t switch_delay;
+};
+
+struct lima_l2_cache {
+	struct lima_ip ip;
+};
+
+struct lima_mmu {
+	struct lima_ip ip;
+	struct lima_sched_pipe *pipe;
+
+	spinlock_t lock;
+	struct lima_vm *vm;
+	bool zap_all;
+};
+
+#define LIMA_GP_TASK_VS    0x01
+#define LIMA_GP_TASK_PLBU  0x02
+
+struct lima_gp {
+	struct lima_ip ip;
+	struct lima_mmu mmu;
+	struct lima_sched_pipe pipe;
+	struct lima_l2_cache *l2_cache;
+
+	int task;
+	bool async_reset;
+};
+
+struct lima_pp_core {
+	struct lima_ip ip;
+	struct lima_mmu mmu;
+	bool async_reset;
+};
+
+#define LIMA_MAX_PP 4
+
+struct lima_pp {
+	struct lima_pp_core core[LIMA_MAX_PP];
+	int num_core;
+	struct lima_sched_pipe pipe;
+	struct lima_l2_cache *l2_cache;
+	atomic_t task;
+	bool error;
+};
+
+#define LIMA_MAX_PIPE 2
+
+struct lima_dlbu {
+	struct lima_ip ip;
+};
+
+struct lima_bcast {
+	struct lima_ip ip;
+};
+
+struct lima_device {
+	struct device *dev;
+	struct drm_device *ddev;
+	struct platform_device *pdev;
+
+	enum lima_gpu_type gpu_type;
+	enum lima_soc_type soc_type;
+	void *soc;
+
+	void __iomem *iomem;
+
+	struct clk *clk_bus;
+	struct clk *clk_gpu;
+	struct reset_control *reset;
+	struct regulator *regulator;
+
+	struct lima_pmu *pmu;
+
+	struct lima_l2_cache *l2_cache;
+
+	struct lima_sched_pipe *pipe[LIMA_MAX_PIPE];
+
+	struct lima_gp *gp;
+	struct lima_pp *pp;
+	int num_pp;
+
+	struct lima_dlbu *dlbu;
+	struct lima_bcast *bcast;
+
+	struct lima_vm *empty_vm;
+	uint64_t va_start;
+	uint64_t va_end;
+};
+
+struct lima_ctx {
+	struct kref refcnt;
+	struct lima_device *dev;
+	struct lima_sched_context context[LIMA_MAX_PIPE];
+	atomic_t guilty;
+};
+
+struct lima_ctx_mgr {
+	spinlock_t lock;
+	struct idr handles;
+};
+
+struct lima_drm_priv {
+	struct lima_vm *vm;
+	struct lima_ctx_mgr ctx_mgr;
+};
+
+struct lima_bo_va_mapping {
+	struct list_head list;
+	struct rb_node rb;
+	uint32_t start;
+	uint32_t last;
+	uint32_t __subtree_last;
+};
+
+struct lima_submit {
+	struct lima_ctx *ctx;
+	int pipe;
+
+	struct drm_lima_gem_submit_bo *bos;
+	struct lima_bo **lbos;
+	u32 nr_bos;
+
+	struct lima_sched_task *task;
+
+	uint32_t fence;
+	uint32_t done;
+};
+
+static inline struct lima_drm_priv *
+to_lima_drm_priv(struct drm_file *file)
+{
+	return file->driver_priv;
+}
+
+int lima_device_init(struct lima_device *ldev);
+void lima_device_fini(struct lima_device *ldev);
+
+int lima_pmu_init(struct lima_pmu *pmu);
+void lima_pmu_fini(struct lima_pmu *pmu);
+
+int lima_l2_cache_init(struct lima_l2_cache *l2_cache);
+void lima_l2_cache_fini(struct lima_l2_cache *l2_cache);
+int lima_l2_cache_flush(struct lima_l2_cache *l2_cache);
+
+int lima_mmu_init(struct lima_mmu *mmu);
+void lima_mmu_fini(struct lima_mmu *mmu);
+void lima_mmu_switch_vm(struct lima_mmu *mmu, struct lima_vm *vm, bool reset);
+void lima_mmu_zap_vm(struct lima_mmu *mmu, struct lima_vm *vm, u32 va, u32 size);
+void lima_mmu_page_fault_resume(struct lima_mmu *mmu);
+
+int lima_gp_init(struct lima_gp *gp);
+void lima_gp_fini(struct lima_gp *gp);
+
+int lima_pp_core_init(struct lima_pp_core *core);
+void lima_pp_core_fini(struct lima_pp_core *core);
+int lima_pp_init(struct lima_pp *pp);
+void lima_pp_fini(struct lima_pp *pp);
+
+unsigned long lima_timeout_to_jiffies(u64 timeout_ns);
+
+int lima_ctx_create(struct lima_device *dev, struct lima_ctx_mgr *mgr, u32 *id);
+int lima_ctx_free(struct lima_ctx_mgr *mgr, u32 id);
+struct lima_ctx *lima_ctx_get(struct lima_ctx_mgr *mgr, u32 id);
+void lima_ctx_put(struct lima_ctx *ctx);
+void lima_ctx_mgr_init(struct lima_ctx_mgr *mgr);
+void lima_ctx_mgr_fini(struct lima_ctx_mgr *mgr);
+
+#endif
diff --git a/drivers/gpu/drm/lima/lima_bcast.c b/drivers/gpu/drm/lima/lima_bcast.c
new file mode 100644
index 0000000..1e4f9fe
--- /dev/null
+++ b/drivers/gpu/drm/lima/lima_bcast.c
@@ -0,0 +1,47 @@
+/*
+ * Copyright (C) 2018 Lima Project
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+#include "lima.h"
+#include "lima_bcast.h"
+
+#define LIMA_BCAST_BROADCAST_MASK    0x0
+#define LIMA_BCAST_INTERRUPT_MASK    0x4
+
+#define bcast_write(reg, data) writel(data, bcast->ip.iomem + LIMA_BCAST_##reg)
+#define bcast_read(reg) readl(bcast->ip.iomem + LIMA_BCAST_##reg)
+
+int lima_bcast_init(struct lima_bcast *bcast)
+{
+	struct lima_device *dev = bcast->ip.dev;
+
+	dev_info(dev->dev, "bcast %x %x\n",
+		 bcast_read(BROADCAST_MASK),
+		 bcast_read(INTERRUPT_MASK));
+
+	return 0;
+}
+
+void lima_bcast_fini(struct lima_bcast *bcast)
+{
+	
+}
+
diff --git a/drivers/gpu/drm/lima/lima_bcast.h b/drivers/gpu/drm/lima/lima_bcast.h
new file mode 100644
index 0000000..3619bad
--- /dev/null
+++ b/drivers/gpu/drm/lima/lima_bcast.h
@@ -0,0 +1,29 @@
+/*
+ * Copyright (C) 2018 Lima Project
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+#ifndef __LIMA_BCAST_H__
+#define __LIMA_BCAST_H__
+
+int lima_bcast_init(struct lima_bcast *bcast);
+void lima_bcast_fini(struct lima_bcast *bcast);
+
+#endif
diff --git a/drivers/gpu/drm/lima/lima_ctx.c b/drivers/gpu/drm/lima/lima_ctx.c
new file mode 100644
index 0000000..24ca647
--- /dev/null
+++ b/drivers/gpu/drm/lima/lima_ctx.c
@@ -0,0 +1,121 @@
+/*
+ * Copyright (C) 2018 Lima Project
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+#include "lima.h"
+
+int lima_ctx_create(struct lima_device *dev, struct lima_ctx_mgr *mgr, u32 *id)
+{
+	struct lima_ctx *ctx;
+	int i, err;
+
+	ctx = kzalloc(sizeof(*ctx), GFP_KERNEL);
+	if (!ctx)
+		return -ENOMEM;
+	ctx->dev = dev;
+	kref_init(&ctx->refcnt);
+
+	for (i = 0; i < LIMA_MAX_PIPE; i++) {
+		err = lima_sched_context_init(dev->pipe[i], ctx->context + i, &ctx->guilty);
+		if (err)
+			goto err_out0;
+	}
+
+	idr_preload(GFP_KERNEL);
+	spin_lock(&mgr->lock);
+	err = idr_alloc(&mgr->handles, ctx, 1, 0, GFP_ATOMIC);
+	spin_unlock(&mgr->lock);
+	idr_preload_end();
+	if (err < 0)
+		goto err_out0;
+
+	*id = err;
+	return 0;
+
+err_out0:
+	for (i--; i >= 0; i--)
+		lima_sched_context_fini(dev->pipe[i], ctx->context + i);
+	kfree(ctx);
+	return err;
+}
+
+static void lima_ctx_do_release(struct kref *ref)
+{
+	struct lima_ctx *ctx = container_of(ref, struct lima_ctx, refcnt);
+	int i;
+
+	for (i = 0; i < LIMA_MAX_PIPE; i++)
+		lima_sched_context_fini(ctx->dev->pipe[i], ctx->context + i);
+	kfree(ctx);
+}
+
+int lima_ctx_free(struct lima_ctx_mgr *mgr, u32 id)
+{
+	struct lima_ctx *ctx;
+
+	spin_lock(&mgr->lock);
+	ctx = idr_remove(&mgr->handles, id);
+	spin_unlock(&mgr->lock);
+
+	if (ctx) {
+		kref_put(&ctx->refcnt, lima_ctx_do_release);
+		return 0;
+	}
+	return -EINVAL;
+}
+
+struct lima_ctx *lima_ctx_get(struct lima_ctx_mgr *mgr, u32 id)
+{
+	struct lima_ctx *ctx;
+
+	spin_lock(&mgr->lock);
+	ctx = idr_find(&mgr->handles, id);
+	if (ctx)
+		kref_get(&ctx->refcnt);
+	spin_unlock(&mgr->lock);
+	return ctx;
+}
+
+void lima_ctx_put(struct lima_ctx *ctx)
+{
+	kref_put(&ctx->refcnt, lima_ctx_do_release);
+}
+
+void lima_ctx_mgr_init(struct lima_ctx_mgr *mgr)
+{
+        spin_lock_init(&mgr->lock);
+	idr_init(&mgr->handles);
+}
+
+void lima_ctx_mgr_fini(struct lima_ctx_mgr *mgr)
+{
+	struct lima_ctx *ctx;
+	struct idr *idp;
+	uint32_t id;
+
+	idp = &mgr->handles;
+
+	idr_for_each_entry(idp, ctx, id) {
+	        kref_put(&ctx->refcnt, lima_ctx_do_release);
+	}
+
+	idr_destroy(&mgr->handles);
+}
diff --git a/drivers/gpu/drm/lima/lima_device.c b/drivers/gpu/drm/lima/lima_device.c
new file mode 100644
index 0000000..62db059
--- /dev/null
+++ b/drivers/gpu/drm/lima/lima_device.c
@@ -0,0 +1,517 @@
+#include <linux/regulator/consumer.h>
+#include <linux/reset.h>
+#include <linux/clk.h>
+#include <linux/dma-mapping.h>
+#include <linux/of_platform.h>
+#include <linux/pm_runtime.h>
+#include <soc/mediatek/smi.h>
+
+
+#include "lima.h"
+#include "lima_dlbu.h"
+#include "lima_bcast.h"
+
+#define LIMA_GP_BASE           0x0000
+#define LIMA_L2_BASE           0x1000
+#define LIMA_PMU_BASE          0x2000
+#define LIMA_GPMMU_BASE        0x3000
+#define LIMA_PPMMU_BASE(i)     ((i < 4) ? 0x4000 + 0x1000 * (i) : \
+					  0x1C000 + 0x1000 * (i - 4))
+#define LIMA_PP_BASE(i)        ((i < 4) ? 0x8000 + 0x2000 * (i) : \
+					  0x28000 + 0x2000 * (i - 4))
+
+/* Separate L2-caches per group on Mali450 */
+#define LIMA450_GPL2_BASE      0x10000
+#define LIMA450_PP03L2_BASE    0x01000
+#define LIMA450_PP47L2_BASE    0x11000
+
+#define LIMA_BCAST_BASE        0x13000
+#define LIMA_PPBCAST_BASE      0x16000
+#define LIMA_PPBCASTMMU_BASE   0x15000
+#define LIMA_DLBU_BASE         0x14000
+#define LIMA_DMA_BASE          0x12000
+
+static int lima_mediatek_init(struct lima_device *dev)
+{
+	struct device_node *np;
+	int err = -ENODEV;
+
+	np = of_parse_phandle(dev->dev->of_node, "mediatek,larb", 0);
+	if (np) {
+		struct platform_device *larb = of_find_device_by_node(np);
+
+		err = !larb || !platform_get_drvdata(larb);
+		if (err) {
+			err = -EPROBE_DEFER;
+			goto out;
+		}
+
+		dev->soc = kzalloc(sizeof(struct device), GFP_KERNEL);
+		if (!dev->soc) {
+			err = -ENOMEM;
+			goto out;
+		}
+
+		err = mtk_smi_larb_get(&larb->dev);
+		if (err) {
+			kfree(dev->soc);
+			goto out;
+		}
+
+		dev->soc = (void *)&larb->dev;
+	}
+
+out:
+	of_node_put(np);
+
+	pr_info("INFO@%s %d err = %d\n", __func__, __LINE__, err);
+
+	return err;
+}
+
+static void lima_mediatek_fini(struct lima_device *dev)
+{
+	mtk_smi_larb_put(dev->soc);
+
+	kfree(dev->soc);
+
+	pr_info("INFO@%s %d\n", __func__, __LINE__);
+}
+
+static int lima_soc_init(struct lima_device *dev)
+{
+	int err = 0;
+
+	pm_runtime_enable(dev->dev);
+	pm_runtime_get_sync(dev->dev);
+
+	if (dev->soc_type == SOC_MEDIATEK)
+		err = lima_mediatek_init(dev);
+
+	return err;
+}
+
+static void lima_soc_fini(struct lima_device *dev)
+{
+	if (dev->soc_type == SOC_MEDIATEK)
+		lima_mediatek_fini(dev);
+
+	pm_runtime_put_sync(dev->dev);
+	pm_runtime_disable(dev->dev);
+}
+
+static int lima_clk_init(struct lima_device *dev)
+{
+	int err;
+	unsigned long bus_rate, gpu_rate;
+
+	dev->clk_bus = devm_clk_get(dev->dev, "bus");
+	if (IS_ERR(dev->clk_bus)) {
+		dev_err(dev->dev, "get bus clk failed %ld\n", PTR_ERR(dev->clk_bus));
+		return PTR_ERR(dev->clk_bus);
+	}
+
+	dev->clk_gpu = devm_clk_get(dev->dev, "core");
+	if (IS_ERR(dev->clk_gpu)) {
+		dev_err(dev->dev, "get core clk failed %ld\n", PTR_ERR(dev->clk_gpu));
+		return PTR_ERR(dev->clk_gpu);
+	}
+
+	bus_rate = clk_get_rate(dev->clk_bus);
+	dev_info(dev->dev, "bus rate = %lu\n", bus_rate);
+
+	gpu_rate = clk_get_rate(dev->clk_gpu);
+	dev_info(dev->dev, "mod rate = %lu", gpu_rate);
+
+	if ((err = clk_prepare_enable(dev->clk_bus)))
+		return err;
+	if ((err = clk_prepare_enable(dev->clk_gpu)))
+		goto error_out0;
+
+	dev->reset = devm_reset_control_get_optional(dev->dev, NULL);
+	if (IS_ERR(dev->reset)) {
+		err = PTR_ERR(dev->reset);
+		goto error_out1;
+	} else if (dev->reset != NULL) {
+		if ((err = reset_control_deassert(dev->reset)))
+			goto error_out1;
+	}
+
+	return 0;
+
+error_out1:
+	clk_disable_unprepare(dev->clk_gpu);
+error_out0:
+	clk_disable_unprepare(dev->clk_bus);
+	return err;
+}
+
+static void lima_clk_fini(struct lima_device *dev)
+{
+	if (dev->reset != NULL)
+		reset_control_assert(dev->reset);
+	clk_disable_unprepare(dev->clk_gpu);
+	clk_disable_unprepare(dev->clk_bus);
+}
+
+static int lima_regulator_init(struct lima_device *dev)
+{
+	int ret;
+	dev->regulator = devm_regulator_get_optional(dev->dev, "mali");
+	if (IS_ERR(dev->regulator)) {
+		ret = PTR_ERR(dev->regulator);
+		dev->regulator = NULL;
+		if (ret == -ENODEV)
+			return 0;
+		dev_err(dev->dev, "failed to get regulator: %ld\n", PTR_ERR(dev->regulator));
+		return ret;
+	}
+
+	ret = regulator_enable(dev->regulator);
+	if (ret < 0) {
+		dev_err(dev->dev, "failed to enable regulator: %d\n", ret);
+		return ret;
+	}
+
+	return 0;
+}
+
+static void lima_regulator_fini(struct lima_device *dev)
+{
+	if (dev->regulator)
+		regulator_disable(dev->regulator);
+}
+
+
+static int lima_init_ip(struct lima_device *dev, const char *name,
+			struct lima_ip *ip, u32 offset)
+{
+	ip->iomem = dev->iomem + offset;
+	ip->dev = dev;
+
+	strncpy(ip->name, name, LIMA_IP_MAX_NAME_LEN);
+	ip->name[LIMA_IP_MAX_NAME_LEN - 1] = '\0';
+
+	if (ip->irq == 0) {
+		ip->irq = platform_get_irq_byname(dev->pdev, name);
+		if (ip->irq < 0) {
+			dev_err(dev->dev, "fail to get irq %s\n", name);
+			return ip->irq;
+		}
+	}
+
+	return 0;
+}
+
+static int lima_gp_group_init(struct lima_device *dev)
+{
+	int err;
+	struct lima_gp *gp;
+
+	gp = kzalloc(sizeof(*gp), GFP_KERNEL);
+	if (!gp)
+		return -ENOMEM;
+
+	/* Init GP-group L2 cache on Mali450 */
+	if (dev->gpu_type == GPU_MALI450) {
+		gp->l2_cache = kzalloc(sizeof(*gp->l2_cache), GFP_KERNEL);
+		if (!gp->l2_cache) {
+			err = -ENOMEM;
+			goto err_out0;
+		}
+		gp->l2_cache->ip.irq = -1;
+		if ((err = lima_init_ip(dev, "gp-l2-cache", &gp->l2_cache->ip, LIMA450_GPL2_BASE)) ||
+		    (err = lima_l2_cache_init(gp->l2_cache))) {
+			goto err_out1;
+		}
+	}
+
+	if ((err = lima_init_ip(dev, "gpmmu", &gp->mmu.ip, LIMA_GPMMU_BASE)) ||
+	    (err = lima_mmu_init(&gp->mmu)))
+		goto err_out1;
+
+	if ((err = lima_init_ip(dev, "gp", &gp->ip, LIMA_GP_BASE)) ||
+	    (err = lima_gp_init(gp)))
+		goto err_out2;
+
+	if ((err = lima_sched_pipe_init(&gp->pipe, gp->ip.name)))
+		goto err_out3;
+
+	dev->pipe[LIMA_PIPE_GP] = &gp->pipe;
+	gp->mmu.pipe = &gp->pipe;
+	dev->gp = gp;
+	return 0;
+
+err_out3:
+	lima_gp_fini(gp);
+err_out2:
+	lima_mmu_fini(&gp->mmu);
+err_out1:
+	if (gp->l2_cache)
+		lima_l2_cache_fini(gp->l2_cache);
+err_out0:
+	if (gp->l2_cache)
+		kfree(gp->l2_cache);
+	kfree(gp);
+	return err;
+}
+
+static int lima_pp_group_init(struct lima_device *dev)
+{
+	int err, i;
+	struct lima_pp *pp;
+	char pp_name[] = "pp0", pp_mmu_name[] = "ppmmu0";
+
+	pp = kzalloc(sizeof(*pp), GFP_KERNEL);
+	if (!pp)
+		return -ENOMEM;
+	dev->pp = pp;
+
+	/* Init PP-group L2 cache on Mali450 */
+	if (dev->gpu_type == GPU_MALI450) {
+		pp->l2_cache = kzalloc(sizeof(*pp->l2_cache), GFP_KERNEL);
+		if (!pp->l2_cache)
+			return -ENOMEM;
+		pp->l2_cache->ip.irq = -1;
+		if ((err = lima_init_ip(dev, "pp-l2-cache", &pp->l2_cache->ip, LIMA450_PP03L2_BASE)) ||
+		    (err = lima_l2_cache_init(pp->l2_cache)))
+			return err;
+	}
+
+	for (i = 0; i < dev->num_pp; i++) {
+		struct lima_pp_core *core = pp->core + pp->num_core;
+
+		pp_name[2] = '0' + i; pp_mmu_name[5] = '0' + i;
+
+		if ((err = lima_init_ip(dev, pp_mmu_name, &core->mmu.ip, LIMA_PPMMU_BASE(i))) ||
+		    (err = lima_mmu_init(&core->mmu))) {
+			memset(core, 0, sizeof(*core));
+			continue;
+		}
+
+		if ((err = lima_init_ip(dev, pp_name, &core->ip, LIMA_PP_BASE(i))) ||
+		    (err = lima_pp_core_init(core))) {
+			lima_mmu_fini(&core->mmu);
+			memset(core, 0, sizeof(*core));
+			continue;
+		}
+
+		pp->num_core++;
+	}
+
+	if (pp->num_core != dev->num_pp)
+		dev_warn(dev->dev, "bringup pp %d/%d\n", pp->num_core, dev->num_pp);
+
+	if (pp->num_core == 0)
+		return -ENODEV;
+
+	if ((err = lima_sched_pipe_init(&pp->pipe, "pp")))
+		return err;
+
+	dev->pipe[LIMA_PIPE_PP] = &pp->pipe;
+	for (i = 0; i < pp->num_core; i++)
+		pp->core[i].mmu.pipe = &pp->pipe;
+
+	err = lima_pp_init(pp);
+	if (err)
+		return err;
+
+	return 0;
+}
+
+int lima_device_init(struct lima_device *ldev)
+{
+	int err, i;
+	struct device_node *np;
+	struct resource *res;
+
+	dma_set_coherent_mask(ldev->dev, DMA_BIT_MASK(32));
+
+	np = ldev->dev->of_node;
+
+	err = lima_soc_init(ldev);
+	if (err) {
+		dev_err(ldev->dev, "soc init fail %d\n", err);
+		return err;
+	}
+
+	err = lima_clk_init(ldev);
+	if (err) {
+		dev_err(ldev->dev, "clk init fail %d\n", err);
+		return err;
+	}
+
+	if ((err = lima_regulator_init(ldev))) {
+		return err;
+	}
+
+	ldev->empty_vm = lima_vm_create(ldev);
+	if (!ldev->empty_vm) {
+		err = -ENOMEM;
+		goto err_out;
+	}
+	ldev->va_start = 0;
+	ldev->va_end = 0x100000000;
+
+	res = platform_get_resource(ldev->pdev, IORESOURCE_MEM, 0);
+	ldev->iomem = devm_ioremap_resource(ldev->dev, res);
+	if (IS_ERR(ldev->iomem)) {
+		dev_err(ldev->dev, "fail to ioremap iomem\n");
+	        err = PTR_ERR(ldev->iomem);
+		goto err_out;
+	}
+
+	/* Get the number of PPs */
+	for (i = 0; i < LIMA_MAX_PP; i++) {
+		char pp_name[] = "pp0";
+		pp_name[2] = '0' + i;
+		if (platform_get_irq_byname(ldev->pdev, pp_name) < 0)
+			break;
+	}
+	dev_info(ldev->dev, "found %d PPs\n", i);
+	ldev->num_pp = i;
+
+	ldev->pmu = kzalloc(sizeof(*ldev->pmu), GFP_KERNEL);
+	if (!ldev->pmu) {
+		err = -ENOMEM;
+		goto err_out;
+	}
+
+	/* pmu is optional and not always present */
+	if (lima_init_ip(ldev, "pmu", &ldev->pmu->ip, LIMA_PMU_BASE)) {
+		dev_info(ldev->dev, "no PMU present\n");
+		kfree(ldev->pmu);
+		ldev->pmu = NULL;
+	}
+	else {
+		/* If this value is too low, when in high GPU clk freq,
+		 * GPU will be in unstable state. */
+		if (of_property_read_u32(np, "switch-delay", &ldev->pmu->switch_delay))
+			ldev->pmu->switch_delay = 0xff;
+
+		if ((err = lima_pmu_init(ldev->pmu))) {
+			kfree(ldev->pmu);
+			ldev->pmu = NULL;
+			goto err_out;
+		}
+	}
+
+	if (ldev->gpu_type == GPU_MALI450) {
+		ldev->dlbu = kzalloc(sizeof(*ldev->dlbu), GFP_KERNEL);
+		if (!ldev->dlbu) {
+			err = -ENOMEM;
+			goto err_out;
+		}
+		ldev->dlbu->ip.irq = -1;
+		if ((err = lima_init_ip(ldev, "dlbu", &ldev->dlbu->ip, LIMA_DLBU_BASE)) ||
+		    (err = lima_dlbu_init(ldev->dlbu))) {
+			kfree(ldev->dlbu);
+			ldev->dlbu = NULL;
+			goto err_out;
+		}
+
+	        ldev->bcast = kzalloc(sizeof(*ldev->bcast), GFP_KERNEL);
+		if (!ldev->bcast) {
+			err = -ENOMEM;
+			goto err_out;
+		}
+		if ((err = lima_init_ip(ldev, "pp", &ldev->bcast->ip, LIMA_BCAST_BASE)) ||
+		    (err = lima_bcast_init(ldev->bcast))) {
+			kfree(ldev->bcast);
+			ldev->bcast = NULL;
+			goto err_out;
+		}
+	}
+	else {
+		ldev->l2_cache = kzalloc(sizeof(*ldev->l2_cache), GFP_KERNEL);
+		if (!ldev->l2_cache) {
+			err = -ENOMEM;
+			goto err_out;
+		}
+		ldev->l2_cache->ip.irq = -1;
+		if ((err = lima_init_ip(ldev, "l2-cache", &ldev->l2_cache->ip, LIMA_L2_BASE)) ||
+		    (err = lima_l2_cache_init(ldev->l2_cache))) {
+			kfree(ldev->l2_cache);
+			ldev->l2_cache = NULL;
+			goto err_out;
+		}
+	}
+
+	if ((err = lima_gp_group_init(ldev)))
+		goto err_out;
+
+	if ((err = lima_pp_group_init(ldev)))
+		goto err_out;
+
+	return 0;
+
+err_out:
+	lima_device_fini(ldev);
+	return err;
+}
+
+void lima_device_fini(struct lima_device *ldev)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(ldev->pipe); i++) {
+		if (ldev->pipe[i])
+			lima_sched_pipe_fini(ldev->pipe[i]);
+	}
+
+	if (ldev->pp) {
+		lima_pp_fini(ldev->pp);
+
+		for (i = 0; i < ldev->pp->num_core; i++) {
+			lima_pp_core_fini(ldev->pp->core + i);
+			lima_mmu_fini(&ldev->pp->core[i].mmu);
+		}
+
+		if (ldev->pp->l2_cache) {
+			lima_l2_cache_fini(ldev->pp->l2_cache);
+			kfree(ldev->pp->l2_cache);
+		}
+
+		kfree(ldev->pp);
+	}
+
+	if (ldev->gp) {
+		lima_gp_fini(ldev->gp);
+		lima_mmu_fini(&ldev->gp->mmu);
+
+		if (ldev->gp->l2_cache) {
+			lima_l2_cache_fini(ldev->gp->l2_cache);
+			kfree(ldev->gp->l2_cache);
+		}
+
+		kfree(ldev->gp);
+	}
+
+	if (ldev->bcast) {
+		lima_bcast_fini(ldev->bcast);
+		kfree(ldev->bcast);
+	}
+
+	if (ldev->dlbu) {
+		lima_dlbu_fini(ldev->dlbu);
+		kfree(ldev->dlbu);
+	}
+
+	if (ldev->l2_cache) {
+		lima_l2_cache_fini(ldev->l2_cache);
+		kfree(ldev->l2_cache);
+	}
+
+	if (ldev->pmu) {
+		lima_pmu_fini(ldev->pmu);
+		kfree(ldev->pmu);
+	}
+
+	lima_vm_put(ldev->empty_vm);
+
+	lima_regulator_fini(ldev);
+
+	lima_clk_fini(ldev);
+
+	lima_soc_fini(ldev);
+}
diff --git a/drivers/gpu/drm/lima/lima_dlbu.c b/drivers/gpu/drm/lima/lima_dlbu.c
new file mode 100644
index 0000000..ffd6813
--- /dev/null
+++ b/drivers/gpu/drm/lima/lima_dlbu.c
@@ -0,0 +1,51 @@
+/*
+ * Copyright (C) 2018 Lima Project
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+#include "lima.h"
+#include "lima_dlbu.h"
+
+#define LIMA_DLBU_MASTER_TLLIST_PHYS_ADDR  0x0000
+#define	LIMA_DLBU_MASTER_TLLIST_VADDR      0x0004
+#define	LIMA_DLBU_TLLIST_VBASEADDR         0x0008
+#define	LIMA_DLBU_FB_DIM                   0x000C
+#define	LIMA_DLBU_TLLIST_CONF              0x0010
+#define	LIMA_DLBU_START_TILE_POS           0x0014
+#define	LIMA_DLBU_PP_ENABLE_MASK           0x0018
+
+#define dlbu_write(reg, data) writel(data, dlbu->ip.iomem + LIMA_DLBU_##reg)
+#define dlbu_read(reg) readl(dlbu->ip.iomem + LIMA_DLBU_##reg)
+
+int lima_dlbu_init(struct lima_dlbu *dlbu)
+{
+	struct lima_device *dev = dlbu->ip.dev;
+
+	dev_info(dev->dev, "dlbu %x %x\n",
+		 dlbu_read(MASTER_TLLIST_PHYS_ADDR),
+		 dlbu_read(PP_ENABLE_MASK));
+
+	return 0;
+}
+
+void lima_dlbu_fini(struct lima_dlbu *dlbu)
+{
+	
+}
diff --git a/drivers/gpu/drm/lima/lima_dlbu.h b/drivers/gpu/drm/lima/lima_dlbu.h
new file mode 100644
index 0000000..182cc78
--- /dev/null
+++ b/drivers/gpu/drm/lima/lima_dlbu.h
@@ -0,0 +1,29 @@
+/*
+ * Copyright (C) 2018 Lima Project
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+#ifndef __LIMA_DLBU_H__
+#define __LIMA_DLBU_H__
+
+int lima_dlbu_init(struct lima_dlbu *dlbu);
+void lima_dlbu_fini(struct lima_dlbu *dlbu);
+
+#endif
diff --git a/drivers/gpu/drm/lima/lima_drv.c b/drivers/gpu/drm/lima/lima_drv.c
new file mode 100644
index 0000000..17b4030
--- /dev/null
+++ b/drivers/gpu/drm/lima/lima_drv.c
@@ -0,0 +1,418 @@
+#include <linux/module.h>
+#include <linux/of_platform.h>
+#include <linux/log2.h>
+#include <drm/drm_prime.h>
+
+#include "lima.h"
+#include "lima_gem.h"
+#include "lima_gem_prime.h"
+
+int lima_sched_timeout_ms = 0;
+int lima_sched_max_tasks = 32;
+
+MODULE_PARM_DESC(sched_timeout_ms, "task run timeout in ms (0 = no timeout (default))");
+module_param_named(sched_timeout_ms, lima_sched_timeout_ms, int, 0444);
+
+MODULE_PARM_DESC(sched_max_tasks, "max queued task num in a context (default 32)");
+module_param_named(sched_max_tasks, lima_sched_max_tasks, int, 0444);
+
+struct mali_soc_data {
+	enum lima_gpu_type gpu_type;
+	enum lima_soc_type soc_type;
+};
+
+static inline struct lima_device *to_lima_dev(struct drm_device *dev)
+{
+	return dev->dev_private;
+}
+
+static int lima_ioctl_info(struct drm_device *dev, void *data, struct drm_file *file)
+{
+	struct drm_lima_info *info = data;
+	struct lima_device *ldev = to_lima_dev(dev);
+
+	switch (ldev->gpu_type) {
+	case GPU_MALI400:
+		info->gpu_id = LIMA_INFO_GPU_MALI400;
+		break;
+	case GPU_MALI450:
+		info->gpu_id = LIMA_INFO_GPU_MALI450;
+		break;
+	default:
+		return -ENODEV;
+	}
+	info->num_pp = ldev->pp->num_core;
+	info->va_start = ldev->va_start;
+	info->va_end = ldev->va_end;
+	return 0;
+}
+
+static int lima_ioctl_gem_create(struct drm_device *dev, void *data, struct drm_file *file)
+{
+	struct drm_lima_gem_create *args = data;
+
+	if (args->flags)
+		return -EINVAL;
+
+	if (args->size == 0)
+		return -EINVAL;
+
+	return lima_gem_create_handle(dev, file, args->size, args->flags, &args->handle);
+}
+
+static int lima_ioctl_gem_info(struct drm_device *dev, void *data, struct drm_file *file)
+{
+	struct drm_lima_gem_info *args = data;
+
+	return lima_gem_mmap_offset(file, args->handle, &args->offset);
+}
+
+static int lima_ioctl_gem_va(struct drm_device *dev, void *data, struct drm_file *file)
+{
+	struct drm_lima_gem_va *args = data;
+
+	switch (args->op) {
+	case LIMA_VA_OP_MAP:
+		return lima_gem_va_map(file, args->handle, args->flags, args->va);
+	case LIMA_VA_OP_UNMAP:
+		return lima_gem_va_unmap(file, args->handle, args->va);
+	default:
+		return -EINVAL;
+	}
+}
+
+static int lima_ioctl_gem_submit(struct drm_device *dev, void *data, struct drm_file *file)
+{
+	struct drm_lima_gem_submit_in *args = data;
+	struct lima_device *ldev = to_lima_dev(dev);
+	struct lima_drm_priv *priv = file->driver_priv;
+	struct drm_lima_gem_submit_bo *bos;
+	struct lima_sched_pipe *pipe;
+	struct lima_sched_task *task;
+	struct lima_ctx *ctx;
+	struct lima_submit submit = {0};
+	int err = 0, size;
+
+	if (args->pipe >= LIMA_MAX_PIPE || args->nr_bos == 0)
+		return -EINVAL;
+
+	pipe = ldev->pipe[args->pipe];
+	if (args->frame_size != pipe->frame_size)
+		return -EINVAL;
+
+	size = args->nr_bos * (sizeof(*submit.bos) + sizeof(*submit.lbos));
+	bos = kzalloc(size, GFP_KERNEL);
+	if (!bos)
+		return -ENOMEM;
+
+	size = args->nr_bos * sizeof(*submit.bos);
+	if (copy_from_user(bos, u64_to_user_ptr(args->bos), size)) {
+		err = -EFAULT;
+		goto out0;
+	}
+
+	task = kmem_cache_zalloc(pipe->task_slab, GFP_KERNEL);
+	if (!task) {
+		err = -ENOMEM;
+		goto out0;
+	}
+
+	task->frame = task + 1;
+	if (copy_from_user(task->frame, u64_to_user_ptr(args->frame), args->frame_size)) {
+		err = -EFAULT;
+		goto out1;
+	}
+
+	err = pipe->task_validate(pipe->data, task);
+	if (err)
+		goto out1;
+
+	ctx = lima_ctx_get(&priv->ctx_mgr, args->ctx);
+	if (!ctx) {
+		err = -ENOENT;
+		goto out1;
+	}
+
+	submit.pipe = args->pipe;
+	submit.bos = bos;
+	submit.lbos = (void *)bos + size;
+	submit.nr_bos = args->nr_bos;
+	submit.task = task;
+	submit.ctx = ctx;
+
+	err = lima_gem_submit(file, &submit);
+	if (!err) {
+		struct drm_lima_gem_submit_out *out = data;
+		out->fence = submit.fence;
+		out->done = submit.done;
+	}
+
+	lima_ctx_put(ctx);
+out1:
+	if (err)
+		kmem_cache_free(pipe->task_slab, task);
+out0:
+	kfree(bos);
+	return err;
+}
+
+static int lima_ioctl_wait_fence(struct drm_device *dev, void *data, struct drm_file *file)
+{
+	struct drm_lima_wait_fence *args = data;
+	struct lima_drm_priv *priv = file->driver_priv;
+	struct lima_ctx *ctx;
+	int err;
+
+	if (args->pipe >= LIMA_MAX_PIPE)
+		return -EINVAL;
+
+	ctx = lima_ctx_get(&priv->ctx_mgr, args->ctx);
+	if (!ctx)
+		return -ENOENT;
+
+	err = lima_sched_context_wait_fence(ctx->context + args->pipe,
+					    args->fence, args->timeout_ns);
+
+	lima_ctx_put(ctx);
+	return err;
+}
+
+static int lima_ioctl_gem_wait(struct drm_device *dev, void *data, struct drm_file *file)
+{
+	struct drm_lima_gem_wait *args = data;
+
+	if (!(args->op & (LIMA_GEM_WAIT_READ|LIMA_GEM_WAIT_WRITE)))
+	    return -EINVAL;
+
+	return lima_gem_wait(file, args->handle, args->op, args->timeout_ns);
+}
+
+static int lima_ioctl_ctx(struct drm_device *dev, void *data, struct drm_file *file)
+{
+	struct drm_lima_ctx *args = data;
+	struct lima_drm_priv *priv = file->driver_priv;
+	struct lima_device *ldev = to_lima_dev(dev);
+
+	if (args->op == LIMA_CTX_OP_CREATE)
+		return lima_ctx_create(ldev, &priv->ctx_mgr, &args->id);
+	else if (args->op == LIMA_CTX_OP_FREE)
+		return lima_ctx_free(&priv->ctx_mgr, args->id);
+
+	return -EINVAL;
+}
+
+static int lima_drm_driver_open(struct drm_device *dev, struct drm_file *file)
+{
+	int err;
+	struct lima_drm_priv *priv;
+	struct lima_device *ldev = to_lima_dev(dev);
+
+	priv = kzalloc(sizeof(*priv), GFP_KERNEL);
+	if (!priv)
+		return -ENOMEM;
+
+	priv->vm = lima_vm_create(ldev);
+	if (!priv->vm) {
+		err = -ENOMEM;
+		goto err_out0;
+	}
+
+        lima_ctx_mgr_init(&priv->ctx_mgr);
+
+	file->driver_priv = priv;
+	return 0;
+
+err_out0:
+	kfree(priv);
+	return err;
+}
+
+static void lima_drm_driver_postclose(struct drm_device *dev, struct drm_file *file)
+{
+	struct lima_drm_priv *priv = file->driver_priv;
+
+        lima_ctx_mgr_fini(&priv->ctx_mgr);
+	lima_vm_put(priv->vm);
+	kfree(priv);
+}
+
+static const struct drm_ioctl_desc lima_drm_driver_ioctls[] = {
+	DRM_IOCTL_DEF_DRV(LIMA_INFO, lima_ioctl_info, DRM_AUTH|DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(LIMA_GEM_CREATE, lima_ioctl_gem_create, DRM_AUTH|DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(LIMA_GEM_INFO, lima_ioctl_gem_info, DRM_AUTH|DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(LIMA_GEM_VA, lima_ioctl_gem_va, DRM_AUTH|DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(LIMA_GEM_SUBMIT, lima_ioctl_gem_submit, DRM_AUTH|DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(LIMA_WAIT_FENCE, lima_ioctl_wait_fence, DRM_AUTH|DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(LIMA_GEM_WAIT, lima_ioctl_gem_wait, DRM_AUTH|DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(LIMA_CTX, lima_ioctl_ctx, DRM_AUTH|DRM_RENDER_ALLOW),
+};
+
+extern const struct vm_operations_struct lima_gem_vm_ops;
+
+static const struct file_operations lima_drm_driver_fops = {
+	.owner              = THIS_MODULE,
+	.open               = drm_open,
+	.release            = drm_release,
+	.unlocked_ioctl     = drm_ioctl,
+#ifdef CONFIG_COMPAT
+	.compat_ioctl       = drm_compat_ioctl,
+#endif
+	.mmap               = lima_gem_mmap,
+};
+
+static struct drm_driver lima_drm_driver = {
+	.driver_features    = DRIVER_RENDER | DRIVER_GEM | DRIVER_PRIME,
+	.open               = lima_drm_driver_open,
+	.postclose          = lima_drm_driver_postclose,
+	.ioctls             = lima_drm_driver_ioctls,
+	.num_ioctls         = ARRAY_SIZE(lima_drm_driver_ioctls),
+	.fops               = &lima_drm_driver_fops,
+	.gem_free_object_unlocked = lima_gem_free_object,
+	.gem_open_object    = lima_gem_object_open,
+	.gem_close_object   = lima_gem_object_close,
+	.gem_vm_ops         = &lima_gem_vm_ops,
+	.name               = "lima",
+	.desc               = "lima DRM",
+	.date               = "20170325",
+	.major              = 1,
+	.minor              = 0,
+	.patchlevel         = 0,
+
+	.prime_fd_to_handle = drm_gem_prime_fd_to_handle,
+	.gem_prime_import   = drm_gem_prime_import,
+	.gem_prime_import_sg_table = lima_gem_prime_import_sg_table,
+	.prime_handle_to_fd = drm_gem_prime_handle_to_fd,
+	.gem_prime_export   = drm_gem_prime_export,
+	.gem_prime_res_obj  = lima_gem_prime_res_obj,
+	.gem_prime_get_sg_table = lima_gem_prime_get_sg_table,
+};
+
+static int lima_pdev_probe(struct platform_device *pdev)
+{
+	const struct mali_soc_data *data;
+	struct lima_device *ldev;
+	struct drm_device *ddev;
+	int err;
+
+	ldev = devm_kzalloc(&pdev->dev, sizeof(*ldev), GFP_KERNEL);
+	if (!ldev)
+		return -ENOMEM;
+
+	data = of_device_get_match_data(&pdev->dev);
+
+	ldev->pdev = pdev;
+	ldev->dev = &pdev->dev;
+	ldev->gpu_type = data->gpu_type;
+	ldev->soc_type = data->soc_type;
+
+	platform_set_drvdata(pdev, ldev);
+
+	/* Allocate and initialize the DRM device. */
+	ddev = drm_dev_alloc(&lima_drm_driver, &pdev->dev);
+	if (IS_ERR(ddev))
+		return PTR_ERR(ddev);
+
+	ddev->dev_private = ldev;
+	ldev->ddev = ddev;
+
+	err = lima_device_init(ldev);
+	if (err) {
+		dev_err(&pdev->dev, "Fatal error during GPU init\n");
+		goto err_out0;
+	}
+
+	/*
+	 * Register the DRM device with the core and the connectors with
+	 * sysfs.
+	 */
+	err = drm_dev_register(ddev, 0);
+	if (err < 0)
+		goto err_out1;
+
+	return 0;
+
+err_out1:
+	lima_device_fini(ldev);
+err_out0:
+	drm_dev_unref(ddev);
+	return err;
+}
+
+static int lima_pdev_remove(struct platform_device *pdev)
+{
+	struct lima_device *ldev = platform_get_drvdata(pdev);
+	struct drm_device *ddev = ldev->ddev;
+
+	drm_dev_unregister(ddev);
+	lima_device_fini(ldev);
+	drm_dev_unref(ddev);
+	return 0;
+}
+
+static const struct mali_soc_data mt7623_data = {
+	.gpu_type = GPU_MALI450,
+	.soc_type = SOC_MEDIATEK,
+};
+
+static const struct mali_soc_data generic_mali400_data = {
+	.gpu_type = GPU_MALI400,
+	.soc_type = SOC_GENERIC,
+};
+
+static const struct mali_soc_data generic_mali450_data = {
+	.gpu_type = GPU_MALI450,
+	.soc_type = SOC_GENERIC,
+};
+
+static const struct of_device_id dt_match[] = {
+	{ .compatible = "arm,mali-400", .data = (void *)&generic_mali400_data },
+	{ .compatible = "arm,mali-450", .data = (void *)&generic_mali450_data },
+	{ .compatible = "mediatek,mt7623-mali", .data = (void *)&mt7623_data },
+	{}
+};
+MODULE_DEVICE_TABLE(of, dt_match);
+
+static struct platform_driver lima_platform_driver = {
+	.probe      = lima_pdev_probe,
+	.remove     = lima_pdev_remove,
+	.driver     = {
+		.name   = "lima",
+		.of_match_table = dt_match,
+	},
+};
+
+static void lima_check_module_param(void)
+{
+	if (lima_sched_max_tasks < 4)
+		lima_sched_max_tasks = 4;
+	else
+		lima_sched_max_tasks = roundup_pow_of_two(lima_sched_max_tasks);
+}
+
+static int __init lima_init(void)
+{
+	int ret;
+
+	lima_check_module_param();
+	ret = lima_sched_slab_init();
+	if (ret)
+		return ret;
+
+	ret = platform_driver_register(&lima_platform_driver);
+	if (ret)
+		lima_sched_slab_fini();
+
+	return ret;
+}
+module_init(lima_init);
+
+static void __exit lima_exit(void)
+{
+	platform_driver_unregister(&lima_platform_driver);
+	lima_sched_slab_fini();
+}
+module_exit(lima_exit);
+
+MODULE_AUTHOR("Qiang Yu <yuq825@gmail.com>");
+MODULE_DESCRIPTION("Lima DRM Driver");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/gpu/drm/lima/lima_gem.c b/drivers/gpu/drm/lima/lima_gem.c
new file mode 100644
index 0000000..9f5a539
--- /dev/null
+++ b/drivers/gpu/drm/lima/lima_gem.c
@@ -0,0 +1,555 @@
+#include <drm/drmP.h>
+#include <linux/dma-mapping.h>
+#include <linux/pagemap.h>
+
+#include "lima.h"
+#include "lima_gem.h"
+
+static void lima_bo_shmem_release(struct lima_bo *bo)
+{
+	if (bo->pages_dma_addr) {
+		int i, npages = bo->gem.size >> PAGE_SHIFT;
+
+		for (i = 0; i < npages; i++) {
+			if (bo->pages_dma_addr[i])
+				dma_unmap_page(bo->gem.dev->dev,
+					       bo->pages_dma_addr[i],
+					       PAGE_SIZE, DMA_BIDIRECTIONAL);
+		}
+
+		kfree(bo->pages_dma_addr);
+	}
+
+	if (bo->pages)
+		drm_gem_put_pages(&bo->gem, bo->pages, true, true);
+}
+
+static int lima_bo_shmem_mmap(struct lima_bo *bo, struct vm_area_struct *vma)
+{
+	pgprot_t prot = vm_get_page_prot(vma->vm_flags);
+
+	/* TODO: is it better to just remap_pfn_range all the pages here? */
+
+	vma->vm_flags |= VM_MIXEDMAP;
+	vma->vm_flags &= ~VM_PFNMAP;
+
+	vma->vm_page_prot = pgprot_writecombine(prot);
+	return 0;
+}
+
+static struct lima_bo_ops lima_bo_shmem_ops = {
+	.release = lima_bo_shmem_release,
+	.mmap = lima_bo_shmem_mmap,
+};
+
+struct lima_bo *lima_gem_create_bo(struct drm_device *dev, u32 size, u32 flags)
+{
+	int err;
+	struct lima_bo *bo;
+
+	size = PAGE_ALIGN(size);
+
+	bo = kzalloc(sizeof(*bo), GFP_KERNEL);
+	if (!bo)
+		return ERR_PTR(-ENOMEM);
+
+	mutex_init(&bo->lock);
+	INIT_LIST_HEAD(&bo->va);
+	reservation_object_init(&bo->_resv);
+
+	err = drm_gem_object_init(dev, &bo->gem, size);
+	if (err)
+		goto err_out0;
+
+	return bo;
+
+err_out0:
+	kfree(bo);
+	return ERR_PTR(err);
+}
+
+int lima_gem_create_handle(struct drm_device *dev, struct drm_file *file,
+			   u32 size, u32 flags, u32 *handle)
+{
+	int err, npages, i;
+	struct lima_bo *bo;
+	gfp_t mask;
+
+	bo = lima_gem_create_bo(dev, size, flags);
+	if (IS_ERR(bo))
+		return PTR_ERR(bo);
+
+#if defined(CONFIG_ARM) && !defined(CONFIG_ARM_LPAE)
+	mask = GFP_HIGHUSER;
+#elif defined(CONFIG_ZONE_DMA32)
+	mask = GFP_DMA32;
+#else
+	mask = GFP_DMA;
+#endif
+
+	bo->type = lima_bo_type_shmem;
+	bo->ops = &lima_bo_shmem_ops;
+
+	mapping_set_gfp_mask(bo->gem.filp->f_mapping, mask);
+	bo->pages = drm_gem_get_pages(&bo->gem);
+	if (IS_ERR(bo->pages)) {
+		err = PTR_ERR(bo->pages);
+		bo->pages = NULL;
+		goto err_out;
+	}
+
+	npages = bo->gem.size >> PAGE_SHIFT;
+	bo->pages_dma_addr = kzalloc(npages * sizeof(dma_addr_t), GFP_KERNEL);
+	if (!bo->pages_dma_addr) {
+		err = -ENOMEM;
+		goto err_out;
+	}
+
+	for (i = 0; i < npages; i++) {
+		dma_addr_t addr = dma_map_page(dev->dev, bo->pages[i], 0,
+					       PAGE_SIZE, DMA_BIDIRECTIONAL);
+		if (dma_mapping_error(dev->dev, addr)) {
+			err = -EFAULT;
+			goto err_out;
+		}
+		bo->pages_dma_addr[i] = addr;
+	}
+
+	bo->resv = &bo->_resv;
+
+	err = drm_gem_handle_create(file, &bo->gem, handle);
+
+	/* drop reference from allocate - handle holds it now */
+	drm_gem_object_unreference_unlocked(&bo->gem);
+
+	return err;
+
+err_out:
+	lima_gem_free_object(&bo->gem);
+	return err;
+}
+
+void lima_gem_free_object(struct drm_gem_object *obj)
+{
+	struct lima_bo *bo = to_lima_bo(obj);
+
+	if (!list_empty(&bo->va))
+		dev_err(obj->dev->dev, "lima gem free bo still has va\n");
+
+        bo->ops->release(bo);
+
+	reservation_object_fini(&bo->_resv);
+	drm_gem_object_release(obj);
+	kfree(bo);
+}
+
+static struct lima_bo_va *lima_gem_find_bo_va(struct lima_bo *bo, struct lima_vm *vm)
+{
+	struct lima_bo_va *bo_va, *ret = NULL;
+
+	list_for_each_entry(bo_va, &bo->va, list) {
+		if (bo_va->vm == vm) {
+			ret = bo_va;
+			break;
+		}
+	}
+
+	return ret;
+}
+
+int lima_gem_object_open(struct drm_gem_object *obj, struct drm_file *file)
+{
+	struct lima_bo *bo = to_lima_bo(obj);
+	struct lima_drm_priv *priv = to_lima_drm_priv(file);
+	struct lima_vm *vm = priv->vm;
+	struct lima_bo_va *bo_va;
+	int err = 0;
+
+	mutex_lock(&bo->lock);
+
+	bo_va = lima_gem_find_bo_va(bo, vm);
+	if (bo_va)
+		bo_va->ref_count++;
+	else {
+		bo_va = kmalloc(sizeof(*bo_va), GFP_KERNEL);
+		if (!bo_va) {
+			err = -ENOMEM;
+			goto out;
+		}
+
+		bo_va->vm = vm;
+		bo_va->ref_count = 1;
+		INIT_LIST_HEAD(&bo_va->mapping);
+		list_add_tail(&bo_va->list, &bo->va);
+	}
+
+out:
+	mutex_unlock(&bo->lock);
+	return err;
+}
+
+void lima_gem_object_close(struct drm_gem_object *obj, struct drm_file *file)
+{
+	struct lima_bo *bo = to_lima_bo(obj);
+	struct lima_drm_priv *priv = to_lima_drm_priv(file);
+	struct lima_vm *vm = priv->vm;
+	struct lima_bo_va *bo_va;
+
+	mutex_lock(&bo->lock);
+
+	bo_va = lima_gem_find_bo_va(bo, vm);
+	BUG_ON(!bo_va);
+
+	if (--bo_va->ref_count == 0) {
+		struct lima_bo_va_mapping *mapping, *tmp;
+		list_for_each_entry_safe(mapping, tmp, &bo_va->mapping, list) {
+			lima_vm_unmap(vm, mapping);
+			list_del(&mapping->list);
+			kfree(mapping);
+		}
+		list_del(&bo_va->list);
+		kfree(bo_va);
+	}
+
+	mutex_unlock(&bo->lock);
+}
+
+int lima_gem_mmap_offset(struct drm_file *file, u32 handle, u64 *offset)
+{
+	int err;
+	struct drm_gem_object *obj;
+
+	obj = drm_gem_object_lookup(file, handle);
+	if (!obj)
+		return -ENOENT;
+
+	err = drm_gem_create_mmap_offset(obj);
+	if (!err)
+		*offset = drm_vma_node_offset_addr(&obj->vma_node);
+
+	drm_gem_object_unreference_unlocked(obj);
+	return err;
+}
+
+int lima_gem_mmap(struct file *filp, struct vm_area_struct *vma)
+{
+	int err;
+	struct lima_bo *bo;
+
+	err = drm_gem_mmap(filp, vma);
+	if (err)
+		return err;
+
+	bo = to_lima_bo(vma->vm_private_data);
+
+	err = bo->ops->mmap(bo, vma);
+	if (err)
+		drm_gem_vm_close(vma);
+
+	return err;
+}
+
+static int lima_gem_fault(struct vm_fault *vmf)
+{
+	struct vm_area_struct *vma = vmf->vma;
+	struct lima_bo *bo = to_lima_bo(vma->vm_private_data);
+	unsigned long offset;
+	int err;
+
+	if (!bo->pages)
+		return VM_FAULT_SIGBUS;
+
+	offset = (vmf->address - vma->vm_start) >> PAGE_SHIFT;
+
+	/* TODO:
+	 *   1. use vm_insert_pfn instead of vm_insert_page which
+	 *     will flush dcache (already done when alloc) so is
+	 *     slower than vm_insert_pfn
+	 *   2. insert more pages at once to reduce page fault as
+	 *     GPU buffer will be accessed by CPU continuously and
+	 *     in big blocks
+	 */
+	err = vm_insert_page(vma, vmf->address, bo->pages[offset]);
+	switch (err) {
+	case -EAGAIN:
+	case 0:
+	case -ERESTARTSYS:
+	case -EINTR:
+	case -EBUSY:
+		return VM_FAULT_NOPAGE;
+
+	case -ENOMEM:
+		return VM_FAULT_OOM;
+	}
+
+	return VM_FAULT_SIGBUS;
+}
+
+const struct vm_operations_struct lima_gem_vm_ops = {
+	.fault = lima_gem_fault,
+	.open = drm_gem_vm_open,
+	.close = drm_gem_vm_close,
+};
+
+int lima_gem_va_map(struct drm_file *file, u32 handle, u32 flags, u32 va)
+{
+	struct lima_drm_priv *priv = to_lima_drm_priv(file);
+	struct lima_vm *vm = priv->vm;
+	struct drm_gem_object *obj;
+	struct lima_bo *bo;
+	struct lima_bo_va *bo_va;
+	struct lima_bo_va_mapping *mapping;
+	int err;
+
+	if (!PAGE_ALIGNED(va))
+		return -EINVAL;
+
+	obj = drm_gem_object_lookup(file, handle);
+	if (!obj)
+		return -ENOENT;
+
+	bo = to_lima_bo(obj);
+
+	/* carefully handle overflow when calculate range */
+	if (va < vm->dev->va_start || vm->dev->va_end - obj->size < va) {
+		err = -EINVAL;
+		goto err_out0;
+	}
+
+	mapping = kmalloc(sizeof(*mapping), GFP_KERNEL);
+	if (!mapping) {
+		err = -ENOMEM;
+		goto err_out0;
+	}
+
+	mapping->start = va;
+	mapping->last = va + obj->size - 1;
+
+	mutex_lock(&bo->lock);
+
+	bo_va = lima_gem_find_bo_va(bo, vm);
+	BUG_ON(!bo_va);
+
+	err = lima_vm_map(vm, bo->pages_dma_addr, mapping);
+	if (err)
+		goto err_out1;
+
+	list_add_tail(&mapping->list, &bo_va->mapping);
+
+	mutex_unlock(&bo->lock);
+
+	drm_gem_object_unreference_unlocked(obj);
+	return 0;
+
+err_out1:
+	mutex_unlock(&bo->lock);
+	kfree(mapping);
+err_out0:
+	drm_gem_object_unreference_unlocked(obj);
+	return err;
+}
+
+int lima_gem_va_unmap(struct drm_file *file, u32 handle, u32 va)
+{
+	struct lima_drm_priv *priv = to_lima_drm_priv(file);
+	struct lima_vm *vm = priv->vm;
+	struct drm_gem_object *obj;
+	struct lima_bo *bo;
+	struct lima_bo_va *bo_va;
+	struct lima_bo_va_mapping *mapping;
+
+	obj = drm_gem_object_lookup(file, handle);
+	if (!obj)
+		return -ENOENT;
+
+	bo = to_lima_bo(obj);
+
+	mutex_lock(&bo->lock);
+
+	bo_va = lima_gem_find_bo_va(bo, vm);
+	BUG_ON(!bo_va);
+
+	list_for_each_entry(mapping, &bo_va->mapping, list) {
+		if (mapping->start == va) {
+			lima_vm_unmap(vm, mapping);
+			list_del(&mapping->list);
+			kfree(mapping);
+			break;
+		}
+	}
+
+	mutex_unlock(&bo->lock);
+
+	drm_gem_object_unreference_unlocked(obj);
+	return 0;
+}
+
+static int lima_gem_lock_bos(struct lima_bo **bos, u32 nr_bos,
+			     struct ww_acquire_ctx *ctx)
+{
+        int i, ret = 0, contended, slow_locked = -1;
+
+	ww_acquire_init(ctx, &reservation_ww_class);
+
+retry:
+	for (i = 0; i < nr_bos; i++) {
+		if (i == slow_locked) {
+			slow_locked = -1;
+			continue;
+		}
+
+		ret = ww_mutex_lock_interruptible(&bos[i]->resv->lock, ctx);
+		if (ret < 0) {
+			contended = i;
+			goto err;
+		}
+	}
+
+	ww_acquire_done(ctx);
+	return 0;
+
+err:
+	for (i--; i >= 0; i--)
+		ww_mutex_unlock(&bos[i]->resv->lock);
+
+	if (slow_locked >= 0)
+		ww_mutex_unlock(&bos[slow_locked]->resv->lock);
+
+	if (ret == -EDEADLK) {
+		/* we lost out in a seqno race, lock and retry.. */
+		ret = ww_mutex_lock_slow_interruptible(&bos[contended]->resv->lock, ctx);
+		if (!ret) {
+			slow_locked = contended;
+			goto retry;
+		}
+	}
+	ww_acquire_fini(ctx);
+
+	return ret;
+}
+
+static int lima_gem_sync_bo(struct lima_sched_task *task, struct lima_bo *bo, bool write)
+{
+	int i, err;
+	struct dma_fence *f;
+	u64 context = task->base.s_fence->finished.context;
+
+	if (write) {
+		struct reservation_object_list *fobj =
+			reservation_object_get_list(bo->resv);
+
+		if (fobj && fobj->shared_count > 0) {
+			for (i = 0; i < fobj->shared_count; i++) {
+				f = rcu_dereference_protected(
+					fobj->shared[i], reservation_object_held(bo->resv));
+				if (f->context != context) {
+					err = lima_sched_task_add_dep(task, f);
+					if (err)
+						return err;
+				}
+			}
+		}
+	}
+
+	f = reservation_object_get_excl(bo->resv);
+	if (f) {
+		err = lima_sched_task_add_dep(task, f);
+		if (err)
+			return err;
+	}
+
+	if (!write) {
+		err = reservation_object_reserve_shared(bo->resv);
+		if (err)
+			return err;
+	}
+
+	return 0;
+}
+
+int lima_gem_submit(struct drm_file *file, struct lima_submit *submit)
+{
+	int i, err = 0;
+	struct ww_acquire_ctx ctx;
+	struct lima_drm_priv *priv = to_lima_drm_priv(file);
+
+	for (i = 0; i < submit->nr_bos; i++) {
+		struct drm_gem_object *obj;
+
+		obj = drm_gem_object_lookup(file, submit->bos[i].handle);
+		if (!obj) {
+			err = -ENOENT;
+			goto out0;
+		}
+		submit->lbos[i] = to_lima_bo(obj);
+	}
+
+	err = lima_gem_lock_bos(submit->lbos, submit->nr_bos, &ctx);
+	if (err)
+		goto out0;
+
+	err = lima_sched_task_init(
+		submit->task, submit->ctx->context + submit->pipe, priv->vm);
+	if (err)
+		goto out1;
+
+	for (i = 0; i < submit->nr_bos; i++) {
+		err = lima_gem_sync_bo(submit->task, submit->lbos[i],
+				       submit->bos[i].flags & LIMA_SUBMIT_BO_WRITE);
+		if (err)
+			goto out2;
+	}
+
+	for (i = 0; i < submit->nr_bos; i++) {
+		if (submit->bos[i].flags & LIMA_SUBMIT_BO_WRITE)
+			reservation_object_add_excl_fence(
+				submit->lbos[i]->resv, &submit->task->base.s_fence->finished);
+		else
+			reservation_object_add_shared_fence(
+				submit->lbos[i]->resv, &submit->task->base.s_fence->finished);
+	}
+
+	submit->fence = lima_sched_context_queue_task(
+		submit->ctx->context + submit->pipe, submit->task, &submit->done);
+
+out2:
+	if (err)
+		lima_sched_task_fini(submit->task);
+out1:
+	for (i = 0; i < submit->nr_bos; i++)
+		ww_mutex_unlock(&submit->lbos[i]->resv->lock);
+	ww_acquire_fini(&ctx);
+out0:
+	for (i = 0; i < submit->nr_bos && submit->lbos[i]; i++)
+		drm_gem_object_unreference_unlocked(&submit->lbos[i]->gem);
+	return err;
+}
+
+int lima_gem_wait(struct drm_file *file, u32 handle, u32 op, u64 timeout_ns)
+{
+	bool write = op & LIMA_GEM_WAIT_WRITE;
+	struct drm_gem_object *obj;
+	struct lima_bo *bo;
+	signed long ret;
+	unsigned long timeout;
+
+	obj = drm_gem_object_lookup(file, handle);
+	if (!obj)
+		return -ENOENT;
+
+	bo = to_lima_bo(obj);
+
+	timeout = timeout_ns ? lima_timeout_to_jiffies(timeout_ns) : 0;
+
+	/* must use long for result check because in 64bit arch int
+	 * will overflow if timeout is too large and get <0 result
+	 */
+	ret = reservation_object_wait_timeout_rcu(bo->resv, write, true, timeout);
+	if (ret == 0)
+		ret = timeout ? -ETIMEDOUT : -EBUSY;
+	else if (ret > 0)
+		ret = 0;
+
+	drm_gem_object_unreference_unlocked(obj);
+	return ret;
+}
diff --git a/drivers/gpu/drm/lima/lima_gem.h b/drivers/gpu/drm/lima/lima_gem.h
new file mode 100644
index 0000000..0bc74ef
--- /dev/null
+++ b/drivers/gpu/drm/lima/lima_gem.h
@@ -0,0 +1,86 @@
+/*
+ * Copyright (C) 2018 Lima Project
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ */
+#ifndef __LIMA_GEM_H__
+#define __LIMA_GEM_H__
+
+#include <drm/drm_gem.h>
+#include <linux/reservation.h>
+
+struct lima_bo;
+struct lima_submit;
+
+struct lima_bo_va {
+	struct list_head list;
+	unsigned ref_count;
+
+	struct list_head mapping;
+
+	struct lima_vm *vm;
+};
+
+struct lima_bo_ops {
+	void (*release)(struct lima_bo *);
+	int (*mmap)(struct lima_bo *, struct vm_area_struct *);
+};
+
+struct lima_bo {
+	struct drm_gem_object gem;
+
+	enum lima_bo_type {
+		lima_bo_type_shmem,
+		lima_bo_type_dma_buf,
+	} type;
+
+	struct page **pages;
+	dma_addr_t *pages_dma_addr;	
+	struct sg_table *sgt;
+
+	struct lima_bo_ops *ops;
+
+	struct mutex lock;
+	struct list_head va;
+
+	/* normally (resv == &_resv) except for imported bo's */
+	struct reservation_object *resv;
+	struct reservation_object _resv;
+};
+
+static inline struct lima_bo *
+to_lima_bo(struct drm_gem_object *obj)
+{
+	return container_of(obj, struct lima_bo, gem);
+}
+
+struct lima_bo *lima_gem_create_bo(struct drm_device *dev, u32 size, u32 flags);
+int lima_gem_create_handle(struct drm_device *dev, struct drm_file *file,
+			   u32 size, u32 flags, u32 *handle);
+void lima_gem_free_object(struct drm_gem_object *obj);
+int lima_gem_object_open(struct drm_gem_object *obj, struct drm_file *file);
+void lima_gem_object_close(struct drm_gem_object *obj, struct drm_file *file);
+int lima_gem_mmap_offset(struct drm_file *file, u32 handle, u64 *offset);
+int lima_gem_mmap(struct file *filp, struct vm_area_struct *vma);
+int lima_gem_va_map(struct drm_file *file, u32 handle, u32 flags, u32 va);
+int lima_gem_va_unmap(struct drm_file *file, u32 handle, u32 va);
+int lima_gem_submit(struct drm_file *file, struct lima_submit *submit);
+int lima_gem_wait(struct drm_file *file, u32 handle, u32 op, u64 timeout_ns);
+
+#endif
diff --git a/drivers/gpu/drm/lima/lima_gem_prime.c b/drivers/gpu/drm/lima/lima_gem_prime.c
new file mode 100644
index 0000000..f2346d5
--- /dev/null
+++ b/drivers/gpu/drm/lima/lima_gem_prime.c
@@ -0,0 +1,106 @@
+/*
+ * Copyright (C) 2018 Lima Project
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+#include <drm/drmP.h>
+#include <linux/dma-buf.h>
+
+#include "lima_gem.h"
+#include "lima_gem_prime.h"
+
+static void lima_bo_dma_buf_release(struct lima_bo *bo)
+{
+	if (bo->pages_dma_addr)
+		kfree(bo->pages_dma_addr);
+
+	if (bo->pages)
+		kfree(bo->pages);
+
+	drm_prime_gem_destroy(&bo->gem, bo->sgt);
+}
+
+static int lima_bo_dma_buf_mmap(struct lima_bo *bo, struct vm_area_struct *vma)
+{
+	return dma_buf_mmap(bo->gem.dma_buf, vma, 0);
+}
+
+static struct lima_bo_ops lima_bo_dma_buf_ops = {
+	.release = lima_bo_dma_buf_release,
+	.mmap = lima_bo_dma_buf_mmap,
+};
+
+struct drm_gem_object *lima_gem_prime_import_sg_table(
+	struct drm_device *dev, struct dma_buf_attachment *attach,
+	struct sg_table *sgt)
+{
+	struct lima_bo *bo;
+	struct drm_gem_object *ret;
+	int err, npages = attach->dmabuf->size >> PAGE_SHIFT;
+
+	bo = lima_gem_create_bo(dev, attach->dmabuf->size, 0);
+	if (!bo)
+		return ERR_PTR(-ENOMEM);
+
+	bo->type = lima_bo_type_dma_buf;
+	bo->ops = &lima_bo_dma_buf_ops;
+	bo->sgt = sgt;
+
+	bo->pages_dma_addr = kzalloc(npages * sizeof(dma_addr_t), GFP_KERNEL);
+	if (!bo->pages_dma_addr) {
+		ret = ERR_PTR(-ENOMEM);
+		goto err_out;
+	}
+
+	bo->pages = kzalloc(npages * sizeof(*bo->pages), GFP_KERNEL);
+	if (!bo->pages) {
+		ret = ERR_PTR(-ENOMEM);
+		goto err_out;
+	}
+
+	err = drm_prime_sg_to_page_addr_arrays(
+		sgt, bo->pages, bo->pages_dma_addr, npages);
+	if (err) {
+		ret = ERR_PTR(err);
+		goto err_out;
+	}
+
+	bo->resv = attach->dmabuf->resv;
+
+	return &bo->gem;
+
+err_out:
+	lima_gem_free_object(&bo->gem);
+	return ret;
+}
+
+struct reservation_object *lima_gem_prime_res_obj(struct drm_gem_object *obj)
+{
+        struct lima_bo *bo = to_lima_bo(obj);
+
+	return bo->resv;
+}
+
+struct sg_table *lima_gem_prime_get_sg_table(struct drm_gem_object *obj)
+{
+	struct lima_bo *bo = to_lima_bo(obj);
+
+	return drm_prime_pages_to_sg(bo->pages, obj->size >> PAGE_SHIFT);
+}
diff --git a/drivers/gpu/drm/lima/lima_gem_prime.h b/drivers/gpu/drm/lima/lima_gem_prime.h
new file mode 100644
index 0000000..a1f2a4b
--- /dev/null
+++ b/drivers/gpu/drm/lima/lima_gem_prime.h
@@ -0,0 +1,31 @@
+/*
+ * Copyright (C) 2018 Lima Project
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ */
+#ifndef __LIMA_GEM_PRIME_H__
+#define __LIMA_GEM_PRIME_H__
+
+struct drm_gem_object *lima_gem_prime_import_sg_table(
+	struct drm_device *dev, struct dma_buf_attachment *attach,
+	struct sg_table *sgt);
+struct sg_table *lima_gem_prime_get_sg_table(struct drm_gem_object *obj);
+struct reservation_object *lima_gem_prime_res_obj(struct drm_gem_object *obj);
+
+#endif
diff --git a/drivers/gpu/drm/lima/lima_gp.c b/drivers/gpu/drm/lima/lima_gp.c
new file mode 100644
index 0000000..b364d1f
--- /dev/null
+++ b/drivers/gpu/drm/lima/lima_gp.c
@@ -0,0 +1,356 @@
+#include "lima.h"
+
+#define LIMA_GP_VSCL_START_ADDR                0x00
+#define LIMA_GP_VSCL_END_ADDR                  0x04
+#define LIMA_GP_PLBUCL_START_ADDR              0x08
+#define LIMA_GP_PLBUCL_END_ADDR                0x0c
+#define LIMA_GP_PLBU_ALLOC_START_ADDR          0x10
+#define LIMA_GP_PLBU_ALLOC_END_ADDR            0x14
+#define LIMA_GP_CMD                            0x20
+#define   LIMA_GP_CMD_START_VS                 (1 << 0)
+#define   LIMA_GP_CMD_START_PLBU               (1 << 1)
+#define   LIMA_GP_CMD_UPDATE_PLBU_ALLOC        (1 << 4)
+#define   LIMA_GP_CMD_RESET                    (1 << 5)
+#define   LIMA_GP_CMD_FORCE_HANG               (1 << 6)
+#define   LIMA_GP_CMD_STOP_BUS                 (1 << 9)
+#define   LIMA_GP_CMD_SOFT_RESET               (1 << 10)
+#define LIMA_GP_INT_RAWSTAT                    0x24
+#define LIMA_GP_INT_CLEAR                      0x28
+#define LIMA_GP_INT_MASK                       0x2C
+#define LIMA_GP_INT_STAT                       0x30
+#define   LIMA_GP_IRQ_VS_END_CMD_LST           (1 << 0)
+#define   LIMA_GP_IRQ_PLBU_END_CMD_LST         (1 << 1)
+#define   LIMA_GP_IRQ_PLBU_OUT_OF_MEM          (1 << 2)
+#define   LIMA_GP_IRQ_VS_SEM_IRQ               (1 << 3)
+#define   LIMA_GP_IRQ_PLBU_SEM_IRQ             (1 << 4)
+#define   LIMA_GP_IRQ_HANG                     (1 << 5)
+#define   LIMA_GP_IRQ_FORCE_HANG               (1 << 6)
+#define   LIMA_GP_IRQ_PERF_CNT_0_LIMIT         (1 << 7)
+#define   LIMA_GP_IRQ_PERF_CNT_1_LIMIT         (1 << 8)
+#define   LIMA_GP_IRQ_WRITE_BOUND_ERR          (1 << 9)
+#define   LIMA_GP_IRQ_SYNC_ERROR               (1 << 10)
+#define   LIMA_GP_IRQ_AXI_BUS_ERROR            (1 << 11)
+#define   LIMA_GP_IRQ_AXI_BUS_STOPPED          (1 << 12)
+#define   LIMA_GP_IRQ_VS_INVALID_CMD           (1 << 13)
+#define   LIMA_GP_IRQ_PLB_INVALID_CMD          (1 << 14)
+#define   LIMA_GP_IRQ_RESET_COMPLETED          (1 << 19)
+#define   LIMA_GP_IRQ_SEMAPHORE_UNDERFLOW      (1 << 20)
+#define   LIMA_GP_IRQ_SEMAPHORE_OVERFLOW       (1 << 21)
+#define   LIMA_GP_IRQ_PTR_ARRAY_OUT_OF_BOUNDS  (1 << 22)
+#define LIMA_GP_WRITE_BOUND_LOW                0x34
+#define LIMA_GP_PERF_CNT_0_ENABLE              0x3C
+#define LIMA_GP_PERF_CNT_1_ENABLE              0x40
+#define LIMA_GP_PERF_CNT_0_SRC                 0x44
+#define LIMA_GP_PERF_CNT_1_SRC                 0x48
+#define LIMA_GP_PERF_CNT_0_VALUE               0x4C
+#define LIMA_GP_PERF_CNT_1_VALUE               0x50
+#define LIMA_GP_PERF_CNT_0_LIMIT               0x54
+#define LIMA_GP_STATUS                         0x68
+#define   LIMA_GP_STATUS_VS_ACTIVE             (1 << 1)
+#define   LIMA_GP_STATUS_BUS_STOPPED	       (1 << 2)
+#define	  LIMA_GP_STATUS_PLBU_ACTIVE	       (1 << 3)
+#define	  LIMA_GP_STATUS_BUS_ERROR	       (1 << 6)
+#define	  LIMA_GP_STATUS_WRITE_BOUND_ERR       (1 << 8)
+#define LIMA_GP_VERSION                        0x6C
+#define LIMA_GP_VSCL_START_ADDR_READ           0x80
+#define LIMA_GP_PLBCL_START_ADDR_READ          0x84
+#define LIMA_GP_CONTR_AXI_BUS_ERROR_STAT       0x94
+
+#define LIMA_GP_IRQ_MASK_ALL		   \
+	(				   \
+	 LIMA_GP_IRQ_VS_END_CMD_LST      | \
+	 LIMA_GP_IRQ_PLBU_END_CMD_LST    | \
+	 LIMA_GP_IRQ_PLBU_OUT_OF_MEM     | \
+	 LIMA_GP_IRQ_VS_SEM_IRQ          | \
+	 LIMA_GP_IRQ_PLBU_SEM_IRQ        | \
+	 LIMA_GP_IRQ_HANG                | \
+	 LIMA_GP_IRQ_FORCE_HANG          | \
+	 LIMA_GP_IRQ_PERF_CNT_0_LIMIT    | \
+	 LIMA_GP_IRQ_PERF_CNT_1_LIMIT    | \
+	 LIMA_GP_IRQ_WRITE_BOUND_ERR     | \
+	 LIMA_GP_IRQ_SYNC_ERROR          | \
+	 LIMA_GP_IRQ_AXI_BUS_ERROR       | \
+	 LIMA_GP_IRQ_AXI_BUS_STOPPED     | \
+	 LIMA_GP_IRQ_VS_INVALID_CMD      | \
+	 LIMA_GP_IRQ_PLB_INVALID_CMD     | \
+	 LIMA_GP_IRQ_RESET_COMPLETED     | \
+	 LIMA_GP_IRQ_SEMAPHORE_UNDERFLOW | \
+	 LIMA_GP_IRQ_SEMAPHORE_OVERFLOW  | \
+	 LIMA_GP_IRQ_PTR_ARRAY_OUT_OF_BOUNDS)
+
+#define LIMA_GP_IRQ_MASK_ERROR             \
+	(                                  \
+	 LIMA_GP_IRQ_PLBU_OUT_OF_MEM     | \
+	 LIMA_GP_IRQ_FORCE_HANG          | \
+	 LIMA_GP_IRQ_WRITE_BOUND_ERR     | \
+	 LIMA_GP_IRQ_SYNC_ERROR          | \
+	 LIMA_GP_IRQ_AXI_BUS_ERROR       | \
+	 LIMA_GP_IRQ_VS_INVALID_CMD      | \
+	 LIMA_GP_IRQ_PLB_INVALID_CMD     | \
+	 LIMA_GP_IRQ_SEMAPHORE_UNDERFLOW | \
+	 LIMA_GP_IRQ_SEMAPHORE_OVERFLOW  | \
+	 LIMA_GP_IRQ_PTR_ARRAY_OUT_OF_BOUNDS)
+
+#define LIMA_GP_IRQ_MASK_USED		   \
+	(				   \
+	 LIMA_GP_IRQ_VS_END_CMD_LST      | \
+	 LIMA_GP_IRQ_PLBU_END_CMD_LST    | \
+	 LIMA_GP_IRQ_MASK_ERROR)
+
+
+#define gp_write(reg, data) writel(data, gp->ip.iomem + LIMA_GP_##reg)
+#define gp_read(reg) readl(gp->ip.iomem + LIMA_GP_##reg)
+
+static irqreturn_t lima_gp_irq_handler(int irq, void *data)
+{
+	struct lima_gp *gp = data;
+	struct lima_device *dev = gp->ip.dev;
+	u32 state = gp_read(INT_STAT);
+	bool task_done = false, fail = false;
+
+	/* for shared irq case */
+	if (!state)
+		return IRQ_NONE;
+
+	if (state & LIMA_GP_IRQ_MASK_ERROR) {
+		u32 status = gp_read(STATUS);
+
+		dev_err(dev->dev, "gp error irq state=%x status=%x\n",
+			state, status);
+
+		fail = true;
+		task_done = true;
+
+		/* mask all interrupts before hard reset */
+		gp_write(INT_MASK, 0);
+	}
+	else {
+		if (state & LIMA_GP_IRQ_VS_END_CMD_LST) {
+			gp->task &= ~LIMA_GP_TASK_VS;
+			task_done = true;
+		}
+
+		if (state & LIMA_GP_IRQ_PLBU_END_CMD_LST) {
+			gp->task &= ~LIMA_GP_TASK_PLBU;
+			task_done = true;
+		}
+	}
+
+	gp_write(INT_CLEAR, state);
+
+	if (task_done) {
+		if (fail)
+			lima_sched_pipe_task_done(&gp->pipe, true);
+		else if (!gp->task)
+			lima_sched_pipe_task_done(&gp->pipe, false);
+	}
+	return IRQ_HANDLED;
+}
+
+static void lima_gp_soft_reset_async(struct lima_gp *gp)
+{
+	if (gp->async_reset)
+		return;
+
+	gp_write(INT_MASK, 0);
+	gp_write(INT_CLEAR, LIMA_GP_IRQ_RESET_COMPLETED);
+	gp_write(CMD, LIMA_GP_CMD_SOFT_RESET);
+	gp->async_reset = true;
+}
+
+static int lima_gp_soft_reset_async_wait(struct lima_gp *gp)
+{
+	struct lima_device *dev = gp->ip.dev;
+	int timeout;
+
+	if (!gp->async_reset)
+		return 0;
+
+	for (timeout = 1000; timeout > 0; timeout--) {
+		if (gp_read(INT_RAWSTAT) & LIMA_GP_IRQ_RESET_COMPLETED)
+			break;
+	}
+	if (!timeout) {
+		dev_err(dev->dev, "gp soft reset time out\n");
+		return -ETIMEDOUT;
+	}
+
+	gp_write(INT_CLEAR, LIMA_GP_IRQ_MASK_ALL);
+	gp_write(INT_MASK, LIMA_GP_IRQ_MASK_USED);
+
+	gp->async_reset = false;
+	return 0;
+}
+
+static int lima_gp_task_validate(void *data, struct lima_sched_task *task)
+{
+	struct drm_lima_m400_gp_frame *f = task->frame;
+	(void)data;
+
+	if (f->vs_cmd_start > f->vs_cmd_end ||
+	    f->plbu_cmd_start > f->plbu_cmd_end ||
+	    f->tile_heap_start > f->tile_heap_end)
+		return -EINVAL;
+
+	if (f->vs_cmd_start == f->vs_cmd_end &&
+	    f->plbu_cmd_start == f->plbu_cmd_end)
+		return -EINVAL;
+
+	return 0;
+}
+
+static void lima_gp_task_run(void *data, struct lima_sched_task *task)
+{
+	struct lima_gp *gp = data;
+	struct drm_lima_m400_gp_frame *frame = task->frame;
+	u32 cmd = 0;
+
+	gp->task = 0;
+	if (frame->vs_cmd_start != frame->vs_cmd_end) {
+		cmd |= LIMA_GP_CMD_START_VS;
+		gp->task |= LIMA_GP_TASK_VS;
+	}
+	if (frame->plbu_cmd_start != frame->plbu_cmd_end) {
+		cmd |= LIMA_GP_CMD_START_PLBU;
+		gp->task |= LIMA_GP_TASK_PLBU;
+	}
+
+	/* before any hw ops, wait last success task async soft reset */
+	lima_gp_soft_reset_async_wait(gp);
+
+	gp_write(VSCL_START_ADDR, frame->vs_cmd_start);
+	gp_write(VSCL_END_ADDR, frame->vs_cmd_end);
+	gp_write(PLBUCL_START_ADDR, frame->plbu_cmd_start);
+	gp_write(PLBUCL_END_ADDR, frame->plbu_cmd_end);
+	gp_write(PLBU_ALLOC_START_ADDR, frame->tile_heap_start);
+	gp_write(PLBU_ALLOC_END_ADDR, frame->tile_heap_end);
+
+	gp_write(CMD, LIMA_GP_CMD_UPDATE_PLBU_ALLOC);
+	gp_write(CMD, cmd);
+}
+
+static int lima_gp_hard_reset(struct lima_gp *gp)
+{
+	struct lima_device *dev = gp->ip.dev;
+	int timeout;
+
+	gp_write(PERF_CNT_0_LIMIT, 0xC0FFE000);
+	gp_write(INT_MASK, 0);
+	gp_write(CMD, LIMA_GP_CMD_RESET);
+	for (timeout = 1000; timeout > 0; timeout--) {
+		gp_write(PERF_CNT_0_LIMIT, 0xC01A0000);
+		if (gp_read(PERF_CNT_0_LIMIT) == 0xC01A0000)
+			break;
+	}
+	if (!timeout) {
+		dev_err(dev->dev, "gp hard reset timeout\n");
+		return -ETIMEDOUT;
+	}
+
+	gp_write(PERF_CNT_0_LIMIT, 0);
+	gp_write(INT_CLEAR, LIMA_GP_IRQ_MASK_ALL);
+	gp_write(INT_MASK, LIMA_GP_IRQ_MASK_USED);
+	return 0;
+}
+
+static void lima_gp_task_fini(void *data)
+{
+	lima_gp_soft_reset_async(data);
+}
+
+static void lima_gp_task_error(void *data)
+{
+	lima_gp_hard_reset(data);
+}
+
+static void lima_gp_task_mmu_error(void *data)
+{
+	struct lima_gp *gp = data;
+
+	lima_sched_pipe_task_done(&gp->pipe, true);
+}
+
+static void lima_gp_print_version(struct lima_gp *gp)
+{
+	u32 version, major, minor;
+	char *name;
+
+	version = gp_read(VERSION);
+	major = (version >> 8) & 0xFF;
+	minor = version & 0xFF;
+	switch (version >> 16) {
+	case 0xA07:
+	    name = "mali200";
+		break;
+	case 0xC07:
+		name = "mali300";
+		break;
+	case 0xB07:
+		name = "mali400";
+		break;
+	case 0xD07:
+		name = "mali450";
+		break;
+	default:
+		name = "unknow";
+		break;
+	}
+	dev_info(gp->ip.dev->dev, "%s - %s version major %d minor %d\n",
+		 gp->ip.name, name, major, minor);
+}
+
+static struct kmem_cache *lima_gp_task_slab = NULL;
+static int lima_gp_task_slab_refcnt = 0;
+
+int lima_gp_init(struct lima_gp *gp)
+{
+	struct lima_device *dev = gp->ip.dev;
+	int err, frame_size;
+
+	lima_gp_print_version(gp);
+
+	gp->async_reset = false;
+	lima_gp_soft_reset_async(gp);
+	err = lima_gp_soft_reset_async_wait(gp);
+	if (err)
+		return err;
+
+	err = devm_request_irq(dev->dev, gp->ip.irq, lima_gp_irq_handler, 0,
+			       gp->ip.name, gp);
+	if (err) {
+		dev_err(dev->dev, "gp %s fail to request irq\n", gp->ip.name);
+		return err;
+	}
+
+	frame_size = sizeof(struct drm_lima_m400_gp_frame);
+	if (!lima_gp_task_slab) {
+		lima_gp_task_slab = kmem_cache_create(
+			"lima_gp_task", sizeof(struct lima_sched_task) + frame_size,
+			0, SLAB_HWCACHE_ALIGN, NULL);
+		if (!lima_gp_task_slab)
+			return -ENOMEM;
+	}
+	lima_gp_task_slab_refcnt++;
+
+	gp->pipe.frame_size = frame_size;
+	gp->pipe.task_slab = lima_gp_task_slab;
+
+	gp->pipe.task_validate = lima_gp_task_validate;
+	gp->pipe.task_run = lima_gp_task_run;
+	gp->pipe.task_fini = lima_gp_task_fini;
+	gp->pipe.task_error = lima_gp_task_error;
+	gp->pipe.task_mmu_error = lima_gp_task_mmu_error;
+	gp->pipe.data = gp;
+
+	gp->pipe.mmu[0] = &gp->mmu;
+	gp->pipe.num_mmu = 1;
+	return 0;
+}
+
+void lima_gp_fini(struct lima_gp *gp)
+{
+	if (!--lima_gp_task_slab_refcnt) {
+		kmem_cache_destroy(lima_gp_task_slab);
+		lima_gp_task_slab = NULL;
+	}
+}
diff --git a/drivers/gpu/drm/lima/lima_l2_cache.c b/drivers/gpu/drm/lima/lima_l2_cache.c
new file mode 100644
index 0000000..397a66e
--- /dev/null
+++ b/drivers/gpu/drm/lima/lima_l2_cache.c
@@ -0,0 +1,70 @@
+#include "lima.h"
+
+#define LIMA_L2_CACHE_SIZE		 0x0004
+#define LIMA_L2_CACHE_STATUS		 0x0008
+#define   LIMA_L2_CACHE_STATUS_COMMAND_BUSY  (1 << 0)
+#define   LIMA_L2_CACHE_STATUS_DATA_BUSY     (1 << 1)
+#define LIMA_L2_CACHE_COMMAND		 0x0010
+#define   LIMA_L2_CACHE_COMMAND_CLEAR_ALL    (1 << 0)
+#define LIMA_L2_CACHE_CLEAR_PAGE	 0x0014
+#define LIMA_L2_CACHE_MAX_READS		 0x0018
+#define LIMA_L2_CACHE_ENABLE		 0x001C
+#define   LIMA_L2_CACHE_ENABLE_ACCESS        (1 << 0)
+#define   LIMA_L2_CACHE_ENABLE_READ_ALLOCATE (1 << 1)
+#define LIMA_L2_CACHE_PERFCNT_SRC0	 0x0020
+#define LIMA_L2_CACHE_PERFCNT_VAL0	 0x0024
+#define LIMA_L2_CACHE_PERFCNT_SRC1	 0x0028
+#define LIMA_L2_CACHE_ERFCNT_VAL1	 0x002C
+
+#define l2_cache_write(reg, data) writel(data, l2_cache->ip.iomem + LIMA_L2_CACHE_##reg)
+#define l2_cache_read(reg) readl(l2_cache->ip.iomem + LIMA_L2_CACHE_##reg)
+
+static int lima_l2_cache_wait_idle(struct lima_l2_cache *l2_cache)
+{
+	int timeout;
+	struct lima_device *dev = l2_cache->ip.dev;
+
+	for (timeout = 100000; timeout > 0; timeout--) {
+	    if (!(l2_cache_read(STATUS) & LIMA_L2_CACHE_STATUS_COMMAND_BUSY))
+		break;
+	}
+	if (!timeout) {
+	    dev_err(dev->dev, "l2 cache wait command timeout\n");
+	    return -ETIMEDOUT;
+	}
+	return 0;
+}
+
+int lima_l2_cache_flush(struct lima_l2_cache *l2_cache)
+{
+	l2_cache_write(COMMAND, LIMA_L2_CACHE_COMMAND_CLEAR_ALL);
+	return lima_l2_cache_wait_idle(l2_cache);
+}
+
+int lima_l2_cache_init(struct lima_l2_cache *l2_cache)
+{
+	int err;
+	u32 size;
+	struct lima_device *dev = l2_cache->ip.dev;
+
+	size = l2_cache_read(SIZE);
+	dev_info(dev->dev, "l2 cache %uK, %u-way, %ubyte cache line, %ubit external bus\n",
+		 1 << (((size >> 16) & 0xff) - 10),
+		 1 << ((size >> 8) & 0xff),
+		 1 << (size & 0xff),
+		 1 << ((size >> 24) & 0xff));
+
+	err = lima_l2_cache_flush(l2_cache);
+	if (err)
+		return err;
+
+	l2_cache_write(ENABLE, LIMA_L2_CACHE_ENABLE_ACCESS | LIMA_L2_CACHE_ENABLE_READ_ALLOCATE);
+	l2_cache_write(MAX_READS, 0x1c);
+
+	return 0;
+}
+
+void lima_l2_cache_fini(struct lima_l2_cache *l2_cache)
+{
+
+}
diff --git a/drivers/gpu/drm/lima/lima_mmu.c b/drivers/gpu/drm/lima/lima_mmu.c
new file mode 100644
index 0000000..dcada87
--- /dev/null
+++ b/drivers/gpu/drm/lima/lima_mmu.c
@@ -0,0 +1,197 @@
+#include <linux/interrupt.h>
+
+#include "lima.h"
+
+#define LIMA_MMU_DTE_ADDR		  0x0000
+#define LIMA_MMU_STATUS			  0x0004
+#define   LIMA_MMU_STATUS_PAGING_ENABLED      (1 << 0)
+#define   LIMA_MMU_STATUS_PAGE_FAULT_ACTIVE   (1 << 1)
+#define   LIMA_MMU_STATUS_STALL_ACTIVE        (1 << 2)
+#define   LIMA_MMU_STATUS_IDLE                (1 << 3)
+#define   LIMA_MMU_STATUS_REPLAY_BUFFER_EMPTY (1 << 4)
+#define   LIMA_MMU_STATUS_PAGE_FAULT_IS_WRITE (1 << 5)
+#define   LIMA_MMU_STATUS_BUS_ID(x)           ((x >> 6) & 0x1F)
+#define LIMA_MMU_COMMAND		  0x0008
+#define   LIMA_MMU_COMMAND_ENABLE_PAGING    0x00
+#define   LIMA_MMU_COMMAND_DISABLE_PAGING   0x01
+#define   LIMA_MMU_COMMAND_ENABLE_STALL     0x02
+#define   LIMA_MMU_COMMAND_DISABLE_STALL    0x03
+#define   LIMA_MMU_COMMAND_ZAP_CACHE        0x04
+#define   LIMA_MMU_COMMAND_PAGE_FAULT_DONE  0x05
+#define   LIMA_MMU_COMMAND_HARD_RESET       0x06
+#define LIMA_MMU_PAGE_FAULT_ADDR          0x000C
+#define LIMA_MMU_ZAP_ONE_LINE	          0x0010
+#define LIMA_MMU_INT_RAWSTAT	          0x0014
+#define LIMA_MMU_INT_CLEAR		  0x0018
+#define LIMA_MMU_INT_MASK		  0x001C
+#define   LIMA_MMU_INT_PAGE_FAULT           0x01
+#define   LIMA_MMU_INT_READ_BUS_ERROR       0x02
+#define LIMA_MMU_INT_STATUS		  0x0020
+
+#define mmu_write(reg, data) writel(data, mmu->ip.iomem + LIMA_MMU_##reg)
+#define mmu_read(reg) readl(mmu->ip.iomem + LIMA_MMU_##reg)
+
+#define lima_mmu_send_command(command, condition)	     \
+({							     \
+	int __timeout, __ret = 0;			     \
+							     \
+	mmu_write(COMMAND, command);			     \
+	for (__timeout = 1000; __timeout > 0; __timeout--) { \
+		if (condition)				     \
+			break;				     \
+	}						     \
+	if (!__timeout)	{				     \
+		dev_err(dev->dev, "mmu command %x timeout\n", command); \
+		__ret = -ETIMEDOUT;			     \
+	}						     \
+	__ret;						     \
+})
+
+static irqreturn_t lima_mmu_irq_handler(int irq, void *data)
+{
+	struct lima_mmu *mmu = data;
+	struct lima_device *dev = mmu->ip.dev;
+	u32 status = mmu_read(INT_STATUS);
+
+	/* for shared irq case */
+	if (!status)
+		return IRQ_NONE;
+
+	if (status & LIMA_MMU_INT_PAGE_FAULT) {
+		u32 fault = mmu_read(PAGE_FAULT_ADDR);
+		dev_err(dev->dev, "mmu page fault at 0x%x from bus id %d of type %s on %s\n",
+			fault, LIMA_MMU_STATUS_BUS_ID(status),
+			status & LIMA_MMU_STATUS_PAGE_FAULT_IS_WRITE ? "write" : "read",
+			mmu->ip.name);
+		//lima_vm_print(mmu->vm);
+	}
+
+	if (status & LIMA_MMU_INT_READ_BUS_ERROR) {
+		dev_err(dev->dev, "mmu %s irq bus error\n", mmu->ip.name);
+	}
+
+	/* mask all interrupts before resume */
+	mmu_write(INT_MASK, 0);
+	mmu_write(INT_CLEAR, status);
+
+	lima_sched_pipe_mmu_error(mmu->pipe);
+
+	return IRQ_HANDLED;
+}
+
+int lima_mmu_init(struct lima_mmu *mmu)
+{
+	struct lima_device *dev = mmu->ip.dev;
+	int err;
+
+	mmu_write(DTE_ADDR, 0xCAFEBABE);
+	if (mmu_read(DTE_ADDR) != 0xCAFEB000) {
+		dev_err(dev->dev, "mmu %s dte write test fail\n", mmu->ip.name);
+		return -EIO;
+	}
+
+	err = lima_mmu_send_command(LIMA_MMU_COMMAND_HARD_RESET, mmu_read(DTE_ADDR) == 0);
+	if (err)
+		return err;
+
+	err = devm_request_irq(dev->dev, mmu->ip.irq, lima_mmu_irq_handler,
+			       IRQF_SHARED, mmu->ip.name, mmu);
+	if (err) {
+		dev_err(dev->dev, "mmu %s fail to request irq\n", mmu->ip.name);
+		return err;
+	}
+
+	mmu_write(INT_MASK, LIMA_MMU_INT_PAGE_FAULT | LIMA_MMU_INT_READ_BUS_ERROR);
+	mmu_write(DTE_ADDR, dev->empty_vm->pd.dma);
+	err = lima_mmu_send_command(LIMA_MMU_COMMAND_ENABLE_PAGING,
+				    mmu_read(STATUS) & LIMA_MMU_STATUS_PAGING_ENABLED);
+	if (err)
+		return err;
+
+	mmu->vm = dev->empty_vm;
+	spin_lock_init(&mmu->lock);
+	mmu->zap_all = false;
+
+	return 0;
+}
+
+void lima_mmu_fini(struct lima_mmu *mmu)
+{
+
+}
+
+void lima_mmu_switch_vm(struct lima_mmu *mmu, struct lima_vm *vm, bool reset)
+{
+	struct lima_device *dev = mmu->ip.dev;
+
+	spin_lock(&mmu->lock);
+
+	if (mmu->vm == vm) {
+		if (reset)
+			vm = dev->empty_vm;
+		else {
+			/* TODO: mmu->zap_all was designed here when active VM is
+			 * updated, zap the MMU TLB. But seems we need always do zap
+			 * (without stall) before start task with the same VM. So
+			 * the MMU TLB can't keep across tasks with the same VM?
+			 */
+			mmu_write(COMMAND, LIMA_MMU_COMMAND_ZAP_CACHE);
+			goto out;
+		}
+	}
+
+	lima_mmu_send_command(LIMA_MMU_COMMAND_ENABLE_STALL,
+			      mmu_read(STATUS) & LIMA_MMU_STATUS_STALL_ACTIVE);
+
+	if (mmu->vm != vm)
+		mmu_write(DTE_ADDR, vm->pd.dma);
+
+	/* flush the TLB */
+	mmu_write(COMMAND, LIMA_MMU_COMMAND_ZAP_CACHE);
+
+	lima_mmu_send_command(LIMA_MMU_COMMAND_DISABLE_STALL,
+			      !(mmu_read(STATUS) & LIMA_MMU_STATUS_STALL_ACTIVE));
+
+	mmu->vm = vm;
+	mmu->zap_all = false;
+
+out:
+	spin_unlock(&mmu->lock);
+}
+
+void lima_mmu_zap_vm(struct lima_mmu *mmu, struct lima_vm *vm, u32 va, u32 size)
+{
+	/* TODO: use LIMA_MMU_ZAP_ONE_LINE to just zap a PDE
+         * needs to investigate:
+         * 1. if LIMA_MMU_ZAP_ONE_LINE need stall mmu, otherwise we can zap it here
+         * 2. how many PDE when LIMA_MMU_ZAP_ONE_LINE is better than zap all,
+         *    otherwise we can use zap all when exceeds that limit
+         */
+
+	spin_lock(&mmu->lock);
+	if (mmu->vm == vm)
+	        mmu->zap_all = true;
+	spin_unlock(&mmu->lock);
+}
+
+void lima_mmu_page_fault_resume(struct lima_mmu *mmu)
+{
+	struct lima_device *dev = mmu->ip.dev;
+	u32 status = mmu_read(STATUS);
+
+	if (status & LIMA_MMU_STATUS_PAGE_FAULT_ACTIVE) {
+		dev_info(dev->dev, "mmu resume\n");
+
+	        spin_lock(&mmu->lock);
+		mmu->vm = dev->empty_vm;
+		spin_unlock(&mmu->lock);
+
+		mmu_write(INT_MASK, 0);
+		mmu_write(DTE_ADDR, 0xCAFEBABE);
+		lima_mmu_send_command(LIMA_MMU_COMMAND_HARD_RESET, mmu_read(DTE_ADDR) == 0);
+	        mmu_write(INT_MASK, LIMA_MMU_INT_PAGE_FAULT | LIMA_MMU_INT_READ_BUS_ERROR);
+		mmu_write(DTE_ADDR, dev->empty_vm->pd.dma);
+		lima_mmu_send_command(LIMA_MMU_COMMAND_ENABLE_PAGING,
+				      mmu_read(STATUS) & LIMA_MMU_STATUS_PAGING_ENABLED);
+	}
+}
diff --git a/drivers/gpu/drm/lima/lima_pmu.c b/drivers/gpu/drm/lima/lima_pmu.c
new file mode 100644
index 0000000..1b518c0
--- /dev/null
+++ b/drivers/gpu/drm/lima/lima_pmu.c
@@ -0,0 +1,90 @@
+#include "lima.h"
+
+#define LIMA_PMU_POWER_UP                  0x00
+#define LIMA_PMU_POWER_DOWN                0x04
+#define   LIMA_PMU_POWER_GP0_MASK          (1 << 0)
+#define   LIMA_PMU_POWER_L2_MASK           (1 << 1)
+#define   LIMA_PMU_POWER_PP_MASK(i)        (1 << (2 + i))
+
+/*
+ * On Mali450 each block automatically starts up its corresponding L2
+ * and the PPs are not fully independent controllable.
+ * Instead PP0, PP1-3 and PP4-7 can be turned on or off.
+ */
+#define   LIMA450_PMU_POWER_PP0_MASK       BIT(1)
+#define   LIMA450_PMU_POWER_PP13_MASK      BIT(2)
+#define   LIMA450_PMU_POWER_PP47_MASK      BIT(3)
+
+#define LIMA_PMU_STATUS                    0x08
+#define LIMA_PMU_INT_MASK                  0x0C
+#define LIMA_PMU_INT_RAWSTAT               0x10
+#define LIMA_PMU_INT_CLEAR                 0x18
+#define   LIMA_PMU_INT_CMD_MASK            (1 << 0)
+#define LIMA_PMU_SW_DELAY                  0x1C
+
+#define pmu_write(reg, data) writel(data, pmu->ip.iomem + LIMA_PMU_##reg)
+#define pmu_read(reg) readl(pmu->ip.iomem + LIMA_PMU_##reg)
+
+static int lima_pmu_wait_cmd(struct lima_pmu *pmu)
+{
+	struct lima_device *dev = pmu->ip.dev;
+	u32 stat, timeout;
+
+	for (timeout = 1000000; timeout > 0; timeout--) {
+		stat = pmu_read(INT_RAWSTAT);
+		if (stat & LIMA_PMU_INT_CMD_MASK)
+			break;
+	}
+
+	if (!timeout) {
+		dev_err(dev->dev, "timeout wait pmd cmd\n");
+		return -ETIMEDOUT;
+	}
+
+	pmu_write(INT_CLEAR, LIMA_PMU_INT_CMD_MASK);
+	return 0;
+}
+
+int lima_pmu_init(struct lima_pmu *pmu)
+{
+	int i, err;
+	u32 stat, mask;
+	struct lima_device *dev = pmu->ip.dev;
+
+	pmu_write(INT_MASK, 0);
+	pmu_write(SW_DELAY, pmu->switch_delay);
+
+	/* status reg 1=off 0=on */
+	stat = pmu_read(STATUS);
+
+	/* power up all ip */
+	switch (dev->gpu_type) {
+	case GPU_MALI400:
+		mask = LIMA_PMU_POWER_GP0_MASK | LIMA_PMU_POWER_L2_MASK;
+		for (i = 0; i < dev->num_pp; i++)
+			mask |= LIMA_PMU_POWER_PP_MASK(i);
+		break;
+	case GPU_MALI450:
+		mask = LIMA_PMU_POWER_GP0_MASK | LIMA450_PMU_POWER_PP0_MASK;
+		if (dev->num_pp > 1)
+			mask |= LIMA450_PMU_POWER_PP13_MASK;
+		if (dev->num_pp > 4)
+			mask |= LIMA450_PMU_POWER_PP47_MASK;
+		break;
+	default:
+		return -ENODEV;
+	}
+
+	if (stat & mask) {
+		pmu_write(POWER_UP, stat & mask);
+		err = lima_pmu_wait_cmd(pmu);
+		if (err)
+			return err;
+	}
+	return 0;
+}
+
+void lima_pmu_fini(struct lima_pmu *pmu)
+{
+
+}
diff --git a/drivers/gpu/drm/lima/lima_pp.c b/drivers/gpu/drm/lima/lima_pp.c
new file mode 100644
index 0000000..ecb7ef2
--- /dev/null
+++ b/drivers/gpu/drm/lima/lima_pp.c
@@ -0,0 +1,355 @@
+#include "lima.h"
+
+#define LIMA_PP_FRAME                        0x0000
+#define LIMA_PP_RSW			     0x0004
+#define LIMA_PP_STACK			     0x0030
+#define LIMA_PP_STACK_SIZE		     0x0034
+#define LIMA_PP_ORIGIN_OFFSET_X	             0x0040
+#define LIMA_PP_WB(i) 			     (0x0100 * (i + 1))
+#define   LIMA_PP_WB_SOURCE_SELECT           0x0000
+#define	  LIMA_PP_WB_SOURCE_ADDR             0x0004
+
+#define LIMA_PP_VERSION                      0x1000
+#define LIMA_PP_CURRENT_REND_LIST_ADDR       0x1004
+#define	LIMA_PP_STATUS                       0x1008
+#define	  LIMA_PP_STATUS_RENDERING_ACTIVE    (1 << 0)
+#define	  LIMA_PP_STATUS_BUS_STOPPED	     (1 << 4)
+#define	LIMA_PP_CTRL                         0x100c
+#define   LIMA_PP_CTRL_STOP_BUS	             (1 << 0)
+#define	  LIMA_PP_CTRL_FLUSH_CACHES          (1 << 3)
+#define	  LIMA_PP_CTRL_FORCE_RESET           (1 << 5)
+#define	  LIMA_PP_CTRL_START_RENDERING       (1 << 6)
+#define	  LIMA_PP_CTRL_SOFT_RESET            (1 << 7)
+#define	LIMA_PP_INT_RAWSTAT                  0x1020
+#define	LIMA_PP_INT_CLEAR                    0x1024
+#define	LIMA_PP_INT_MASK                     0x1028
+#define	LIMA_PP_INT_STATUS                   0x102c
+#define	  LIMA_PP_IRQ_END_OF_FRAME           (1 << 0)
+#define	  LIMA_PP_IRQ_END_OF_TILE	     (1 << 1)
+#define	  LIMA_PP_IRQ_HANG		     (1 << 2)
+#define	  LIMA_PP_IRQ_FORCE_HANG	     (1 << 3)
+#define	  LIMA_PP_IRQ_BUS_ERROR		     (1 << 4)
+#define	  LIMA_PP_IRQ_BUS_STOP		     (1 << 5)
+#define	  LIMA_PP_IRQ_CNT_0_LIMIT	     (1 << 6)
+#define	  LIMA_PP_IRQ_CNT_1_LIMIT	     (1 << 7)
+#define	  LIMA_PP_IRQ_WRITE_BOUNDARY_ERROR   (1 << 8)
+#define	  LIMA_PP_IRQ_INVALID_PLIST_COMMAND  (1 << 9)
+#define	  LIMA_PP_IRQ_CALL_STACK_UNDERFLOW   (1 << 10)
+#define	  LIMA_PP_IRQ_CALL_STACK_OVERFLOW    (1 << 11)
+#define	  LIMA_PP_IRQ_RESET_COMPLETED	     (1 << 12)
+#define	LIMA_PP_WRITE_BOUNDARY_LOW           0x1044
+#define	LIMA_PP_BUS_ERROR_STATUS             0x1050
+#define	LIMA_PP_PERF_CNT_0_ENABLE            0x1080
+#define	LIMA_PP_PERF_CNT_0_SRC               0x1084
+#define	LIMA_PP_PERF_CNT_0_LIMIT             0x1088
+#define	LIMA_PP_PERF_CNT_0_VALUE             0x108c
+#define	LIMA_PP_PERF_CNT_1_ENABLE            0x10a0
+#define	LIMA_PP_PERF_CNT_1_SRC               0x10a4
+#define	LIMA_PP_PERF_CNT_1_LIMIT             0x10a8
+#define	LIMA_PP_PERF_CNT_1_VALUE             0x10ac
+#define LIMA_PP_PERFMON_CONTR                0x10b0
+#define LIMA_PP_PERFMON_BASE                 0x10b4
+
+#define LIMA_PP_IRQ_MASK_ALL                 \
+	(                                    \
+	 LIMA_PP_IRQ_END_OF_FRAME          | \
+	 LIMA_PP_IRQ_END_OF_TILE           | \
+	 LIMA_PP_IRQ_HANG                  | \
+	 LIMA_PP_IRQ_FORCE_HANG            | \
+	 LIMA_PP_IRQ_BUS_ERROR             | \
+	 LIMA_PP_IRQ_BUS_STOP              | \
+	 LIMA_PP_IRQ_CNT_0_LIMIT           | \
+	 LIMA_PP_IRQ_CNT_1_LIMIT           | \
+	 LIMA_PP_IRQ_WRITE_BOUNDARY_ERROR  | \
+	 LIMA_PP_IRQ_INVALID_PLIST_COMMAND | \
+	 LIMA_PP_IRQ_CALL_STACK_UNDERFLOW  | \
+	 LIMA_PP_IRQ_CALL_STACK_OVERFLOW   | \
+	 LIMA_PP_IRQ_RESET_COMPLETED)
+
+#define LIMA_PP_IRQ_MASK_ERROR               \
+	(                                    \
+	 LIMA_PP_IRQ_FORCE_HANG            | \
+	 LIMA_PP_IRQ_BUS_ERROR             | \
+	 LIMA_PP_IRQ_WRITE_BOUNDARY_ERROR  | \
+	 LIMA_PP_IRQ_INVALID_PLIST_COMMAND | \
+	 LIMA_PP_IRQ_CALL_STACK_UNDERFLOW  | \
+	 LIMA_PP_IRQ_CALL_STACK_OVERFLOW)
+
+#define LIMA_PP_IRQ_MASK_USED                \
+	(                                    \
+	 LIMA_PP_IRQ_END_OF_FRAME          | \
+	 LIMA_PP_IRQ_MASK_ERROR)
+
+#define pp_write(reg, data) writel(data, core->ip.iomem + LIMA_PP_##reg)
+#define pp_read(reg) readl(core->ip.iomem + LIMA_PP_##reg)
+
+static irqreturn_t lima_pp_core_irq_handler(int irq, void *data)
+{
+	struct lima_pp_core *core = data;
+	struct lima_device *dev = core->ip.dev;
+	struct lima_pp *pp = dev->pp;
+	u32 state = pp_read(INT_STATUS);
+	bool task_done = false;
+
+	/* for shared irq case */
+	if (!state)
+		return IRQ_NONE;
+
+	if (state & LIMA_PP_IRQ_MASK_ERROR) {
+		u32 status = pp_read(STATUS);
+
+		dev_err(dev->dev, "pp error irq state=%x status=%x\n",
+			state, status);
+
+		task_done = true;
+		pp->error = true;
+
+		/* mask all interrupts before hard reset */
+		pp_write(INT_MASK, 0);
+	}
+	else {
+		if (state & LIMA_PP_IRQ_END_OF_FRAME)
+			task_done = true;
+	}
+
+	pp_write(INT_CLEAR, state);
+
+	if (task_done && atomic_dec_and_test(&pp->task))
+		lima_sched_pipe_task_done(&pp->pipe, pp->error);
+
+	return IRQ_HANDLED;
+}
+
+static void lima_pp_core_soft_reset_async(struct lima_pp_core *core)
+{
+	if (core->async_reset)
+		return;
+
+	pp_write(INT_MASK, 0);
+	pp_write(INT_RAWSTAT, LIMA_PP_IRQ_MASK_ALL);
+	pp_write(CTRL, LIMA_PP_CTRL_SOFT_RESET);
+	core->async_reset = true;
+}
+
+static int lima_pp_core_soft_reset_async_wait(struct lima_pp_core *core)
+{
+	struct lima_device *dev = core->ip.dev;
+	int timeout;
+
+	if (!core->async_reset)
+		return 0;
+
+	for (timeout = 1000; timeout > 0; timeout--) {
+		if (!(pp_read(STATUS) & LIMA_PP_STATUS_RENDERING_ACTIVE) &&
+		    pp_read(INT_RAWSTAT) == LIMA_PP_IRQ_RESET_COMPLETED)
+			break;
+	}
+	if (!timeout) {
+		dev_err(dev->dev, "gp reset time out\n");
+		return -ETIMEDOUT;
+	}
+
+	pp_write(INT_CLEAR, LIMA_PP_IRQ_MASK_ALL);
+	pp_write(INT_MASK, LIMA_PP_IRQ_MASK_USED);
+
+	core->async_reset = false;
+	return 0;
+}
+
+static void lima_pp_core_start_task(struct lima_pp_core *core, int index,
+				    struct lima_sched_task *task)
+{
+	struct drm_lima_m400_pp_frame *frame = task->frame;
+	u32 *frame_reg = (void *)&frame->frame;
+	const int num_frame_reg = 23, num_wb_reg = 12;
+	int i, j;
+
+	lima_pp_core_soft_reset_async_wait(core);
+
+	frame->frame.plbu_array_address = frame->plbu_array_address[index];
+	frame->frame.fragment_stack_address = frame->fragment_stack_address[index];
+
+	for (i = 0; i < num_frame_reg; i++)
+		writel(frame_reg[i], core->ip.iomem + LIMA_PP_FRAME + i * 4);
+
+	for (i = 0; i < 3; i++) {
+		u32 *wb_reg = (void *)&frame->wb[i];
+		for (j = 0; j < num_wb_reg; j++)
+			writel(wb_reg[j], core->ip.iomem + LIMA_PP_WB(i) + j * 4);
+	}
+
+	pp_write(CTRL, LIMA_PP_CTRL_START_RENDERING);
+}
+
+static int lima_pp_core_hard_reset(struct lima_pp_core *core)
+{
+	struct lima_device *dev = core->ip.dev;
+	int timeout;
+
+	pp_write(PERF_CNT_0_LIMIT, 0xC0FFE000);
+	pp_write(INT_MASK, 0);
+	pp_write(CTRL, LIMA_PP_CTRL_FORCE_RESET);
+	for (timeout = 1000; timeout > 0; timeout--) {
+		pp_write(PERF_CNT_0_LIMIT, 0xC01A0000);
+		if (pp_read(PERF_CNT_0_LIMIT) == 0xC01A0000)
+			break;
+	}
+	if (!timeout) {
+		dev_err(dev->dev, "pp hard reset timeout\n");
+		return -ETIMEDOUT;
+	}
+
+	pp_write(PERF_CNT_0_LIMIT, 0);
+	pp_write(INT_CLEAR, LIMA_PP_IRQ_MASK_ALL);
+	pp_write(INT_MASK, LIMA_PP_IRQ_MASK_USED);
+	return 0;
+}
+
+static void lima_pp_print_version(struct lima_pp_core *core)
+{
+	u32 version, major, minor;
+	char *name;
+
+	version = pp_read(VERSION);
+	major = (version >> 8) & 0xFF;
+	minor = version & 0xFF;
+	switch (version >> 16) {
+	case 0xC807:
+	    name = "mali200";
+		break;
+	case 0xCE07:
+		name = "mali300";
+		break;
+	case 0xCD07:
+		name = "mali400";
+		break;
+	case 0xCF07:
+		name = "mali450";
+		break;
+	default:
+		name = "unknow";
+		break;
+	}
+	dev_info(core->ip.dev->dev, "%s - %s version major %d minor %d\n",
+		 core->ip.name, name, major, minor);
+}
+
+int lima_pp_core_init(struct lima_pp_core *core)
+{
+	struct lima_device *dev = core->ip.dev;
+	int err;
+
+	lima_pp_print_version(core);
+
+	core->async_reset = false;
+	lima_pp_core_soft_reset_async(core);
+	err = lima_pp_core_soft_reset_async_wait(core);
+	if (err)
+		return err;
+
+	err = devm_request_irq(dev->dev, core->ip.irq, lima_pp_core_irq_handler,
+			       IRQF_SHARED, core->ip.name, core);
+	if (err) {
+		dev_err(dev->dev, "pp %s fail to request irq\n", core->ip.name);
+		return err;
+	}
+
+	return 0;
+}
+
+void lima_pp_core_fini(struct lima_pp_core *core)
+{
+	
+}
+
+static int lima_pp_task_validate(void *data, struct lima_sched_task *task)
+{
+	struct lima_pp *pp = data;
+	struct drm_lima_m400_pp_frame *f = task->frame;
+
+	if (f->num_pp > pp->num_core)
+		return -EINVAL;
+
+	return 0;
+}
+
+static void lima_pp_task_run(void *data, struct lima_sched_task *task)
+{
+	struct lima_pp *pp = data;
+	struct drm_lima_m400_pp_frame *frame = task->frame;
+	int i;
+
+	pp->error = false;
+	atomic_set(&pp->task, frame->num_pp);
+
+	for (i = 0; i < frame->num_pp; i++)
+		lima_pp_core_start_task(pp->core + i, i, task);
+}
+
+static void lima_pp_task_fini(void *data)
+{
+	struct lima_pp *pp = data;
+	int i;
+
+	for (i = 0; i < pp->num_core; i++)
+		lima_pp_core_soft_reset_async(pp->core + i);
+}
+
+static void lima_pp_task_error(void *data)
+{
+	struct lima_pp *pp = data;
+	int i;
+
+	for (i = 0; i < pp->num_core; i++)
+		lima_pp_core_hard_reset(pp->core + i);
+}
+
+static void lima_pp_task_mmu_error(void *data)
+{
+	struct lima_pp *pp = data;
+
+	pp->error = true;
+	if (atomic_dec_and_test(&pp->task))
+		lima_sched_pipe_task_done(&pp->pipe, pp->error);
+}
+
+static struct kmem_cache *lima_pp_task_slab = NULL;
+static int lima_pp_task_slab_refcnt = 0;
+
+int lima_pp_init(struct lima_pp *pp)
+{
+	int i, frame_size;
+
+	frame_size = sizeof(struct drm_lima_m400_pp_frame);
+	if (!lima_pp_task_slab) {
+		lima_pp_task_slab = kmem_cache_create(
+			"lima_pp_task", sizeof(struct lima_sched_task) + frame_size,
+			0, SLAB_HWCACHE_ALIGN, NULL);
+		if (!lima_pp_task_slab)
+			return -ENOMEM;
+	}
+	lima_pp_task_slab_refcnt++;
+
+	pp->pipe.frame_size = frame_size;
+	pp->pipe.task_slab = lima_pp_task_slab;
+
+	pp->pipe.task_validate = lima_pp_task_validate;
+	pp->pipe.task_run = lima_pp_task_run;
+	pp->pipe.task_fini = lima_pp_task_fini;
+	pp->pipe.task_error = lima_pp_task_error;
+	pp->pipe.task_mmu_error = lima_pp_task_mmu_error;
+	pp->pipe.data = pp;
+
+	for (i = 0; i < pp->num_core; i++)
+		pp->pipe.mmu[i] = &pp->core[i].mmu;
+	pp->pipe.num_mmu = pp->num_core;
+	return 0;
+}
+
+void lima_pp_fini(struct lima_pp *pp)
+{
+	if (!--lima_pp_task_slab_refcnt) {
+		kmem_cache_destroy(lima_pp_task_slab);
+		lima_pp_task_slab = NULL;
+	}
+}
diff --git a/drivers/gpu/drm/lima/lima_sched.c b/drivers/gpu/drm/lima/lima_sched.c
new file mode 100644
index 0000000..5c3c80a
--- /dev/null
+++ b/drivers/gpu/drm/lima/lima_sched.c
@@ -0,0 +1,473 @@
+#include <linux/kthread.h>
+
+#include "lima.h"
+
+struct lima_fence {
+	struct dma_fence base;
+	struct lima_sched_pipe *pipe;
+};
+
+static struct kmem_cache *lima_fence_slab = NULL;
+
+int lima_sched_slab_init(void)
+{
+	lima_fence_slab = kmem_cache_create(
+		"lima_fence", sizeof(struct lima_fence), 0,
+		SLAB_HWCACHE_ALIGN, NULL);
+	if (!lima_fence_slab)
+		return -ENOMEM;
+
+	return 0;
+}
+
+void lima_sched_slab_fini(void)
+{
+	if (lima_fence_slab)
+		kmem_cache_destroy(lima_fence_slab);
+}
+
+static inline struct lima_fence *to_lima_fence(struct dma_fence *fence)
+{
+	return container_of(fence, struct lima_fence, base);
+}
+
+static const char *lima_fence_get_driver_name(struct dma_fence *fence)
+{
+	return "lima";
+}
+
+static const char *lima_fence_get_timeline_name(struct dma_fence *fence)
+{
+	struct lima_fence *f = to_lima_fence(fence);
+
+	return f->pipe->base.name;
+}
+
+static bool lima_fence_enable_signaling(struct dma_fence *fence)
+{
+	return true;
+}
+
+static void lima_fence_release_rcu(struct rcu_head *rcu)
+{
+	struct dma_fence *f = container_of(rcu, struct dma_fence, rcu);
+	struct lima_fence *fence = to_lima_fence(f);
+
+	kmem_cache_free(lima_fence_slab, fence);
+}
+
+static void lima_fence_release(struct dma_fence *fence)
+{
+	struct lima_fence *f = to_lima_fence(fence);
+
+	call_rcu(&f->base.rcu, lima_fence_release_rcu);
+}
+
+static const struct dma_fence_ops lima_fence_ops = {
+	.get_driver_name = lima_fence_get_driver_name,
+	.get_timeline_name = lima_fence_get_timeline_name,
+	.enable_signaling = lima_fence_enable_signaling,
+	.wait = dma_fence_default_wait,
+	.release = lima_fence_release,
+};
+
+static struct lima_fence *lima_fence_create(struct lima_sched_pipe *pipe)
+{
+	struct lima_fence *fence;
+
+	fence = kmem_cache_zalloc(lima_fence_slab, GFP_KERNEL);
+	if (!fence)
+	       return NULL;
+
+	fence->pipe = pipe;
+	dma_fence_init(&fence->base, &lima_fence_ops, &pipe->fence_lock,
+		       pipe->fence_context, ++pipe->fence_seqno);
+
+	return fence;
+}
+
+static inline struct lima_sched_task *to_lima_task(struct drm_sched_job *job)
+{
+	return container_of(job, struct lima_sched_task, base);
+}
+
+static inline struct lima_sched_pipe *to_lima_pipe(struct drm_gpu_scheduler *sched)
+{
+	return container_of(sched, struct lima_sched_pipe, base);
+}
+
+int lima_sched_task_init(struct lima_sched_task *task,
+			 struct lima_sched_context *context,
+			 struct lima_vm *vm)
+{
+	int err;
+
+	err = drm_sched_job_init(&task->base, context->base.sched,
+				 &context->base, context);
+	if (err)
+		return err;
+
+	task->vm = lima_vm_get(vm);
+	return 0;
+}
+
+void lima_sched_task_fini(struct lima_sched_task *task)
+{
+	dma_fence_put(&task->base.s_fence->finished);
+	lima_vm_put(task->vm);
+}
+
+int lima_sched_task_add_dep(struct lima_sched_task *task, struct dma_fence *fence)
+{
+	int i, new_dep = 4;
+
+	if (task->dep && task->num_dep == task->max_dep)
+		new_dep = task->max_dep * 2;
+
+	if (task->max_dep < new_dep) {
+		void *dep = krealloc(task->dep, sizeof(*task->dep) * new_dep, GFP_KERNEL);
+		if (!dep)
+			return -ENOMEM;
+		task->max_dep = new_dep;
+		task->dep = dep;
+	}
+
+	dma_fence_get(fence);
+	for (i = 0; i < task->num_dep; i++) {
+		if (task->dep[i]->context == fence->context &&
+		    dma_fence_is_later(fence, task->dep[i])) {
+			dma_fence_put(task->dep[i]);
+			task->dep[i] = fence;
+			return 0;
+		}
+	}
+
+	task->dep[task->num_dep++] = fence;
+	return 0;
+}
+
+int lima_sched_context_init(struct lima_sched_pipe *pipe,
+			    struct lima_sched_context *context,
+			    atomic_t *guilty)
+{
+	struct drm_sched_rq *rq = pipe->base.sched_rq + DRM_SCHED_PRIORITY_NORMAL;
+	int err;
+
+	context->fences =
+		kzalloc(sizeof(*context->fences) * lima_sched_max_tasks, GFP_KERNEL);
+	if (!context->fences)
+		return -ENOMEM;
+
+	spin_lock_init(&context->lock);
+	err = drm_sched_entity_init(&pipe->base, &context->base, rq,
+				    lima_sched_max_tasks, guilty);
+	if (err) {
+		kfree(context->fences);
+		context->fences = NULL;
+		return err;
+	}
+
+	return 0;
+}
+
+void lima_sched_context_fini(struct lima_sched_pipe *pipe,
+			     struct lima_sched_context *context)
+{
+	drm_sched_entity_fini(&pipe->base, &context->base);
+
+	if (context->fences)
+		kfree(context->fences);
+}
+
+static uint32_t lima_sched_context_add_fence(struct lima_sched_context *context,
+					     struct dma_fence *fence,
+					     uint32_t *done)
+{
+	uint32_t seq, idx, i, n;
+	struct dma_fence *other;
+
+	spin_lock(&context->lock);
+
+	seq = context->sequence;
+	idx = seq & (lima_sched_max_tasks - 1);
+	other = context->fences[idx];
+
+	if (other) {
+		int err = dma_fence_wait(other, false);
+		if (err)
+			DRM_ERROR("Error %d waiting context fence\n", err);
+	}
+
+	context->fences[idx] = dma_fence_get(fence);
+	context->sequence++;
+
+	/* get finished fence offset from seq */
+	n = min(seq + 1, (uint32_t)lima_sched_max_tasks);
+	for (i = 1; i < n; i++) {
+		idx = (seq - i) & (lima_sched_max_tasks - 1);
+		if (dma_fence_is_signaled(context->fences[idx]))
+			break;
+	}
+
+	spin_unlock(&context->lock);
+
+	dma_fence_put(other);
+
+	*done = i;
+	return seq;
+}
+
+static struct dma_fence *lima_sched_context_get_fence(struct lima_sched_context *context,
+						      uint32_t seq)
+{
+	struct dma_fence *fence;
+	int idx;
+
+	spin_lock(&context->lock);
+
+	/* assume no overflow */
+	if (seq >= context->sequence) {
+		fence = ERR_PTR(-EINVAL);
+		goto out;
+	}
+
+	if (seq + lima_sched_max_tasks < context->sequence) {
+		fence = NULL;
+		goto out;
+	}
+
+	idx = seq & (lima_sched_max_tasks - 1);
+	fence = dma_fence_get(context->fences[idx]);
+
+out:
+	spin_unlock(&context->lock);
+
+	return fence;
+}
+
+uint32_t lima_sched_context_queue_task(struct lima_sched_context *context,
+				       struct lima_sched_task *task,
+				       uint32_t *done)
+{
+	uint32_t seq = lima_sched_context_add_fence(
+		context, &task->base.s_fence->finished, done);
+	drm_sched_entity_push_job(&task->base, &context->base);
+	return seq;
+}
+
+int lima_sched_context_wait_fence(struct lima_sched_context *context,
+				  u32 fence, u64 timeout_ns)
+{
+	signed long ret;
+	struct dma_fence *f = lima_sched_context_get_fence(context, fence);
+
+	if (IS_ERR(f))
+		return PTR_ERR(f);
+	else if (!f)
+		return 0;
+
+	if (!timeout_ns)
+		ret = dma_fence_is_signaled(f) ? 0 : -EBUSY;
+	else {
+		unsigned long timeout = lima_timeout_to_jiffies(timeout_ns);
+
+		/* must use long for result check because in 64bit arch int
+		 * will overflow if timeout is too large and get <0 result
+		 */
+		ret = dma_fence_wait_timeout(f, true, timeout);
+		if (ret == 0)
+			ret = timeout ? -ETIMEDOUT : -EBUSY;
+		else if (ret > 0)
+			ret = 0;
+	}
+
+	dma_fence_put(f);
+	return ret;
+}
+
+static struct dma_fence *lima_sched_dependency(struct drm_sched_job *job,
+					       struct drm_sched_entity *entity)
+{
+	struct lima_sched_task *task = to_lima_task(job);
+	int i;
+
+	for (i = 0; i < task->num_dep; i++) {
+		struct dma_fence *fence = task->dep[i];
+
+		if (!task->dep[i])
+			continue;
+
+		task->dep[i] = NULL;
+
+		if (!dma_fence_is_signaled(fence))
+			return fence;
+
+		dma_fence_put(fence);
+	}
+
+	return NULL;
+}
+
+static struct dma_fence *lima_sched_run_job(struct drm_sched_job *job)
+{
+	struct lima_sched_task *task = to_lima_task(job);
+	struct lima_sched_pipe *pipe = to_lima_pipe(job->sched);
+	struct lima_fence *fence;
+	struct dma_fence *ret;
+	int i;
+
+	/* after GPU reset */
+	if (job->s_fence->finished.error < 0)
+		return NULL;
+
+	fence = lima_fence_create(pipe);
+	if (!fence)
+		return NULL;
+	task->fence = &fence->base;
+
+	/* for caller usage of the fence, otherwise irq handler 
+	 * may consume the fence before caller use it */
+	ret = dma_fence_get(task->fence);
+
+	pipe->current_task = task;
+
+	/* this is needed for MMU to work correctly, otherwise GP/PP
+	 * will hang or page fault for unknown reason after running for
+	 * a while.
+	 *
+	 * Need to investigate:
+	 * 1. is it related to TLB
+	 * 2. how much performance will be affected by L2 cache flush
+	 * 3. can we reduce the calling of this function because all
+	 *    GP/PP use the same L2 cache
+	 */
+	if (pipe->mmu[0]->ip.dev->gpu_type == GPU_MALI450) {
+		lima_l2_cache_flush(pipe->mmu[0]->ip.dev->gp->l2_cache);
+		lima_l2_cache_flush(pipe->mmu[0]->ip.dev->pp->l2_cache);
+	} else {
+		lima_l2_cache_flush(pipe->mmu[0]->ip.dev->l2_cache);
+	}
+
+	for (i = 0; i < pipe->num_mmu; i++)
+		lima_mmu_switch_vm(pipe->mmu[i], task->vm, false);
+
+	pipe->task_run(pipe->data, task);
+
+	return task->fence;
+}
+
+static void lima_sched_handle_error_task(struct lima_sched_pipe *pipe,
+					 struct lima_sched_task *task)
+{
+	int i;
+
+	kthread_park(pipe->base.thread);
+	drm_sched_hw_job_reset(&pipe->base, &task->base);
+
+	pipe->task_error(pipe->data);
+
+	for (i = 0; i < pipe->num_mmu; i++)
+		lima_mmu_page_fault_resume(pipe->mmu[i]);
+
+	drm_sched_job_recovery(&pipe->base);
+	kthread_unpark(pipe->base.thread);
+}
+
+static void lima_sched_timedout_job(struct drm_sched_job *job)
+{
+	struct lima_sched_pipe *pipe = to_lima_pipe(job->sched);
+	struct lima_sched_task *task = to_lima_task(job);
+
+	lima_sched_handle_error_task(pipe, task);
+}
+
+static void lima_sched_free_job(struct drm_sched_job *job)
+{
+	struct lima_sched_task *task = to_lima_task(job);
+	struct lima_sched_pipe *pipe = to_lima_pipe(job->sched);
+	int i;
+
+	dma_fence_put(task->fence);
+
+	for (i = 0; i < task->num_dep; i++) {
+		if (task->dep[i])
+			dma_fence_put(task->dep[i]);
+	}
+
+	if (task->dep)
+		kfree(task->dep);
+
+	lima_vm_put(task->vm);
+	kmem_cache_free(pipe->task_slab, task);
+}
+
+const struct drm_sched_backend_ops lima_sched_ops = {
+	.dependency = lima_sched_dependency,
+	.run_job = lima_sched_run_job,
+	.timedout_job = lima_sched_timedout_job,
+	.free_job = lima_sched_free_job,
+};
+
+static void lima_sched_error_work(struct work_struct *work)
+{
+	struct lima_sched_pipe *pipe =
+		container_of(work, struct lima_sched_pipe, error_work);
+	struct lima_sched_task *task = pipe->current_task;
+
+	lima_sched_handle_error_task(pipe, task);
+}
+
+int lima_sched_pipe_init(struct lima_sched_pipe *pipe, const char *name)
+{
+	long timeout;
+
+	if (lima_sched_timeout_ms <= 0)
+		timeout = MAX_SCHEDULE_TIMEOUT;
+	else
+		timeout = msecs_to_jiffies(lima_sched_timeout_ms);
+
+	pipe->fence_context = dma_fence_context_alloc(1);
+	spin_lock_init(&pipe->fence_lock);
+
+	INIT_WORK(&pipe->error_work, lima_sched_error_work);
+
+	return drm_sched_init(&pipe->base, &lima_sched_ops, 1, 0, timeout, name);
+}
+
+void lima_sched_pipe_fini(struct lima_sched_pipe *pipe)
+{
+	drm_sched_fini(&pipe->base);
+}
+
+unsigned long lima_timeout_to_jiffies(u64 timeout_ns)
+{
+	unsigned long timeout_jiffies;
+	ktime_t timeout;
+
+	/* clamp timeout if it's to large */
+	if (((s64)timeout_ns) < 0)
+		return MAX_SCHEDULE_TIMEOUT;
+
+	timeout = ktime_sub(ns_to_ktime(timeout_ns), ktime_get());
+	if (ktime_to_ns(timeout) < 0)
+		return 0;
+
+	timeout_jiffies = nsecs_to_jiffies(ktime_to_ns(timeout));
+	/*  clamp timeout to avoid unsigned-> signed overflow */
+	if (timeout_jiffies > MAX_SCHEDULE_TIMEOUT )
+		return MAX_SCHEDULE_TIMEOUT;
+
+	return timeout_jiffies;
+}
+
+void lima_sched_pipe_task_done(struct lima_sched_pipe *pipe, bool error)
+{
+	if (error)
+	        schedule_work(&pipe->error_work);
+	else {
+		struct lima_sched_task *task = pipe->current_task;
+
+		pipe->task_fini(pipe->data);
+		dma_fence_signal(task->fence);
+	}
+}
diff --git a/drivers/gpu/drm/lima/lima_sched.h b/drivers/gpu/drm/lima/lima_sched.h
new file mode 100644
index 0000000..7648c25
--- /dev/null
+++ b/drivers/gpu/drm/lima/lima_sched.h
@@ -0,0 +1,103 @@
+/*
+ * Copyright (C) 2017 Lima Project
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ */
+#ifndef __LIMA_SCHED_H__
+#define __LIMA_SCHED_H__
+
+#include <drm/gpu_scheduler.h>
+
+struct lima_sched_task {
+	struct drm_sched_job base;
+
+	struct lima_vm *vm;
+	void *frame;
+
+	struct dma_fence **dep;
+	int num_dep;
+	int max_dep;
+
+	/* pipe fence */
+	struct dma_fence *fence;
+};
+
+struct lima_sched_context {
+	struct drm_sched_entity base;
+	spinlock_t lock;
+	struct dma_fence **fences;
+	uint32_t sequence;
+};
+
+#define LIMA_SCHED_PIPE_MAX_MMU 4
+struct lima_sched_pipe {
+	struct drm_gpu_scheduler base;
+
+	u64 fence_context;
+	u32 fence_seqno;
+	spinlock_t fence_lock;
+
+	struct lima_sched_task *current_task;
+
+	struct lima_mmu *mmu[LIMA_SCHED_PIPE_MAX_MMU];
+	int num_mmu;
+
+	int frame_size;
+	struct kmem_cache *task_slab;
+
+	int (*task_validate)(void *data, struct lima_sched_task *task);
+	void (*task_run)(void *data, struct lima_sched_task *task);
+	void (*task_fini)(void *data);
+	void (*task_error)(void *data);
+	void (*task_mmu_error)(void *data);
+	void *data;
+
+	struct work_struct error_work;
+};
+
+int lima_sched_task_init(struct lima_sched_task *task,
+			 struct lima_sched_context *context,
+			 struct lima_vm *vm);
+void lima_sched_task_fini(struct lima_sched_task *task);
+int lima_sched_task_add_dep(struct lima_sched_task *task, struct dma_fence *fence);
+
+int lima_sched_context_init(struct lima_sched_pipe *pipe,
+			    struct lima_sched_context *context,
+			    atomic_t *guilty);
+void lima_sched_context_fini(struct lima_sched_pipe *pipe,
+			     struct lima_sched_context *context);
+uint32_t lima_sched_context_queue_task(struct lima_sched_context *context,
+				       struct lima_sched_task *task,
+				       uint32_t *done);
+int lima_sched_context_wait_fence(struct lima_sched_context *context,
+				  u32 fence, u64 timeout_ns);
+
+int lima_sched_pipe_init(struct lima_sched_pipe *pipe, const char *name);
+void lima_sched_pipe_fini(struct lima_sched_pipe *pipe);
+void lima_sched_pipe_task_done(struct lima_sched_pipe *pipe, bool error);
+
+static inline void lima_sched_pipe_mmu_error(struct lima_sched_pipe *pipe)
+{
+	pipe->task_mmu_error(pipe->data);
+}
+
+int lima_sched_slab_init(void);
+void lima_sched_slab_fini(void);
+
+#endif
diff --git a/drivers/gpu/drm/lima/lima_vm.c b/drivers/gpu/drm/lima/lima_vm.c
new file mode 100644
index 0000000..7f36260
--- /dev/null
+++ b/drivers/gpu/drm/lima/lima_vm.c
@@ -0,0 +1,223 @@
+#include <linux/slab.h>
+#include <linux/dma-mapping.h>
+#include <linux/interval_tree_generic.h>
+
+#include "lima.h"
+
+#define LIMA_PDE(va) (va >> 22)
+#define LIMA_PTE(va) ((va << 10) >> 22)
+
+#define LIMA_VM_FLAG_PRESENT          (1 << 0)
+#define LIMA_VM_FLAG_READ_PERMISSION  (1 << 1)
+#define LIMA_VM_FLAG_WRITE_PERMISSION (1 << 2)
+#define LIMA_VM_FLAG_OVERRIDE_CACHE   (1 << 3)
+#define LIMA_VM_FLAG_WRITE_CACHEABLE  (1 << 4)
+#define LIMA_VM_FLAG_WRITE_ALLOCATE   (1 << 5)
+#define LIMA_VM_FLAG_WRITE_BUFFERABLE (1 << 6)
+#define LIMA_VM_FLAG_READ_CACHEABLE   (1 << 7)
+#define LIMA_VM_FLAG_READ_ALLOCATE    (1 << 8)
+#define LIMA_VM_FLAG_MASK             0x1FF
+
+#define LIMA_VM_FLAGS_CACHE (			 \
+		LIMA_VM_FLAG_PRESENT |		 \
+		LIMA_VM_FLAG_READ_PERMISSION |	 \
+		LIMA_VM_FLAG_WRITE_PERMISSION |	 \
+		LIMA_VM_FLAG_OVERRIDE_CACHE |	 \
+		LIMA_VM_FLAG_WRITE_CACHEABLE |	 \
+		LIMA_VM_FLAG_WRITE_BUFFERABLE |	 \
+		LIMA_VM_FLAG_READ_CACHEABLE |	 \
+		LIMA_VM_FLAG_READ_ALLOCATE )
+
+#define LIMA_VM_FLAGS_UNCACHE (			\
+		LIMA_VM_FLAG_PRESENT |		\
+		LIMA_VM_FLAG_READ_PERMISSION |	\
+		LIMA_VM_FLAG_WRITE_PERMISSION )
+
+#define START(node) ((node)->start)
+#define LAST(node) ((node)->last)
+
+INTERVAL_TREE_DEFINE(struct lima_bo_va_mapping, rb, uint32_t, __subtree_last,
+		     START, LAST, static, lima_vm_it)
+
+#undef START
+#undef LAST
+
+static void lima_vm_unmap_page_table(struct lima_vm *vm, u32 start, u32 end)
+{
+	u32 addr;
+
+	for (addr = start; addr < end; addr += LIMA_PAGE_SIZE) {
+		u32 pde = LIMA_PDE(addr);
+		u32 pte = LIMA_PTE(addr);
+
+		vm->pts[pde].cpu[pte] = 0;
+		vm->pts[pde].dma--;
+		if (!(vm->pts[pde].dma & LIMA_PAGE_MASK)) {
+			vm->pd.cpu[pde] = 0;
+			vm->pd.dma--;
+
+			dma_free_wc(vm->dev->dev, LIMA_PAGE_SIZE,
+				    vm->pts[pde].cpu, vm->pts[pde].dma);
+			vm->pts[pde].cpu = 0;
+		}
+	}
+}
+
+int lima_vm_map(struct lima_vm *vm, dma_addr_t *pages_dma,
+		struct lima_bo_va_mapping *mapping)
+{
+	int err, i = 0;
+	struct lima_bo_va_mapping *it;
+	u32 addr;
+
+	mutex_lock(&vm->lock);
+
+	it = lima_vm_it_iter_first(&vm->va, mapping->start, mapping->last);
+	if (it) {
+		dev_err(vm->dev->dev, "lima vm map va overlap %x-%x %x-%x\n",
+			mapping->start, mapping->last, it->start, it->last);
+		err = -EINVAL;
+		goto err_out0;
+	}
+
+	lima_vm_it_insert(mapping, &vm->va);
+
+	for (addr = mapping->start; addr <= mapping->last; addr += LIMA_PAGE_SIZE) {
+		u32 pde = LIMA_PDE(addr);
+		u32 pte = LIMA_PTE(addr);
+
+		if (!vm->pts[pde].cpu) {
+			vm->pts[pde].cpu =
+				dma_alloc_wc(vm->dev->dev, LIMA_PAGE_SIZE,
+					     &vm->pts[pde].dma, GFP_KERNEL);
+			if (!vm->pts[pde].cpu) {
+				err = -ENOMEM;
+				goto err_out1;
+			}
+			memset(vm->pts[pde].cpu, 0, LIMA_PAGE_SIZE);
+			vm->pd.cpu[pde] = vm->pts[pde].dma | LIMA_VM_FLAG_PRESENT;
+			vm->pd.dma++;
+		}
+
+		/* dma address should be 4K aligned, so use the lower 12 bit
+		 * as a reference count, 12bit is enough for 1024 max count
+		 */
+		vm->pts[pde].dma++;
+		vm->pts[pde].cpu[pte] = pages_dma[i++] | LIMA_VM_FLAGS_CACHE;
+	}
+
+	mutex_unlock(&vm->lock);
+	return 0;
+
+err_out1:
+	lima_vm_unmap_page_table(vm, mapping->start, addr);
+	lima_vm_it_remove(mapping, &vm->va);
+err_out0:
+	mutex_unlock(&vm->lock);
+	return err;
+}
+
+int lima_vm_unmap(struct lima_vm *vm, struct lima_bo_va_mapping *mapping)
+{
+	int i, j;
+	struct lima_device *dev = vm->dev;
+
+	mutex_lock(&vm->lock);
+
+	lima_vm_it_remove(mapping, &vm->va);
+
+	lima_vm_unmap_page_table(vm, mapping->start, mapping->last + 1);
+
+	mutex_unlock(&vm->lock);
+
+	for (i = 0; i < ARRAY_SIZE(dev->pipe); i++) {
+		for (j = 0; j < dev->pipe[i]->num_mmu; j++)
+			lima_mmu_zap_vm(dev->pipe[i]->mmu[j], vm, mapping->start,
+					mapping->last + 1 - mapping->start);
+	}
+
+	return 0;
+}
+
+struct lima_vm *lima_vm_create(struct lima_device *dev)
+{
+	struct lima_vm *vm;
+
+	vm = kvzalloc(sizeof(*vm), GFP_KERNEL);
+	if (!vm)
+		return NULL;
+
+	vm->dev = dev;
+	vm->va = RB_ROOT_CACHED;
+	mutex_init(&vm->lock);
+	kref_init(&vm->refcount);
+
+	vm->pd.cpu = dma_alloc_wc(dev->dev, LIMA_PAGE_SIZE, &vm->pd.dma, GFP_KERNEL);
+	if (!vm->pd.cpu)
+		goto err_out;
+	memset(vm->pd.cpu, 0, LIMA_PAGE_SIZE);
+
+	return vm;
+
+err_out:
+	kvfree(vm);
+	return NULL;
+}
+
+void lima_vm_release(struct kref *kref)
+{
+	struct lima_vm *vm = container_of(kref, struct lima_vm, refcount);
+	struct lima_device *dev = vm->dev;
+	int i, j;
+
+	/* switch mmu vm to empty vm if this vm is used by it */
+	if (vm != dev->empty_vm) {
+		for (i = 0; i < ARRAY_SIZE(dev->pipe); i++) {
+			for (j = 0; j < dev->pipe[i]->num_mmu; j++)
+				lima_mmu_switch_vm(dev->pipe[i]->mmu[j], vm, true);
+		}
+	}
+
+	if (!RB_EMPTY_ROOT(&vm->va.rb_root)) {
+		dev_err(vm->dev->dev, "still active bo inside vm\n");
+	}
+
+	for (i = 0; (vm->pd.dma & LIMA_PAGE_MASK) && i < LIMA_PAGE_ENT_NUM; i++) {
+		if (vm->pts[i].cpu) {
+			dma_free_wc(vm->dev->dev, LIMA_PAGE_SIZE,
+				    vm->pts[i].cpu, vm->pts[i].dma & ~LIMA_PAGE_MASK);
+			vm->pd.dma--;
+		}
+	}
+
+	if (vm->pd.cpu)
+		dma_free_wc(vm->dev->dev, LIMA_PAGE_SIZE, vm->pd.cpu,
+			    vm->pd.dma & ~LIMA_PAGE_MASK);
+
+	kvfree(vm);
+}
+
+void lima_vm_print(struct lima_vm *vm)
+{
+	int i, j;
+
+	/* to avoid the defined by not used warning */
+	(void)&lima_vm_it_iter_next;
+
+	if (!vm->pd.cpu)
+		return;
+
+	for (i = 0; i < LIMA_PAGE_ENT_NUM; i++) {
+		if (vm->pd.cpu[i]) {
+			printk(KERN_INFO "lima vm pd %03x:%08x\n", i, vm->pd.cpu[i]);
+			if ((vm->pd.cpu[i] & ~LIMA_VM_FLAG_MASK) != vm->pts[i].dma)
+				printk(KERN_INFO "pd %x not match pt %x\n",
+				       i, (u32)vm->pts[i].dma);
+			for (j = 0; j < LIMA_PAGE_ENT_NUM; j++) {
+				if (vm->pts[i].cpu[j])
+					printk(KERN_INFO "  pt %03x:%08x\n",
+					       j, vm->pts[i].cpu[j]);
+			}
+		}
+	}
+}
diff --git a/drivers/gpu/drm/lima/lima_vm.h b/drivers/gpu/drm/lima/lima_vm.h
new file mode 100644
index 0000000..7d07085
--- /dev/null
+++ b/drivers/gpu/drm/lima/lima_vm.h
@@ -0,0 +1,75 @@
+/*
+ * Copyright (C) 2017 Lima Project
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ */
+#ifndef __LIMA_VM_H__
+#define __LIMA_VM_H__
+
+#include <linux/mutex.h>
+#include <linux/rbtree.h>
+#include <linux/kref.h>
+
+#define LIMA_PAGE_SIZE    4096
+#define LIMA_PAGE_MASK    (LIMA_PAGE_SIZE - 1)
+#define LIMA_PAGE_ENT_NUM (LIMA_PAGE_SIZE / sizeof(u32))
+
+struct lima_device;
+
+struct lima_vm_page {
+	u32 *cpu;
+	dma_addr_t dma;
+};
+
+struct lima_vm {
+	struct mutex lock;
+	struct kref refcount;
+
+	struct lima_device *dev;
+
+	/* tree of virtual addresses mapped */
+	struct rb_root_cached va;
+
+        struct lima_vm_page pd;
+	struct lima_vm_page pts[LIMA_PAGE_ENT_NUM];
+};
+
+struct lima_vm *lima_vm_create(struct lima_device *dev);
+void lima_vm_release(struct kref *kref);
+
+static inline struct lima_vm *lima_vm_get(struct lima_vm *vm)
+{
+	kref_get(&vm->refcount);
+	return vm;
+}
+
+static inline void lima_vm_put(struct lima_vm *vm)
+{
+	kref_put(&vm->refcount, lima_vm_release);
+}
+
+struct lima_bo_va_mapping;
+
+int lima_vm_map(struct lima_vm *vm, dma_addr_t *pages_dma,
+		struct lima_bo_va_mapping *mapping);
+int lima_vm_unmap(struct lima_vm *vm, struct lima_bo_va_mapping *mapping);
+
+void lima_vm_print(struct lima_vm *vm);
+
+#endif
diff --git a/drivers/gpu/drm/scheduler/Makefile b/drivers/gpu/drm/scheduler/Makefile
new file mode 100644
index 0000000..bd0377c
--- /dev/null
+++ b/drivers/gpu/drm/scheduler/Makefile
@@ -0,0 +1,26 @@
+#
+# Copyright 2017 Advanced Micro Devices, Inc.
+#
+# Permission is hereby granted, free of charge, to any person obtaining a
+# copy of this software and associated documentation files (the "Software"),
+# to deal in the Software without restriction, including without limitation
+# the rights to use, copy, modify, merge, publish, distribute, sublicense,
+# and/or sell copies of the Software, and to permit persons to whom the
+# Software is furnished to do so, subject to the following conditions:
+#
+# The above copyright notice and this permission notice shall be included in
+# all copies or substantial portions of the Software.
+#
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+# THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+# OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+# ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+# OTHER DEALINGS IN THE SOFTWARE.
+#
+#
+ccflags-y := -Iinclude/drm
+gpu-sched-y := gpu_scheduler.o sched_fence.o
+
+obj-$(CONFIG_DRM_SCHED) += gpu-sched.o
diff --git a/drivers/gpu/drm/scheduler/gpu_scheduler.c b/drivers/gpu/drm/scheduler/gpu_scheduler.c
new file mode 100644
index 0000000..0d95888
--- /dev/null
+++ b/drivers/gpu/drm/scheduler/gpu_scheduler.c
@@ -0,0 +1,744 @@
+/*
+ * Copyright 2015 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+#include <linux/kthread.h>
+#include <linux/wait.h>
+#include <linux/sched.h>
+#include <uapi/linux/sched/types.h>
+#include <drm/drmP.h>
+#include <drm/gpu_scheduler.h>
+#include <drm/spsc_queue.h>
+
+#define CREATE_TRACE_POINTS
+#include <drm/gpu_scheduler_trace.h>
+
+#define to_drm_sched_job(sched_job)		\
+		container_of((sched_job), struct drm_sched_job, queue_node)
+
+static bool drm_sched_entity_is_ready(struct drm_sched_entity *entity);
+static void drm_sched_wakeup(struct drm_gpu_scheduler *sched);
+static void drm_sched_process_job(struct dma_fence *f, struct dma_fence_cb *cb);
+
+/* Initialize a given run queue struct */
+static void drm_sched_rq_init(struct drm_sched_rq *rq)
+{
+	spin_lock_init(&rq->lock);
+	INIT_LIST_HEAD(&rq->entities);
+	rq->current_entity = NULL;
+}
+
+static void drm_sched_rq_add_entity(struct drm_sched_rq *rq,
+				    struct drm_sched_entity *entity)
+{
+	if (!list_empty(&entity->list))
+		return;
+	spin_lock(&rq->lock);
+	list_add_tail(&entity->list, &rq->entities);
+	spin_unlock(&rq->lock);
+}
+
+static void drm_sched_rq_remove_entity(struct drm_sched_rq *rq,
+				       struct drm_sched_entity *entity)
+{
+	if (list_empty(&entity->list))
+		return;
+	spin_lock(&rq->lock);
+	list_del_init(&entity->list);
+	if (rq->current_entity == entity)
+		rq->current_entity = NULL;
+	spin_unlock(&rq->lock);
+}
+
+/**
+ * Select an entity which could provide a job to run
+ *
+ * @rq		The run queue to check.
+ *
+ * Try to find a ready entity, returns NULL if none found.
+ */
+static struct drm_sched_entity *
+drm_sched_rq_select_entity(struct drm_sched_rq *rq)
+{
+	struct drm_sched_entity *entity;
+
+	spin_lock(&rq->lock);
+
+	entity = rq->current_entity;
+	if (entity) {
+		list_for_each_entry_continue(entity, &rq->entities, list) {
+			if (drm_sched_entity_is_ready(entity)) {
+				rq->current_entity = entity;
+				spin_unlock(&rq->lock);
+				return entity;
+			}
+		}
+	}
+
+	list_for_each_entry(entity, &rq->entities, list) {
+
+		if (drm_sched_entity_is_ready(entity)) {
+			rq->current_entity = entity;
+			spin_unlock(&rq->lock);
+			return entity;
+		}
+
+		if (entity == rq->current_entity)
+			break;
+	}
+
+	spin_unlock(&rq->lock);
+
+	return NULL;
+}
+
+/**
+ * Init a context entity used by scheduler when submit to HW ring.
+ *
+ * @sched	The pointer to the scheduler
+ * @entity	The pointer to a valid drm_sched_entity
+ * @rq		The run queue this entity belongs
+ * @kernel	If this is an entity for the kernel
+ * @jobs	The max number of jobs in the job queue
+ *
+ * return 0 if succeed. negative error code on failure
+*/
+int drm_sched_entity_init(struct drm_gpu_scheduler *sched,
+			  struct drm_sched_entity *entity,
+			  struct drm_sched_rq *rq,
+			  uint32_t jobs, atomic_t *guilty)
+{
+	if (!(sched && entity && rq))
+		return -EINVAL;
+
+	memset(entity, 0, sizeof(struct drm_sched_entity));
+	INIT_LIST_HEAD(&entity->list);
+	entity->rq = rq;
+	entity->sched = sched;
+	entity->guilty = guilty;
+
+	spin_lock_init(&entity->rq_lock);
+	spin_lock_init(&entity->queue_lock);
+	spsc_queue_init(&entity->job_queue);
+
+	atomic_set(&entity->fence_seq, 0);
+	entity->fence_context = dma_fence_context_alloc(2);
+
+	return 0;
+}
+EXPORT_SYMBOL(drm_sched_entity_init);
+
+/**
+ * Query if entity is initialized
+ *
+ * @sched       Pointer to scheduler instance
+ * @entity	The pointer to a valid scheduler entity
+ *
+ * return true if entity is initialized, false otherwise
+*/
+static bool drm_sched_entity_is_initialized(struct drm_gpu_scheduler *sched,
+					    struct drm_sched_entity *entity)
+{
+	return entity->sched == sched &&
+		entity->rq != NULL;
+}
+
+/**
+ * Check if entity is idle
+ *
+ * @entity	The pointer to a valid scheduler entity
+ *
+ * Return true if entity don't has any unscheduled jobs.
+ */
+static bool drm_sched_entity_is_idle(struct drm_sched_entity *entity)
+{
+	rmb();
+	if (spsc_queue_peek(&entity->job_queue) == NULL)
+		return true;
+
+	return false;
+}
+
+/**
+ * Check if entity is ready
+ *
+ * @entity	The pointer to a valid scheduler entity
+ *
+ * Return true if entity could provide a job.
+ */
+static bool drm_sched_entity_is_ready(struct drm_sched_entity *entity)
+{
+	if (spsc_queue_peek(&entity->job_queue) == NULL)
+		return false;
+
+	if (READ_ONCE(entity->dependency))
+		return false;
+
+	return true;
+}
+
+/**
+ * Destroy a context entity
+ *
+ * @sched       Pointer to scheduler instance
+ * @entity	The pointer to a valid scheduler entity
+ *
+ * Cleanup and free the allocated resources.
+ */
+void drm_sched_entity_fini(struct drm_gpu_scheduler *sched,
+			   struct drm_sched_entity *entity)
+{
+	int r;
+
+	if (!drm_sched_entity_is_initialized(sched, entity))
+		return;
+	/**
+	 * The client will not queue more IBs during this fini, consume existing
+	 * queued IBs or discard them on SIGKILL
+	*/
+	if ((current->flags & PF_SIGNALED) && current->exit_code == SIGKILL)
+		r = -ERESTARTSYS;
+	else
+		r = wait_event_killable(sched->job_scheduled,
+					drm_sched_entity_is_idle(entity));
+	drm_sched_entity_set_rq(entity, NULL);
+	if (r) {
+		struct drm_sched_job *job;
+
+		/* Park the kernel for a moment to make sure it isn't processing
+		 * our enity.
+		 */
+		kthread_park(sched->thread);
+		kthread_unpark(sched->thread);
+		if (entity->dependency) {
+			dma_fence_remove_callback(entity->dependency,
+						  &entity->cb);
+			dma_fence_put(entity->dependency);
+			entity->dependency = NULL;
+		}
+
+		while ((job = to_drm_sched_job(spsc_queue_pop(&entity->job_queue)))) {
+			struct drm_sched_fence *s_fence = job->s_fence;
+			drm_sched_fence_scheduled(s_fence);
+			dma_fence_set_error(&s_fence->finished, -ESRCH);
+			drm_sched_fence_finished(s_fence);
+			WARN_ON(s_fence->parent);
+			dma_fence_put(&s_fence->finished);
+			sched->ops->free_job(job);
+		}
+	}
+}
+EXPORT_SYMBOL(drm_sched_entity_fini);
+
+static void drm_sched_entity_wakeup(struct dma_fence *f, struct dma_fence_cb *cb)
+{
+	struct drm_sched_entity *entity =
+		container_of(cb, struct drm_sched_entity, cb);
+	entity->dependency = NULL;
+	dma_fence_put(f);
+	drm_sched_wakeup(entity->sched);
+}
+
+static void drm_sched_entity_clear_dep(struct dma_fence *f, struct dma_fence_cb *cb)
+{
+	struct drm_sched_entity *entity =
+		container_of(cb, struct drm_sched_entity, cb);
+	entity->dependency = NULL;
+	dma_fence_put(f);
+}
+
+void drm_sched_entity_set_rq(struct drm_sched_entity *entity,
+			     struct drm_sched_rq *rq)
+{
+	if (entity->rq == rq)
+		return;
+
+	spin_lock(&entity->rq_lock);
+
+	if (entity->rq)
+		drm_sched_rq_remove_entity(entity->rq, entity);
+
+	entity->rq = rq;
+	if (rq)
+		drm_sched_rq_add_entity(rq, entity);
+
+	spin_unlock(&entity->rq_lock);
+}
+EXPORT_SYMBOL(drm_sched_entity_set_rq);
+
+bool drm_sched_dependency_optimized(struct dma_fence* fence,
+				    struct drm_sched_entity *entity)
+{
+	struct drm_gpu_scheduler *sched = entity->sched;
+	struct drm_sched_fence *s_fence;
+
+	if (!fence || dma_fence_is_signaled(fence))
+		return false;
+	if (fence->context == entity->fence_context)
+		return true;
+	s_fence = to_drm_sched_fence(fence);
+	if (s_fence && s_fence->sched == sched)
+		return true;
+
+	return false;
+}
+EXPORT_SYMBOL(drm_sched_dependency_optimized);
+
+static bool drm_sched_entity_add_dependency_cb(struct drm_sched_entity *entity)
+{
+	struct drm_gpu_scheduler *sched = entity->sched;
+	struct dma_fence * fence = entity->dependency;
+	struct drm_sched_fence *s_fence;
+
+	if (fence->context == entity->fence_context) {
+		/* We can ignore fences from ourself */
+		dma_fence_put(entity->dependency);
+		return false;
+	}
+
+	s_fence = to_drm_sched_fence(fence);
+	if (s_fence && s_fence->sched == sched) {
+
+		/*
+		 * Fence is from the same scheduler, only need to wait for
+		 * it to be scheduled
+		 */
+		fence = dma_fence_get(&s_fence->scheduled);
+		dma_fence_put(entity->dependency);
+		entity->dependency = fence;
+		if (!dma_fence_add_callback(fence, &entity->cb,
+					    drm_sched_entity_clear_dep))
+			return true;
+
+		/* Ignore it when it is already scheduled */
+		dma_fence_put(fence);
+		return false;
+	}
+
+	if (!dma_fence_add_callback(entity->dependency, &entity->cb,
+				    drm_sched_entity_wakeup))
+		return true;
+
+	dma_fence_put(entity->dependency);
+	return false;
+}
+
+static struct drm_sched_job *
+drm_sched_entity_pop_job(struct drm_sched_entity *entity)
+{
+	struct drm_gpu_scheduler *sched = entity->sched;
+	struct drm_sched_job *sched_job = to_drm_sched_job(
+						spsc_queue_peek(&entity->job_queue));
+
+	if (!sched_job)
+		return NULL;
+
+	while ((entity->dependency = sched->ops->dependency(sched_job, entity)))
+		if (drm_sched_entity_add_dependency_cb(entity))
+			return NULL;
+
+	/* skip jobs from entity that marked guilty */
+	if (entity->guilty && atomic_read(entity->guilty))
+		dma_fence_set_error(&sched_job->s_fence->finished, -ECANCELED);
+
+	spsc_queue_pop(&entity->job_queue);
+	return sched_job;
+}
+
+/**
+ * Submit a job to the job queue
+ *
+ * @sched_job		The pointer to job required to submit
+ *
+ * Returns 0 for success, negative error code otherwise.
+ */
+void drm_sched_entity_push_job(struct drm_sched_job *sched_job,
+			       struct drm_sched_entity *entity)
+{
+	struct drm_gpu_scheduler *sched = sched_job->sched;
+	bool first = false;
+
+	trace_drm_sched_job(sched_job, entity);
+
+	spin_lock(&entity->queue_lock);
+	first = spsc_queue_push(&entity->job_queue, &sched_job->queue_node);
+
+	spin_unlock(&entity->queue_lock);
+
+	/* first job wakes up scheduler */
+	if (first) {
+		/* Add the entity to the run queue */
+		spin_lock(&entity->rq_lock);
+		drm_sched_rq_add_entity(entity->rq, entity);
+		spin_unlock(&entity->rq_lock);
+		drm_sched_wakeup(sched);
+	}
+}
+EXPORT_SYMBOL(drm_sched_entity_push_job);
+
+/* job_finish is called after hw fence signaled
+ */
+static void drm_sched_job_finish(struct work_struct *work)
+{
+	struct drm_sched_job *s_job = container_of(work, struct drm_sched_job,
+						   finish_work);
+	struct drm_gpu_scheduler *sched = s_job->sched;
+
+	/* remove job from ring_mirror_list */
+	spin_lock(&sched->job_list_lock);
+	list_del_init(&s_job->node);
+	if (sched->timeout != MAX_SCHEDULE_TIMEOUT) {
+		struct drm_sched_job *next;
+
+		spin_unlock(&sched->job_list_lock);
+		cancel_delayed_work_sync(&s_job->work_tdr);
+		spin_lock(&sched->job_list_lock);
+
+		/* queue TDR for next job */
+		next = list_first_entry_or_null(&sched->ring_mirror_list,
+						struct drm_sched_job, node);
+
+		if (next)
+			schedule_delayed_work(&next->work_tdr, sched->timeout);
+	}
+	spin_unlock(&sched->job_list_lock);
+	dma_fence_put(&s_job->s_fence->finished);
+	sched->ops->free_job(s_job);
+}
+
+static void drm_sched_job_finish_cb(struct dma_fence *f,
+				    struct dma_fence_cb *cb)
+{
+	struct drm_sched_job *job = container_of(cb, struct drm_sched_job,
+						 finish_cb);
+	schedule_work(&job->finish_work);
+}
+
+static void drm_sched_job_begin(struct drm_sched_job *s_job)
+{
+	struct drm_gpu_scheduler *sched = s_job->sched;
+
+	dma_fence_add_callback(&s_job->s_fence->finished, &s_job->finish_cb,
+			       drm_sched_job_finish_cb);
+
+	spin_lock(&sched->job_list_lock);
+	list_add_tail(&s_job->node, &sched->ring_mirror_list);
+	if (sched->timeout != MAX_SCHEDULE_TIMEOUT &&
+	    list_first_entry_or_null(&sched->ring_mirror_list,
+				     struct drm_sched_job, node) == s_job)
+		schedule_delayed_work(&s_job->work_tdr, sched->timeout);
+	spin_unlock(&sched->job_list_lock);
+}
+
+static void drm_sched_job_timedout(struct work_struct *work)
+{
+	struct drm_sched_job *job = container_of(work, struct drm_sched_job,
+						 work_tdr.work);
+
+	job->sched->ops->timedout_job(job);
+}
+
+void drm_sched_hw_job_reset(struct drm_gpu_scheduler *sched, struct drm_sched_job *bad)
+{
+	struct drm_sched_job *s_job;
+	struct drm_sched_entity *entity, *tmp;
+	int i;
+
+	spin_lock(&sched->job_list_lock);
+	list_for_each_entry_reverse(s_job, &sched->ring_mirror_list, node) {
+		if (s_job->s_fence->parent &&
+		    dma_fence_remove_callback(s_job->s_fence->parent,
+					      &s_job->s_fence->cb)) {
+			dma_fence_put(s_job->s_fence->parent);
+			s_job->s_fence->parent = NULL;
+			atomic_dec(&sched->hw_rq_count);
+		}
+	}
+	spin_unlock(&sched->job_list_lock);
+
+	if (bad && bad->s_priority != DRM_SCHED_PRIORITY_KERNEL) {
+		atomic_inc(&bad->karma);
+		/* don't increase @bad's karma if it's from KERNEL RQ,
+		 * becuase sometimes GPU hang would cause kernel jobs (like VM updating jobs)
+		 * corrupt but keep in mind that kernel jobs always considered good.
+		 */
+		for (i = DRM_SCHED_PRIORITY_MIN; i < DRM_SCHED_PRIORITY_KERNEL; i++ ) {
+			struct drm_sched_rq *rq = &sched->sched_rq[i];
+
+			spin_lock(&rq->lock);
+			list_for_each_entry_safe(entity, tmp, &rq->entities, list) {
+				if (bad->s_fence->scheduled.context == entity->fence_context) {
+				    if (atomic_read(&bad->karma) > bad->sched->hang_limit)
+						if (entity->guilty)
+							atomic_set(entity->guilty, 1);
+					break;
+				}
+			}
+			spin_unlock(&rq->lock);
+			if (&entity->list != &rq->entities)
+				break;
+		}
+	}
+}
+EXPORT_SYMBOL(drm_sched_hw_job_reset);
+
+void drm_sched_job_recovery(struct drm_gpu_scheduler *sched)
+{
+	struct drm_sched_job *s_job, *tmp;
+	bool found_guilty = false;
+	int r;
+
+	spin_lock(&sched->job_list_lock);
+	s_job = list_first_entry_or_null(&sched->ring_mirror_list,
+					 struct drm_sched_job, node);
+	if (s_job && sched->timeout != MAX_SCHEDULE_TIMEOUT)
+		schedule_delayed_work(&s_job->work_tdr, sched->timeout);
+
+	list_for_each_entry_safe(s_job, tmp, &sched->ring_mirror_list, node) {
+		struct drm_sched_fence *s_fence = s_job->s_fence;
+		struct dma_fence *fence;
+		uint64_t guilty_context;
+
+		if (!found_guilty && atomic_read(&s_job->karma) > sched->hang_limit) {
+			found_guilty = true;
+			guilty_context = s_job->s_fence->scheduled.context;
+		}
+
+		if (found_guilty && s_job->s_fence->scheduled.context == guilty_context)
+			dma_fence_set_error(&s_fence->finished, -ECANCELED);
+
+		spin_unlock(&sched->job_list_lock);
+		fence = sched->ops->run_job(s_job);
+		atomic_inc(&sched->hw_rq_count);
+		if (fence) {
+			s_fence->parent = dma_fence_get(fence);
+			r = dma_fence_add_callback(fence, &s_fence->cb,
+						   drm_sched_process_job);
+			if (r == -ENOENT)
+				drm_sched_process_job(fence, &s_fence->cb);
+			else if (r)
+				DRM_ERROR("fence add callback failed (%d)\n",
+					  r);
+			dma_fence_put(fence);
+		} else {
+			drm_sched_process_job(NULL, &s_fence->cb);
+		}
+		spin_lock(&sched->job_list_lock);
+	}
+	spin_unlock(&sched->job_list_lock);
+}
+EXPORT_SYMBOL(drm_sched_job_recovery);
+
+/* init a sched_job with basic field */
+int drm_sched_job_init(struct drm_sched_job *job,
+		       struct drm_gpu_scheduler *sched,
+		       struct drm_sched_entity *entity,
+		       void *owner)
+{
+	job->sched = sched;
+	job->s_priority = entity->rq - sched->sched_rq;
+	job->s_fence = drm_sched_fence_create(entity, owner);
+	if (!job->s_fence)
+		return -ENOMEM;
+	job->id = atomic64_inc_return(&sched->job_id_count);
+
+	INIT_WORK(&job->finish_work, drm_sched_job_finish);
+	INIT_LIST_HEAD(&job->node);
+	INIT_DELAYED_WORK(&job->work_tdr, drm_sched_job_timedout);
+
+	return 0;
+}
+EXPORT_SYMBOL(drm_sched_job_init);
+
+/**
+ * Return ture if we can push more jobs to the hw.
+ */
+static bool drm_sched_ready(struct drm_gpu_scheduler *sched)
+{
+	return atomic_read(&sched->hw_rq_count) <
+		sched->hw_submission_limit;
+}
+
+/**
+ * Wake up the scheduler when it is ready
+ */
+static void drm_sched_wakeup(struct drm_gpu_scheduler *sched)
+{
+	if (drm_sched_ready(sched))
+		wake_up_interruptible(&sched->wake_up_worker);
+}
+
+/**
+ * Select next entity to process
+*/
+static struct drm_sched_entity *
+drm_sched_select_entity(struct drm_gpu_scheduler *sched)
+{
+	struct drm_sched_entity *entity;
+	int i;
+
+	if (!drm_sched_ready(sched))
+		return NULL;
+
+	/* Kernel run queue has higher priority than normal run queue*/
+	for (i = DRM_SCHED_PRIORITY_MAX - 1; i >= DRM_SCHED_PRIORITY_MIN; i--) {
+		entity = drm_sched_rq_select_entity(&sched->sched_rq[i]);
+		if (entity)
+			break;
+	}
+
+	return entity;
+}
+
+static void drm_sched_process_job(struct dma_fence *f, struct dma_fence_cb *cb)
+{
+	struct drm_sched_fence *s_fence =
+		container_of(cb, struct drm_sched_fence, cb);
+	struct drm_gpu_scheduler *sched = s_fence->sched;
+
+	dma_fence_get(&s_fence->finished);
+	atomic_dec(&sched->hw_rq_count);
+	drm_sched_fence_finished(s_fence);
+
+	trace_drm_sched_process_job(s_fence);
+	dma_fence_put(&s_fence->finished);
+	wake_up_interruptible(&sched->wake_up_worker);
+}
+
+static bool drm_sched_blocked(struct drm_gpu_scheduler *sched)
+{
+	if (kthread_should_park()) {
+		kthread_parkme();
+		return true;
+	}
+
+	return false;
+}
+
+static int drm_sched_main(void *param)
+{
+	struct sched_param sparam = {.sched_priority = 1};
+	struct drm_gpu_scheduler *sched = (struct drm_gpu_scheduler *)param;
+	int r;
+
+	sched_setscheduler(current, SCHED_FIFO, &sparam);
+
+	while (!kthread_should_stop()) {
+		struct drm_sched_entity *entity = NULL;
+		struct drm_sched_fence *s_fence;
+		struct drm_sched_job *sched_job;
+		struct dma_fence *fence;
+
+		wait_event_interruptible(sched->wake_up_worker,
+					 (!drm_sched_blocked(sched) &&
+					  (entity = drm_sched_select_entity(sched))) ||
+					 kthread_should_stop());
+
+		if (!entity)
+			continue;
+
+		sched_job = drm_sched_entity_pop_job(entity);
+		if (!sched_job)
+			continue;
+
+		s_fence = sched_job->s_fence;
+
+		atomic_inc(&sched->hw_rq_count);
+		drm_sched_job_begin(sched_job);
+
+		fence = sched->ops->run_job(sched_job);
+		drm_sched_fence_scheduled(s_fence);
+
+		if (fence) {
+			s_fence->parent = dma_fence_get(fence);
+			r = dma_fence_add_callback(fence, &s_fence->cb,
+						   drm_sched_process_job);
+			if (r == -ENOENT)
+				drm_sched_process_job(fence, &s_fence->cb);
+			else if (r)
+				DRM_ERROR("fence add callback failed (%d)\n",
+					  r);
+			dma_fence_put(fence);
+		} else {
+			drm_sched_process_job(NULL, &s_fence->cb);
+		}
+
+		wake_up(&sched->job_scheduled);
+	}
+	return 0;
+}
+
+/**
+ * Init a gpu scheduler instance
+ *
+ * @sched		The pointer to the scheduler
+ * @ops			The backend operations for this scheduler.
+ * @hw_submissions	Number of hw submissions to do.
+ * @name		Name used for debugging
+ *
+ * Return 0 on success, otherwise error code.
+*/
+int drm_sched_init(struct drm_gpu_scheduler *sched,
+		   const struct drm_sched_backend_ops *ops,
+		   unsigned hw_submission,
+		   unsigned hang_limit,
+		   long timeout,
+		   const char *name)
+{
+	int i;
+	sched->ops = ops;
+	sched->hw_submission_limit = hw_submission;
+	sched->name = name;
+	sched->timeout = timeout;
+	sched->hang_limit = hang_limit;
+	for (i = DRM_SCHED_PRIORITY_MIN; i < DRM_SCHED_PRIORITY_MAX; i++)
+		drm_sched_rq_init(&sched->sched_rq[i]);
+
+	init_waitqueue_head(&sched->wake_up_worker);
+	init_waitqueue_head(&sched->job_scheduled);
+	INIT_LIST_HEAD(&sched->ring_mirror_list);
+	spin_lock_init(&sched->job_list_lock);
+	atomic_set(&sched->hw_rq_count, 0);
+	atomic64_set(&sched->job_id_count, 0);
+
+	/* Each scheduler will run on a seperate kernel thread */
+	sched->thread = kthread_run(drm_sched_main, sched, sched->name);
+	if (IS_ERR(sched->thread)) {
+		DRM_ERROR("Failed to create scheduler for %s.\n", name);
+		return PTR_ERR(sched->thread);
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL(drm_sched_init);
+
+/**
+ * Destroy a gpu scheduler
+ *
+ * @sched	The pointer to the scheduler
+ */
+void drm_sched_fini(struct drm_gpu_scheduler *sched)
+{
+	if (sched->thread)
+		kthread_stop(sched->thread);
+}
+EXPORT_SYMBOL(drm_sched_fini);
diff --git a/drivers/gpu/drm/scheduler/sched_fence.c b/drivers/gpu/drm/scheduler/sched_fence.c
new file mode 100644
index 0000000..69aab08
--- /dev/null
+++ b/drivers/gpu/drm/scheduler/sched_fence.c
@@ -0,0 +1,191 @@
+/*
+ * Copyright 2015 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+#include <linux/kthread.h>
+#include <linux/wait.h>
+#include <linux/sched.h>
+#include <drm/drmP.h>
+#include <drm/gpu_scheduler.h>
+
+static struct kmem_cache *sched_fence_slab;
+
+static int __init drm_sched_fence_slab_init(void)
+{
+	sched_fence_slab = kmem_cache_create(
+		"drm_sched_fence", sizeof(struct drm_sched_fence), 0,
+		SLAB_HWCACHE_ALIGN, NULL);
+	if (!sched_fence_slab)
+		return -ENOMEM;
+
+	return 0;
+}
+
+static void __exit drm_sched_fence_slab_fini(void)
+{
+	rcu_barrier();
+	kmem_cache_destroy(sched_fence_slab);
+}
+
+void drm_sched_fence_scheduled(struct drm_sched_fence *fence)
+{
+	int ret = dma_fence_signal(&fence->scheduled);
+
+	if (!ret)
+		DMA_FENCE_TRACE(&fence->scheduled,
+				"signaled from irq context\n");
+	else
+		DMA_FENCE_TRACE(&fence->scheduled,
+				"was already signaled\n");
+}
+
+void drm_sched_fence_finished(struct drm_sched_fence *fence)
+{
+	int ret = dma_fence_signal(&fence->finished);
+
+	if (!ret)
+		DMA_FENCE_TRACE(&fence->finished,
+				"signaled from irq context\n");
+	else
+		DMA_FENCE_TRACE(&fence->finished,
+				"was already signaled\n");
+}
+
+static const char *drm_sched_fence_get_driver_name(struct dma_fence *fence)
+{
+	return "drm_sched";
+}
+
+static const char *drm_sched_fence_get_timeline_name(struct dma_fence *f)
+{
+	struct drm_sched_fence *fence = to_drm_sched_fence(f);
+	return (const char *)fence->sched->name;
+}
+
+static bool drm_sched_fence_enable_signaling(struct dma_fence *f)
+{
+	return true;
+}
+
+/**
+ * amd_sched_fence_free - free up the fence memory
+ *
+ * @rcu: RCU callback head
+ *
+ * Free up the fence memory after the RCU grace period.
+ */
+static void drm_sched_fence_free(struct rcu_head *rcu)
+{
+	struct dma_fence *f = container_of(rcu, struct dma_fence, rcu);
+	struct drm_sched_fence *fence = to_drm_sched_fence(f);
+
+	dma_fence_put(fence->parent);
+	kmem_cache_free(sched_fence_slab, fence);
+}
+
+/**
+ * amd_sched_fence_release_scheduled - callback that fence can be freed
+ *
+ * @fence: fence
+ *
+ * This function is called when the reference count becomes zero.
+ * It just RCU schedules freeing up the fence.
+ */
+static void drm_sched_fence_release_scheduled(struct dma_fence *f)
+{
+	struct drm_sched_fence *fence = to_drm_sched_fence(f);
+
+	call_rcu(&fence->finished.rcu, drm_sched_fence_free);
+}
+
+/**
+ * amd_sched_fence_release_finished - drop extra reference
+ *
+ * @f: fence
+ *
+ * Drop the extra reference from the scheduled fence to the base fence.
+ */
+static void drm_sched_fence_release_finished(struct dma_fence *f)
+{
+	struct drm_sched_fence *fence = to_drm_sched_fence(f);
+
+	dma_fence_put(&fence->scheduled);
+}
+
+const struct dma_fence_ops drm_sched_fence_ops_scheduled = {
+	.get_driver_name = drm_sched_fence_get_driver_name,
+	.get_timeline_name = drm_sched_fence_get_timeline_name,
+	.enable_signaling = drm_sched_fence_enable_signaling,
+	.signaled = NULL,
+	.wait = dma_fence_default_wait,
+	.release = drm_sched_fence_release_scheduled,
+};
+
+const struct dma_fence_ops drm_sched_fence_ops_finished = {
+	.get_driver_name = drm_sched_fence_get_driver_name,
+	.get_timeline_name = drm_sched_fence_get_timeline_name,
+	.enable_signaling = drm_sched_fence_enable_signaling,
+	.signaled = NULL,
+	.wait = dma_fence_default_wait,
+	.release = drm_sched_fence_release_finished,
+};
+
+struct drm_sched_fence *to_drm_sched_fence(struct dma_fence *f)
+{
+	if (f->ops == &drm_sched_fence_ops_scheduled)
+		return container_of(f, struct drm_sched_fence, scheduled);
+
+	if (f->ops == &drm_sched_fence_ops_finished)
+		return container_of(f, struct drm_sched_fence, finished);
+
+	return NULL;
+}
+EXPORT_SYMBOL(to_drm_sched_fence);
+
+struct drm_sched_fence *drm_sched_fence_create(struct drm_sched_entity *entity,
+					       void *owner)
+{
+	struct drm_sched_fence *fence = NULL;
+	unsigned seq;
+
+	fence = kmem_cache_zalloc(sched_fence_slab, GFP_KERNEL);
+	if (fence == NULL)
+		return NULL;
+
+	fence->owner = owner;
+	fence->sched = entity->sched;
+	spin_lock_init(&fence->lock);
+
+	seq = atomic_inc_return(&entity->fence_seq);
+	dma_fence_init(&fence->scheduled, &drm_sched_fence_ops_scheduled,
+		       &fence->lock, entity->fence_context, seq);
+	dma_fence_init(&fence->finished, &drm_sched_fence_ops_finished,
+		       &fence->lock, entity->fence_context + 1, seq);
+
+	return fence;
+}
+
+module_init(drm_sched_fence_slab_init);
+module_exit(drm_sched_fence_slab_fini);
+
+MODULE_DESCRIPTION("DRM GPU scheduler");
+MODULE_LICENSE("GPL and additional rights");
diff --git a/include/drm/gpu_scheduler.h b/include/drm/gpu_scheduler.h
new file mode 100644
index 0000000..dfd54fb
--- /dev/null
+++ b/include/drm/gpu_scheduler.h
@@ -0,0 +1,173 @@
+/*
+ * Copyright 2015 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+#ifndef _DRM_GPU_SCHEDULER_H_
+#define _DRM_GPU_SCHEDULER_H_
+
+#include <drm/spsc_queue.h>
+#include <linux/dma-fence.h>
+
+struct drm_gpu_scheduler;
+struct drm_sched_rq;
+
+enum drm_sched_priority {
+	DRM_SCHED_PRIORITY_MIN,
+	DRM_SCHED_PRIORITY_LOW = DRM_SCHED_PRIORITY_MIN,
+	DRM_SCHED_PRIORITY_NORMAL,
+	DRM_SCHED_PRIORITY_HIGH_SW,
+	DRM_SCHED_PRIORITY_HIGH_HW,
+	DRM_SCHED_PRIORITY_KERNEL,
+	DRM_SCHED_PRIORITY_MAX,
+	DRM_SCHED_PRIORITY_INVALID = -1,
+	DRM_SCHED_PRIORITY_UNSET = -2
+};
+
+/**
+ * A scheduler entity is a wrapper around a job queue or a group
+ * of other entities. Entities take turns emitting jobs from their
+ * job queues to corresponding hardware ring based on scheduling
+ * policy.
+*/
+struct drm_sched_entity {
+	struct list_head		list;
+	struct drm_sched_rq		*rq;
+	spinlock_t			rq_lock;
+	struct drm_gpu_scheduler	*sched;
+
+	spinlock_t			queue_lock;
+	struct spsc_queue		job_queue;
+
+	atomic_t			fence_seq;
+	uint64_t			fence_context;
+
+	struct dma_fence		*dependency;
+	struct dma_fence_cb		cb;
+	atomic_t			*guilty; /* points to ctx's guilty */
+};
+
+/**
+ * Run queue is a set of entities scheduling command submissions for
+ * one specific ring. It implements the scheduling policy that selects
+ * the next entity to emit commands from.
+*/
+struct drm_sched_rq {
+	spinlock_t			lock;
+	struct list_head		entities;
+	struct drm_sched_entity		*current_entity;
+};
+
+struct drm_sched_fence {
+	struct dma_fence		scheduled;
+	struct dma_fence		finished;
+	struct dma_fence_cb		cb;
+	struct dma_fence		*parent;
+	struct drm_gpu_scheduler	*sched;
+	spinlock_t			lock;
+	void				*owner;
+};
+
+struct drm_sched_fence *to_drm_sched_fence(struct dma_fence *f);
+
+struct drm_sched_job {
+	struct spsc_node		queue_node;
+	struct drm_gpu_scheduler	*sched;
+	struct drm_sched_fence		*s_fence;
+	struct dma_fence_cb		finish_cb;
+	struct work_struct		finish_work;
+	struct list_head		node;
+	struct delayed_work		work_tdr;
+	uint64_t			id;
+	atomic_t			karma;
+	enum drm_sched_priority		s_priority;
+};
+
+static inline bool drm_sched_invalidate_job(struct drm_sched_job *s_job,
+					    int threshold)
+{
+	return (s_job && atomic_inc_return(&s_job->karma) > threshold);
+}
+
+/**
+ * Define the backend operations called by the scheduler,
+ * these functions should be implemented in driver side
+*/
+struct drm_sched_backend_ops {
+	struct dma_fence *(*dependency)(struct drm_sched_job *sched_job,
+					struct drm_sched_entity *s_entity);
+	struct dma_fence *(*run_job)(struct drm_sched_job *sched_job);
+	void (*timedout_job)(struct drm_sched_job *sched_job);
+	void (*free_job)(struct drm_sched_job *sched_job);
+};
+
+/**
+ * One scheduler is implemented for each hardware ring
+*/
+struct drm_gpu_scheduler {
+	const struct drm_sched_backend_ops	*ops;
+	uint32_t			hw_submission_limit;
+	long				timeout;
+	const char			*name;
+	struct drm_sched_rq		sched_rq[DRM_SCHED_PRIORITY_MAX];
+	wait_queue_head_t		wake_up_worker;
+	wait_queue_head_t		job_scheduled;
+	atomic_t			hw_rq_count;
+	atomic64_t			job_id_count;
+	struct task_struct		*thread;
+	struct list_head		ring_mirror_list;
+	spinlock_t			job_list_lock;
+	int				hang_limit;
+};
+
+int drm_sched_init(struct drm_gpu_scheduler *sched,
+		   const struct drm_sched_backend_ops *ops,
+		   uint32_t hw_submission, unsigned hang_limit, long timeout,
+		   const char *name);
+void drm_sched_fini(struct drm_gpu_scheduler *sched);
+
+int drm_sched_entity_init(struct drm_gpu_scheduler *sched,
+			  struct drm_sched_entity *entity,
+			  struct drm_sched_rq *rq,
+			  uint32_t jobs, atomic_t *guilty);
+void drm_sched_entity_fini(struct drm_gpu_scheduler *sched,
+			   struct drm_sched_entity *entity);
+void drm_sched_entity_push_job(struct drm_sched_job *sched_job,
+			       struct drm_sched_entity *entity);
+void drm_sched_entity_set_rq(struct drm_sched_entity *entity,
+			     struct drm_sched_rq *rq);
+
+struct drm_sched_fence *drm_sched_fence_create(
+	struct drm_sched_entity *s_entity, void *owner);
+void drm_sched_fence_scheduled(struct drm_sched_fence *fence);
+void drm_sched_fence_finished(struct drm_sched_fence *fence);
+int drm_sched_job_init(struct drm_sched_job *job,
+		       struct drm_gpu_scheduler *sched,
+		       struct drm_sched_entity *entity,
+		       void *owner);
+void drm_sched_hw_job_reset(struct drm_gpu_scheduler *sched,
+			    struct drm_sched_job *job);
+void drm_sched_job_recovery(struct drm_gpu_scheduler *sched);
+bool drm_sched_dependency_optimized(struct dma_fence* fence,
+				    struct drm_sched_entity *entity);
+void drm_sched_job_kickout(struct drm_sched_job *s_job);
+
+#endif
diff --git a/include/drm/gpu_scheduler_trace.h b/include/drm/gpu_scheduler_trace.h
new file mode 100644
index 0000000..0789e8d
--- /dev/null
+++ b/include/drm/gpu_scheduler_trace.h
@@ -0,0 +1,82 @@
+/*
+ * Copyright 2017 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+#if !defined(_GPU_SCHED_TRACE_H) || defined(TRACE_HEADER_MULTI_READ)
+#define _GPU_SCHED_TRACE_H_
+
+#include <linux/stringify.h>
+#include <linux/types.h>
+#include <linux/tracepoint.h>
+
+#include <drm/drmP.h>
+
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM gpu_scheduler
+#define TRACE_INCLUDE_FILE gpu_scheduler_trace
+
+TRACE_EVENT(drm_sched_job,
+	    TP_PROTO(struct drm_sched_job *sched_job, struct drm_sched_entity *entity),
+	    TP_ARGS(sched_job, entity),
+	    TP_STRUCT__entry(
+			     __field(struct drm_sched_entity *, entity)
+			     __field(struct dma_fence *, fence)
+			     __field(const char *, name)
+			     __field(uint64_t, id)
+			     __field(u32, job_count)
+			     __field(int, hw_job_count)
+			     ),
+
+	    TP_fast_assign(
+			   __entry->entity = entity;
+			   __entry->id = sched_job->id;
+			   __entry->fence = &sched_job->s_fence->finished;
+			   __entry->name = sched_job->sched->name;
+			   __entry->job_count = spsc_queue_count(&entity->job_queue);
+			   __entry->hw_job_count = atomic_read(
+				   &sched_job->sched->hw_rq_count);
+			   ),
+	    TP_printk("entity=%p, id=%llu, fence=%p, ring=%s, job count:%u, hw job count:%d",
+		      __entry->entity, __entry->id,
+		      __entry->fence, __entry->name,
+		      __entry->job_count, __entry->hw_job_count)
+);
+
+TRACE_EVENT(drm_sched_process_job,
+	    TP_PROTO(struct drm_sched_fence *fence),
+	    TP_ARGS(fence),
+	    TP_STRUCT__entry(
+		    __field(struct dma_fence *, fence)
+		    ),
+
+	    TP_fast_assign(
+		    __entry->fence = &fence->finished;
+		    ),
+	    TP_printk("fence=%p signaled", __entry->fence)
+);
+
+#endif
+
+/* This part must be outside protection */
+#undef TRACE_INCLUDE_PATH
+#define TRACE_INCLUDE_PATH .
+#include <trace/define_trace.h>
diff --git a/include/drm/spsc_queue.h b/include/drm/spsc_queue.h
new file mode 100644
index 0000000..125f096
--- /dev/null
+++ b/include/drm/spsc_queue.h
@@ -0,0 +1,122 @@
+/*
+ * Copyright 2017 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+#ifndef DRM_SCHEDULER_SPSC_QUEUE_H_
+#define DRM_SCHEDULER_SPSC_QUEUE_H_
+
+#include <linux/atomic.h>
+#include <linux/preempt.h>
+
+/** SPSC lockless queue */
+
+struct spsc_node {
+
+	/* Stores spsc_node* */
+	struct spsc_node *next;
+};
+
+struct spsc_queue {
+
+	 struct spsc_node *head;
+
+	/* atomic pointer to struct spsc_node* */
+	atomic_long_t tail;
+
+	atomic_t job_count;
+};
+
+static inline void spsc_queue_init(struct spsc_queue *queue)
+{
+	queue->head = NULL;
+	atomic_long_set(&queue->tail, (long)&queue->head);
+	atomic_set(&queue->job_count, 0);
+}
+
+static inline struct spsc_node *spsc_queue_peek(struct spsc_queue *queue)
+{
+	return queue->head;
+}
+
+static inline int spsc_queue_count(struct spsc_queue *queue)
+{
+	return atomic_read(&queue->job_count);
+}
+
+static inline bool spsc_queue_push(struct spsc_queue *queue, struct spsc_node *node)
+{
+	struct spsc_node **tail;
+
+	node->next = NULL;
+
+	preempt_disable();
+
+	tail = (struct spsc_node **)atomic_long_xchg(&queue->tail, (long)&node->next);
+	WRITE_ONCE(*tail, node);
+	atomic_inc(&queue->job_count);
+
+	/*
+	 * In case of first element verify new node will be visible to the consumer
+	 * thread when we ping the kernel thread that there is new work to do.
+	 */
+	smp_wmb();
+
+	preempt_enable();
+
+	return tail == &queue->head;
+}
+
+
+static inline struct spsc_node *spsc_queue_pop(struct spsc_queue *queue)
+{
+	struct spsc_node *next, *node;
+
+	/* Verify reading from memory and not the cache */
+	smp_rmb();
+
+	node = READ_ONCE(queue->head);
+
+	if (!node)
+		return NULL;
+
+	next = READ_ONCE(node->next);
+	WRITE_ONCE(queue->head, next);
+
+	if (unlikely(!next)) {
+		/* slowpath for the last element in the queue */
+
+		if (atomic_long_cmpxchg(&queue->tail,
+				(long)&node->next, (long) &queue->head) != (long)&node->next) {
+			/* Updating tail failed wait for new next to appear */
+			do {
+				smp_rmb();
+			} while (unlikely(!(queue->head = READ_ONCE(node->next))));
+		}
+	}
+
+	atomic_dec(&queue->job_count);
+	return node;
+}
+
+
+
+#endif /* DRM_SCHEDULER_SPSC_QUEUE_H_ */
diff --git a/include/dt-bindings/clock/mt2701-clk.h b/include/dt-bindings/clock/mt2701-clk.h
index 1956ebb..9ac2f2b 100644
--- a/include/dt-bindings/clock/mt2701-clk.h
+++ b/include/dt-bindings/clock/mt2701-clk.h
@@ -431,6 +431,10 @@
 #define CLK_ETHSYS_CRYPTO			8
 #define CLK_ETHSYS_NR				9
 
+/* G3DSYS */
+#define CLK_G3DSYS_CORE				1
+#define CLK_G3DSYS_NR				2
+
 /* BDP */
 
 #define CLK_BDP_BRG_BA				1
diff --git a/include/dt-bindings/reset/mt2701-resets.h b/include/dt-bindings/reset/mt2701-resets.h
index 21deb54..50b7f06 100644
--- a/include/dt-bindings/reset/mt2701-resets.h
+++ b/include/dt-bindings/reset/mt2701-resets.h
@@ -87,4 +87,7 @@
 #define MT2701_ETHSYS_GMAC_RST			23
 #define MT2701_ETHSYS_PPE_RST			31
 
+/* G3DSYS resets */
+#define MT2701_G3DSYS_CORE_RST			0
+
 #endif  /* _DT_BINDINGS_RESET_CONTROLLER_MT2701 */
diff --git a/include/uapi/drm/lima_drm.h b/include/uapi/drm/lima_drm.h
new file mode 100644
index 0000000..56fc019
--- /dev/null
+++ b/include/uapi/drm/lima_drm.h
@@ -0,0 +1,201 @@
+/*
+ * Copyright (C) 2017 Lima Project
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ */
+#ifndef __LIMA_DRM_H__
+#define __LIMA_DRM_H__
+
+#include "drm.h"
+
+#if defined(__cplusplus)
+extern "C" {
+#endif
+
+#define LIMA_INFO_GPU_MALI400 0x00
+#define LIMA_INFO_GPU_MALI450 0x01
+
+struct drm_lima_info {
+	__u32 gpu_id;   /* out */
+	__u32 num_pp;   /* out */
+	__u64 va_start; /* out */
+	__u64 va_end;   /* out */
+};
+
+struct drm_lima_gem_create {
+	__u32 size;    /* in */
+	__u32 flags;   /* in */
+	__u32 handle;  /* out */
+	__u32 pad;
+};
+
+struct drm_lima_gem_info {
+	__u32 handle;  /* in */
+	__u32 pad;
+	__u64 offset;  /* out */
+};
+
+#define LIMA_VA_OP_MAP    1
+#define LIMA_VA_OP_UNMAP  2
+
+struct drm_lima_gem_va {
+	__u32 handle;  /* in */
+	__u32 op;      /* in */
+	__u32 flags;   /* in */
+	__u32 va;      /* in */
+};
+
+#define LIMA_SUBMIT_BO_READ   0x01
+#define LIMA_SUBMIT_BO_WRITE  0x02
+
+struct drm_lima_gem_submit_bo {
+	__u32 handle;  /* in */
+	__u32 flags;   /* in */
+};
+
+struct drm_lima_m400_gp_frame {
+	__u32 vs_cmd_start;
+	__u32 vs_cmd_end;
+	__u32 plbu_cmd_start;
+	__u32 plbu_cmd_end;
+	__u32 tile_heap_start;
+	__u32 tile_heap_end;
+};
+
+struct drm_lima_m400_pp_frame_reg {
+	__u32 plbu_array_address;
+	__u32 render_address;
+	__u32 unused_0;
+	__u32 flags;
+	__u32 clear_value_depth;
+	__u32 clear_value_stencil;
+	__u32 clear_value_color;
+	__u32 clear_value_color_1;
+	__u32 clear_value_color_2;
+	__u32 clear_value_color_3;
+	__u32 width;
+	__u32 height;
+	__u32 fragment_stack_address;
+	__u32 fragment_stack_size;
+	__u32 unused_1;
+	__u32 unused_2;
+	__u32 one;
+	__u32 supersampled_height;
+	__u32 dubya;
+	__u32 onscreen;
+	__u32 blocking;
+	__u32 scale;
+	__u32 foureight;
+	__u32 _pad;
+};
+
+struct drm_lima_m400_pp_wb_reg {
+	__u32 type;
+	__u32 address;
+	__u32 pixel_format;
+	__u32 downsample_factor;
+	__u32 pixel_layout;
+	__u32 pitch;
+	__u32 mrt_bits;
+	__u32 mrt_pitch;
+	__u32 zero;
+	__u32 unused0;
+	__u32 unused1;
+	__u32 unused2;
+};
+
+struct drm_lima_m400_pp_frame {
+	struct drm_lima_m400_pp_frame_reg frame;
+	struct drm_lima_m400_pp_wb_reg wb[3];
+	__u32 plbu_array_address[4];
+	__u32 fragment_stack_address[4];
+	__u32 num_pp;
+	__u32 _pad;
+};
+
+#define LIMA_PIPE_GP  0x00
+#define LIMA_PIPE_PP  0x01
+
+struct drm_lima_gem_submit_in {
+	__u32 ctx;
+	__u32 pipe;
+	__u32 nr_bos;
+	__u32 frame_size;
+	__u64 bos;
+	__u64 frame;
+};
+
+struct drm_lima_gem_submit_out {
+	__u32 fence;
+	__u32 done;
+};
+
+union drm_lima_gem_submit {
+	struct drm_lima_gem_submit_in in;
+	struct drm_lima_gem_submit_out out;
+};
+
+struct drm_lima_wait_fence {
+	__u32 pipe;        /* in */
+	__u32 fence;       /* in */
+	__u64 timeout_ns;  /* in */
+	__u32 ctx;         /* in */
+	__u32 _pad;
+};
+
+#define LIMA_GEM_WAIT_READ   0x01
+#define LIMA_GEM_WAIT_WRITE  0x02
+
+struct drm_lima_gem_wait {
+	__u32 handle;      /* in */
+	__u32 op;          /* in */
+	__u64 timeout_ns;  /* in */
+};
+
+#define LIMA_CTX_OP_CREATE 1
+#define LIMA_CTX_OP_FREE   2
+
+struct drm_lima_ctx {
+	__u32 op;          /* in */
+	__u32 id;          /* in/out */
+};
+
+#define DRM_LIMA_INFO        0x00
+#define DRM_LIMA_GEM_CREATE  0x01
+#define DRM_LIMA_GEM_INFO    0x02
+#define DRM_LIMA_GEM_VA      0x03
+#define DRM_LIMA_GEM_SUBMIT  0x04
+#define DRM_LIMA_WAIT_FENCE  0x05
+#define DRM_LIMA_GEM_WAIT    0x06
+#define DRM_LIMA_CTX         0x07
+
+#define DRM_IOCTL_LIMA_INFO DRM_IOR(DRM_COMMAND_BASE + DRM_LIMA_INFO, struct drm_lima_info)
+#define DRM_IOCTL_LIMA_GEM_CREATE DRM_IOWR(DRM_COMMAND_BASE + DRM_LIMA_GEM_CREATE, struct drm_lima_gem_create)
+#define DRM_IOCTL_LIMA_GEM_INFO DRM_IOWR(DRM_COMMAND_BASE + DRM_LIMA_GEM_INFO, struct drm_lima_gem_info)
+#define DRM_IOCTL_LIMA_GEM_VA DRM_IOW(DRM_COMMAND_BASE + DRM_LIMA_GEM_VA, struct drm_lima_gem_va)
+#define DRM_IOCTL_LIMA_GEM_SUBMIT DRM_IOWR(DRM_COMMAND_BASE + DRM_LIMA_GEM_SUBMIT, union drm_lima_gem_submit)
+#define DRM_IOCTL_LIMA_WAIT_FENCE DRM_IOW(DRM_COMMAND_BASE + DRM_LIMA_WAIT_FENCE, struct drm_lima_wait_fence)
+#define DRM_IOCTL_LIMA_GEM_WAIT DRM_IOW(DRM_COMMAND_BASE + DRM_LIMA_GEM_WAIT, struct drm_lima_gem_wait)
+#define DRM_IOCTL_LIMA_CTX DRM_IOWR(DRM_COMMAND_BASE + DRM_LIMA_CTX, struct drm_lima_ctx)
+
+#if defined(__cplusplus)
+}
+#endif
+
+#endif /* __LIMA_DRM_H__ */
-- 
2.7.4

